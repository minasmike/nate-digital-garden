[
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "Executive Briefing: EU AI Act Enforcement, Risk, and Positioning Scenarios",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. If you&#8217;d like to read, you can change ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "<p><em>This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. </em></p><p><em>If you&#8217;d like to read, you can <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">change your plan here</a>.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Founding subscribers get these once weekly briefings!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What Good Is a College Degree When AI Knows Everything? Grab the Job Skills That Matter in an AI World",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What do I do about college? Is school worth it? Is everyone just going to be a vibe coder?Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "<p><em><strong>What do I do about college?</strong> Is school worth it? Is everyone just going to be a vibe coder?</em></p><p><em>Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will pay attention!</em></p><p><em>I get these a lot. And it comes down to something really fundamental: we based our economy for a long time on knowledge, and knowledge is an inflationary currency.</em></p><p><em>In fact, knowledge has been hyper-inflating recently.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!Il0V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" width=\"1456\" height=\"974\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:411021,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166953534?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Thanks to <a href=\"https://www.perplexity.ai/search/aede5b3a-886f-446d-b087-df4bc509ec27\">Perplexity</a> for some work on this</figcaption></figure></div><p><em>For us humans, the question is simple: in a world where the currency we&#8217;ve used to articulate meaning is up for grabs, what do we do? </em></p><p><em>This stuff tends to get whispered about, shouted about (doomer style) and tends to not be discussed very thoughtfully. I want to dig a bit deeper here. I want to first look at the differentiation between jobs and skills more&#8212;I&#8217;ve referenced it but not unpacked it in detail before.</em></p><p><em>Then I want to respond to two of the biggest elephants in the room directly: the question of AGI and the question of college. They&#8217;re entangled, but I lay out a response to both, and I think both really rest on this knowledge economy questions, so it makes sense to address them here.</em></p><blockquote><p><em><strong>This free post is available to all subscribers</strong>, and is an example of the kind of thing paid subscribers get daily. <br><br>I&#8217;ll be sending my inaugural weekly exec brief to members of the AI Exec Circle this coming Sunday morning. If you&#8217;d like to receive it, <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">you can change your plan here</a>.</em></p></blockquote><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h4>The audio for this post:</h4><div class=\"native-audio-embed\" data-component-name=\"AudioPlaceholder\" data-attrs=\"{&quot;label&quot;:null,&quot;mediaUploadId&quot;:&quot;45a22c45-0be1-4eab-9d5e-c7751b8ae38d&quot;,&quot;duration&quot;:1912.8424,&quot;isEditorNode&quot;:true}\"></div><h1><strong>The Meaning Collapse</strong></h1><p>There's a radiologist in Cleveland right now staring at an AI system that reads scans better than she does. She's not worried about her job&#8212;demand for radiologists is actually increasing. But something darker gnaws at her: if a machine can do in seconds what took her a decade to master, what exactly is she?</p><p>This isn't a story about jobs. It's about the collapse of knowledge as the fundamental currency of human worth&#8212;and the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "reorganization of meaning that follows. Knowledge is going to keep growing at break-neck pace, but what about us? That&#8217;s what we all wonder, and I want to lay out a structured way to think about it head-on here.</p><h2><strong>Knowledge hyperinflation</strong></h2><p>We're experiencing the first knowledge hyperinflation in human history. Not the gentle devaluation that came with printing presses or calculators, but a complete collapse of the economic and social value of knowing things. Your MBA, your decades of experience, your painstakingly acquired expertise&#8212;they're Weimar Republic Deutschmarks, worthless before the ink dries on your certificate.</p><p>This isn't new. Knowledge has been inflating for decades&#8212;each generation needing more education for the same economic position their parents held. But AI represents the moment inflation tips into hyperinflation. When anyone can access all human knowledge instantly and synthesize it perfectly, what happens to the social order built on information scarcity? The same thing that happens to any currency when supply goes infinite: complete systemic collapse.</p><p>Andrew Peterson's research at the University of Poitiers names it \"<a href=\"https://hal.science/hal-04534111v1/file/Knowledge_collapse.pdf\">knowledge collapse</a>\"&#8212;when AI systems generate outputs clustered around probability centers, progressively narrowing the spectrum of available knowledge. His models show that discounting AI content by just 20% causes public beliefs to drift 2.3 times further from truth. We're not just devaluing knowledge; we're homogenizing it.</p><h2><strong>Jobs aren&#8217;t skills&#8230;</strong></h2><p>Here's where every automation attempt goes wrong, from Roger Smith's GM to today's AI evangelists: they decompose jobs into discrete skills, see machines master those skills, then assume the job itself can be automated. This reductionist view misses everything that makes human work valuable.</p><p>A lawyer isn't just someone who knows precedents&#8212;that's the skill. The job involves reading a room, sensing when a client is lying, knowing when to push and when to yield, building trust over decades. A radiologist doesn't just read scans&#8212;she translates between machine precision and human fear, catches the one-in-a-thousand case that breaks the pattern, provides the human presence that transforms diagnosis from data to meaning.</p><p>The skills can be automated. The job&#8212;that complex web of judgment, relationships, and context&#8212;we have no realistic map for what automation of that job looks like.</p><p>PwC's 2025 data reveals this starkly:<strong> industries with highest AI exposure show 3x revenue growth and wages rising twice as fast</strong>. Why? Because organizations that understand jobs as more than skill-bundles use AI to amplify human capability rather than replace it. Those who see only skills to automate end up like GM&#8212;with robots painting each other while market share evaporates.</p><h2><strong>GM finds out the hard way</strong></h2><p>Roger Smith spent upwards of $90 billion in the 1980s pursuing the ultimate reductionist dream: decompose car manufacturing into discrete tasks, automate each task, eliminate the workers. The Detroit-Hamtramck Assembly plant opened in 1985 as his monument to this vision.</p><p>The reality proved prophetic for our current moment. Spray-painting robots coated each other instead of cars. Welding robots sealed doors shut. The \"robogate\" systems that were supposed to revolutionize assembly instead created catastrophic bottlenecks whenever they encountered variations outside their programming. GM's market share plummeted from 46% to 35.1% during",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Smith's tenure.</p><p>The bitter lesson came through NUMMI, GM's joint venture with Toyota. Using GM's worst workforce, Toyota achieved world-class quality not through automation but through recognizing that manufacturing jobs involved continuous improvement, problem-solving, and adaptation&#8212;human capabilities that no amount of discrete skill automation could replicate. GM had automated the skills but lost the job.</p><h2><strong>So what sticks if knowledge doesn&#8217;t?</strong></h2><p>In a world of hyperinflated knowledge, what is hard currency? Let me suggest a non-exhaustive list here. I&#8217;ve written about a few of these before, but never nailed them down in a single clean list like this. What would you add?</p><p><strong>Glue Work</strong>: The invisible labor that connects systems, translates between domains, and maintains coherence. The nurse who bridges between AI diagnosis and patient understanding. The project manager who transforms algorithmic outputs into team direction. Undervalued before, essential now.</p><p><strong>Taste</strong>: In infinite possibility, knowing what to build matters more than knowing how. The creative director choosing from AI's million options. The product manager selecting features when anything is technically possible. Taste can't be automated because it's not a skill&#8212;it's accumulated judgment about what matters.</p><p><strong>Extreme Agency</strong>: The ability to operate with minimal direction, maximal ownership. When AI handles execution, humans must excel at goal-setting, priority-defining, and course-correcting. Agency isn't following instructions&#8212;it's knowing what instructions to create.</p><p><strong>Learning Velocity</strong>: Not knowledge accumulation but adaptation speed. The half-life of technical skills has compressed to 2.5 years. Value accrues to those who learn faster than knowledge inflates, who surf the wave of obsolescence rather than drowning in it.</p><p><strong>Intent Horizons</strong>: The capacity to maintain coherent goals across extended timeframes. AI excels at optimizing immediate objectives but lacks the ability to balance competing long-term priorities. Humans provide the narrative coherence that prevents optimization from becoming self-defeating.</p><p><strong>Interruptibility</strong>: The meta-skill of knowing when to stop the machine. Like Toyota's jidoka principle&#8212;automation with human touch&#8212;value concentrates in those who recognize when systems are failing in ways metrics can't capture.</p><p>These aren't skills in the traditional sense. They're ways of being that emerge from the complex intersection of personality, experience, and judgment. They resist the decomposition that makes automation possible.</p><h2><strong>If we don&#8217;t know, what are we?</strong></h2><p>Here&#8217;s the thing: The radiologist's crisis isn't economic&#8212;it's ontological. For centuries, we built identity on accumulating knowledge. \"I am what I know\" was the tacit creed of the professional class. Degrees, certifications, years of experience&#8212;these weren't just economic signals but existential anchors.</p><p>Knowledge workers report increasing anxiety about professional relevance, imposter syndrome when AI outperforms their core competencies, and fundamental uncertainty about career direction. The psychological impact extends beyond individual identity to social structure. When knowledge no longer confers status, what organizing principle replaces it?</p><p>The answer emerging from successful AI adoptions: we shift from \"I am what I know\" to \"I am how I connect, judge, and create meaning from what machines know.\" This is the world of AI agent managers that Jensen Huang laid out in January of this year. The lawyer who saves four hours weekly with AI while maintaining higher accuracy than either AI or lawyers alone isn't replaced&#8212;she's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "amplified. Her value shifts from information storage to wisdom application.</p><h2><strong>The paradox of circuit breakers</strong></h2><p>Financial markets discovered this truth through trillion-dollar near-disasters. Despite algorithms executing 70% of trades, markets depend on human-triggered circuit breakers to prevent catastrophic losses. When the S&amp;P 500 drops 7%, trading halts for 15 minutes&#8212;not for algorithms to recalibrate, but for humans to assess whether the selloff reflects reality or algorithmic panic.</p><p>March 2020 proved the point: circuit breakers triggered four times in a single month, preventing algorithmic feedback loops from destroying market value. The paradox: as trading becomes more automated, human judgment becomes more critical, not less. The humans don't execute trades&#8212;they decide when to stop the machines from trading.</p><p>Tesla's Full Self-Driving is a really interesting test of this hypothesis. Tesla&#8217;s essential bet is they&#8217;ve seen enough edge cases now to be safe on the road. Safer than humans and safe enough to launch Robotaxi. Tesla is probably right that they are safer than people (it&#8217;s a low bar), but we humans are likely to remain very intolerant of robot errors in driving&#8212;we will absolutely use a double standard here based on patterns of previous investigations. And I have no doubt we will continue to be a majority human driving world for a good long while.</p><h2><strong>The great miscalculation</strong></h2><p>Organizations pursuing pure automation make the same error: they see tasks, not systems. They automate skills, not jobs. They replace capabilities, not judgment. The failure rate is predictable and brutal.</p><p>IBM Watson Health, sold for $1 billion after $5 billion investment. Google's diabetic retinopathy system, perfect in labs but rejecting 21% of real-world images due to lighting variations. Amazon's AI recruiting tool, scrapped after systematically discriminating against women. Each failed by automating the measurable while failing to measure what mattered.</p><p>The successes tell the opposite story. Swedish breast cancer screening combining AI with radiologists detected 20% more cancers while reducing workload 44%. Law firms report AI-augmented lawyers generating $100,000 additional billable time annually. Manufacturing technology investment reached $2.81 billion in 2024, focused on collaborative robots working alongside humans rather than replacing them.</p><p>I&#8217;m not here to promise you that organizations won&#8217;t make this screw-up again. They will. Nor am I here to make the Pollyanna-ish claim that no jobs will be lost to AI. Nor am I trying to say that automation won&#8217;t work.</p><p>I&#8217;m making a subtler point: by framing jobs in terms of systems of skills we are extending 20th century managerial philosophy to the nth degree, and we are going to find out (again) that jobs are more complex than we realize. Work that matters is more complex than we realize. And the skill list I&#8217;ve outlined above is a way to start to characterize the world of work beyond the world of knowledge&#8212;a description of all the other stuff we do besides know stuff!</p><h2><strong>The 56% premium</strong></h2><p>What&#8217;s 56%? This: Workers with AI skills command 56% wage premiums&#8212;up from 25% a year ago. The real tell? Zuckerberg is now poaching AI researchers for $10 million (or more) packages, not just to build",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "better models but to figure out how to help Meta win the AI race. The premium isn't for AI expertise itself&#8212;it's for knowing how to bridge AI capabilities to real-world value.</p><p>The World Economic Forum projects 78 million net new jobs by 2030&#8212;170 million created, 92 million eliminated&#8212;concentrated in roles that leverage AI as tool rather than replacement. McKinsey (I know lol) finds 30% of work hours face automation potential, but actual displacement remains far lower as organizations discover the irreplaceable value of human judgment.</p><p>The premium doesn't attach to AI skills themselves&#8212;those commodify quickly and also are somewhat ephemeral as AI systems evolve. It attaches to the meta-skill of knowing how to remain valuable and sticky against problems as skills commodify. It rewards those who understand that in a world of infinite knowledge, the scarcity shifts outside knowledge toward other human capacities that can&#8217;t be caught by knowing more things.</p><h2><strong>The choice that defines the next decade</strong></h2><p>We stand at an inflection point. Not between humans and machines&#8212;I think that's a false binary. But between two visions of human worth:</p><ol><li><p>Desperately trying to out-know machines, accumulating credentials in a hyperinflationary spiral&#8212;trying to compete for the knowledge prize with machines</p></li><li><p>Developing the judgment to know when machines are wrong, rigid, or heading toward catastrophe&#8212;or more deeply, learning to partner with machines</p></li></ol><p>The first path leads to existential crisis and economic irrelevance. The second leads to a new form of human value&#8212;not despite AI's capabilities, but because of them.</p><p>The radiologist in Cleveland faces something more complex than a skills crisis. ChatGPT scores higher on empathy tests than most doctors. AI reads scans more accurately. But work isn't individual tasks&#8212;it's the bundling of tasks with ownership, liability, interruptability, long-term thinking, and meaning-making within a team and between a team and patients. When the AI misses a tumor, who gets sued? When a patient needs someone to blame, who stands there and takes it? </p><p>And it&#8217;s not just negative. Can the AI give the patient a hug? Can the AI ask for a second opinion? Can the AI step over and look over a buddy&#8217;s shoulder? Does the AI tolerate switching to an entirely different patient history within the same chat? </p><p>We can go on and on but more fundamentally: if work is the act of making meaning, and LLMs are what Karpathy calls \"stochastic parrots\"&#8212;spirits that simulate meaning without creating it&#8212;we face unforeseen obstacles to getting actual work done. We don&#8217;t know what it takes to truly automate work! The gap between an LLM's summary of War and Peace and actually reading War and Peace isn't about information transfer. It's about experience, transformation, the irreducible difference between knowing about something and knowing it.</p><p>That's not a job description. It's the difference between simulating human and being human. And maybe that means that the knowledge isn&#8217;t the point.</p><p>The question isn't whether AI will take your job. It's whether you'll discover what humans are actually for before your knowledge becomes worthless. The clock is ticking, and the currency of expertise is collapsing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "faster than any of us are comfortable with, and it&#8217;s demanding that we cultivate a much wider range of skills than most of our educational systems prepared us for. It&#8217;s not easy.</p><p>But maybe&#8212;just maybe&#8212;that's exactly when we discover what being human was always supposed to mean. I know it&#8217;s cheesy but I&#8217;ve seen people go through this reflection stage individually in journeys over the past few years, and I think it&#8217;s a real post-AI human realization moment. Call it post-AI midlife crisis if you like.</p><p>And right on cue, we have the biggest objection to all this:</p><h2><strong>Yes, but what about AGI?</strong></h2><p><em>\"Why prepare for a human-AI hybrid future when AGI will make us all obsolete in a year anyway?\"</em></p><p>Here's the honest answer: that&#8217;s probably not a correctly framed question. </p><p>Annoying I know. But it matters.</p><p>AGI (maybe?) might be achievable with scaled LLM architectures. Intent horizons are doubling every few months&#8212;from 3 hours to 7 to potentially 30. Memory implementations get more sophisticated daily. In a year or two, these systems might maintain coherent goals for weeks or months. Things will get smarter in jagged ways.</p><p>But here's what's becoming clear: as LLMs scale brilliantly on the knowledge dimension, we have no clear picture of how they're scaling on the dimensions that actually matter for getting work done. They're building a ladder to the moon while we need bridges between islands.</p><p>The core issue isn't whether they'll achieve long-term memory&#8212;they probably will. It's whether that memory will have the magical intuitive flexibility that makes human minds special. Current implementations are like lossy JPEGs that sometimes hallucinate what was in the compressed bits: they can retrieve the gist, miss crucial details, and occasionally invent things that were never there. When you need the exact contract clause or the specific drug interaction, \"mostly right\" isn't right.</p><p>I&#8217;m not clear if the direction we&#8217;re going is the correct direction to address some of these fundamental capabilities gaps with humans, and what I&#8217;ve seen so far doesn&#8217;t give me the impression we&#8217;re making very fast progress on some of the human skills I outlined above. (Most humans breathe in relief here lol)</p><p>Another example: LLMs excel at going deep within domains but struggle at the boundaries where real work happens. They can generate brilliant code within a well-defined problem space but miss when the technical challenge has become organizational. They can write perfect legal briefs but not recognize when the legal strategy needs to become a business negotiation. This isn't a bug&#8212;it's the difference between knowledge and judgment.</p><p>Which brings us to what matters: in a world where knowledge can be instantly accessed and credentials can be faked with a ChatGPT account, what constitutes genuine proof of work?</p><p><strong>Really excellent software that actually ships and works.</strong> You can't fake a codebase that handles real users, real scale, real edge cases. The gap between \"demo that impresses in an interview\" and \"system that survives production\" can't be bridged by prompting. It requires the thousand small decisions, trade-offs, and intuitions that come from genuine experience.</p><p><strong>Writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "that changes how people think.</strong> Not just grammatically correct prose or well-structured arguments, but writing that creates new mental models, that makes readers see the world differently. AI can imitate style but can't generate the lived experience and unique perspective that makes writing resonate at a deep level.</p><p><strong>Successful cross-functional projects.</strong> Anything requiring navigation across technical, business, and human domains. The project manager who ships a feature by aligning engineering, design, sales, and legal isn't just coordinating&#8212;they're translating between incompatible worldviews, maintaining coherence across context switches that break AI systems.</p><p><strong>Building and maintaining trust networks.</strong> Reputation that accumulates over years through consistent judgment calls. The venture capitalist whose portfolio succeeds not through any single brilliant insight but through thousands of micro-decisions about people and possibilities. This can't be speedrun or simulated.</p><p><strong>Cultural creation and curation.</strong> The creative director who consistently identifies what will resonate before it's obvious. The editor who develops writers. The A&amp;R person who finds artists before they break. Taste that predicts and shapes culture rather than following it.</p><p><strong>High-stakes decision-making under uncertainty.</strong> The surgeon who recognizes when to deviate from protocol. The pilot who safely lands a damaged plane. The CEO who navigates a crisis. Situations where judgment must integrate incomplete information, competing priorities, and irreversible consequences.</p><p>The pattern? These are all outputs that emerge from the messy intersection of knowledge, experience, judgment, and human connection. They're what remains when pure information processing becomes commoditized.</p><p>Anyway, there's something deeper here. If work is fundamentally about making meaning&#8212;not just processing information but creating significance&#8212;then we face an unexpected obstacle. LLMs are brilliant at simulating meaning, generating text that feels profound, responses that seem empathetic. They're stochastic spirits that can mimic understanding perfectly. But mimicry isn't meaning. The difference between an AI's summary of War and Peace and the experience of reading it isn't about information&#8212;it's about transformation, about being changed by the encounter.</p><p>Work bundles tasks with ownership, liability, and the human act of meaning-making. When AI makes a medical error, who owns it? When a project fails, who takes responsibility? When success happens, who finds meaning in it? These aren't technical problems but existential ones. They can't be solved by better algorithms because they're not about processing&#8212;they're about being.</p><p>We're watching the greatest shift in human value since the industrial revolution. Not because AI will replace humans, but because it forces us to identify what was always most valuable about human work: not the knowledge we store but the connections we make, not the problems we solve but knowing which problems matter, not executing tasks but navigating the undefined spaces between them.</p><p>The timeline question misses the point. Whether AGI arrives in 2025 or 2050, the humans who thrive will be those who understand that in a world of infinite knowledge, value concentrates in judgment, taste, and the ability to navigate discontinuity. The clock isn't ticking on human obsolescence&#8212;it's ticking on our willingness to recognize what makes us irreplaceable.</p><p>Ok and one last question for the road&#8230;</p><h2><strong>What about college?</strong></h2><p>On June 24, 2025&#8212;just days ago&#8212;Monster and CareerBuilder filed",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "for Chapter 11 bankruptcy. These titans of the early internet, who once bought Super Bowl ads to revolutionize job hunting, collapsed under $100-500 million in debt. Their demise isn't just another tech casualty. It's the canary in the coal mine for our entire credentialing system.</p><p>Since Oxford's founding&#8212;teaching existed there by 1096, though the university rapidly developed from 1167&#8212;universities have served as civilization's knowledge gatekeepers. For nearly a millennium, the path was clear: accumulate knowledge, earn credentials, trade them for economic and social position. This system survived the printing press, the industrial revolution, and the internet. It won't survive AI.</p><p>The numbers are staggering, even if contested. Surveys show anywhere from 30% to 89% of college students using ChatGPT for homework, with most estimates hovering around 40% for regular use. Regardless of the exact figure, one educator's observation rings true: this is education's \"Lance Armstrong moment.\" When enough players cheat with high upside and low consequences, others feel forced to cheat to compete. It becomes, in Armstrong's words, \"impossible to win without doping.\"</p><p>But here's the deeper crisis: if knowledge is now free and instant, what exactly are universities selling? Not information&#8212;ChatGPT provides that. Not skills&#8212;YouTube tutorials teach those. Not even critical thinking&#8212;AI can simulate that too. They're selling something increasingly abstract: the <em>idea</em> of credibility in a world where credentials can be faked with a prompt.</p><p>The job market reflects this confusion. Nearly half of companies say they plan to eliminate bachelor's degree requirements, yet paradoxically, 59% of employers say degrees matter MORE than five years ago. We're watching a system in violent transition, unsure whether to double down on traditional credentials or abandon them entirely.</p><p>This isn't just about cheating or job requirements. It's about the collapse of an entire social technology. Degrees served as universal signals&#8212;imperfect but shared fictions that enabled coordination. An MBA from Wharton meant something specific. Ten years at McKinsey conveyed particular competencies. Now these signals are noise. You can fake the knowledge, simulate the skills, even mimic the writing style. What can't be faked?</p><p>The answer emerging from forward-thinking educators and employers: proof of work that demonstrates judgment, not just knowledge. Ship code that handles real users. Write something that changes how people think. Navigate complex projects across domains. Build trust networks over years. Create culture rather than consume it. Make high-stakes decisions where failure has real consequences. Yes there&#8217;s an on-ramp here, but the ideas are there to change how we show value and that&#8217;s important.</p><p>These outputs resist AI assistance not because they're technically difficult but because they require ownership, liability, and the human act of meaning-making. When an AI-written essay fails to persuade, who takes responsibility? When generated code crashes in production, who fixes it at 3am? When a decision goes wrong, who stands before the board?</p><p>Universities that survive will transform from knowledge-delivery systems to judgment-development institutions. They'll teach not what to think but how to think when infinite information is available. They'll credential not information retention but the ability to navigate discontinuity, own outcomes, and create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "meaning from noise.</p><p>The students cheating with ChatGPT aren't lazy&#8212;they're rational actors in an irrational system (I think <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">Roy and Cluely</a> agree with me on this one thing lmao). They're using 21st-century tools to game 19th-century assessments for 11th-century credentials. The real scandal isn't that they're cheating. It's that we're still pretending the old game matters.</p><p>Monster's bankruptcy filing listed the cause as a \"challenging and uncertain macroeconomic environment.\" But that's corporate speak for a simpler truth: when knowledge becomes worthless, the infrastructure of knowledge-trading collapses too. First the job boards. Next, perhaps, the universities that feed them.</p><p>Unless they remember what they're actually for!</p><p>Good luck out there. Stay Curious. Stay human.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!_LYt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" width=\"1456\" height=\"971\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2037707,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166955816?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h2><strong>Sources</strong></h2><h3><strong>Monster/CareerBuilder Bankruptcy Sources</strong></h3><ul><li><p><a href=\"https://www.newsx.com/business/monster-and-careerbuilder-file-for-bankruptcy-begin-asset-sales-amid-market-shift-9215/\">NewsX &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2025/06/24/careerbuilder-monster-bankruptcy/\">Washington Post &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.cnn.com/2025/06/25/business/monster-careerbuilder-bankruptcy\">CNN &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.staffingindustry.com/news/global-daily-news/careerbuilder-monster-selling-businesses-filing-for-bankruptcy\">Staffing Industry Analysts &#8211; CareerBuilder, Monster Selling Businesses</a></p></li><li><p><a href=\"https://www.reuters.com/legal/litigation/careerbuilder-monster-which-once-dominated-online-job-boards-file-bankruptcy-2025-06-24/\">Reuters &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.abc27.com/news/consumer/careerbuilder-monster-com-enter-chapter-11-bankruptcy/\">ABC27 &#8211; CareerBuilder and Monster Enter Chapter 11</a></p></li><li><p><a href=\"https://www.foxbusiness.com/economy/online-job-listing-company-careerbuilding-monster-files-bankruptcy\">Fox Business &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Monster.com\">Wikipedia &#8211; Monster.com</a></p></li><li><p><a href=\"https://www.hrdive.com/news/careerbuilder-monster-files-chapter-11-bankruptcy/751498/\">HR Dive &#8211; CareerBuilder, Monster File Chapter 11</a></p></li><li><p><a href=\"https://www.northbaybusinessjournal.com/article/industrynews/careerbuilder-monster-bankruptcy-recruitment-workforce/\">North Bay Business Journal &#8211; CareerBuilder, Monster Bankruptcy</a></p></li></ul><div><hr></div><h3><strong>AI/ChatGPT in Education Sources</strong></h3><ul><li><p><a href=\"https://www.axios.com/2025/05/26/ai-chatgpt-cheating-college-teachers\">Axios &#8211; Teachers Worry About AI and Cheating</a></p></li><li><p><a href=\"https://slate.com/life/2025/05/college-student-cheating-ai-detector-chatgpt-school-education.html\">Slate &#8211; College Student Cheating and AI Detectors</a></p></li><li><p><a href=\"https://longreads.com/2025/05/21/chatgpt-ai-college-cheating/\">Longreads &#8211; ChatGPT and the Cheating Crisis</a></p></li><li><p><a href=\"https://nerdynav.com/chatgpt-cheating-statistics/\">NerdyNav &#8211; ChatGPT Cheating Statistics</a></p></li><li><p><a href=\"https://ed.stanford.edu/news/what-do-ai-chatbots-really-mean-students-and-cheating\">Stanford Graduate School of Education &#8211; AI Chatbots and Cheating</a></p></li><li><p><a href=\"https://www.edweek.org/technology/opinion-the-ai-cheating-crisis-education-needs-its-anti-doping-movement/2024/02\">Education Week &#8211; The AI Cheating Crisis</a></p></li><li><p><a href=\"https://apnews.com/article/chatgpt-cheating-ai-college-1b654b44de2d0dfa4e50bf0186137fc1\">Associated Press &#8211; AI and College Cheating</a></p></li><li><p><a href=\"https://www.sciencedirect.com/science/article/pii/S2666920X24000560\">ScienceDirect &#8211; Academic Integrity and AI</a></p></li><li><p><a href=\"https://www.technologyreview.com/2023/04/06/1071059/chatgpt-change-not-destroy-education-openai\">MIT Technology Review &#8211; ChatGPT Will Change Education</a></p></li><li><p><a href=\"https://slate.com/technology/2023/02/chat-gpt-cheating-college-ai-detection.html\">Slate (2023) &#8211; Early Warning Signs of AI Cheating</a></p></li></ul><div><hr></div><h3><strong>Employment/Hiring Trends Sources</strong></h3><ul><li><p><a href=\"https://www.naceweb.org/job-market/trends-and-predictions/hiring-projections-level-off-for-the-college-class-of-2025\">NACE &#8211; Hiring Projections for Class of 2025</a></p></li><li><p><a href=\"https://www.hiringlab.org/2024/12/10/indeed-2025-us-jobs-and-hiring-trends-report/\">Indeed Hiring Lab &#8211; 2025 U.S. Jobs and Hiring Trends</a></p></li><li><p><a href=\"https://www.survivalworld.com/economics/20-college-degrees-employers-dont-want-in-2025/\">Survival World &#8211; Degrees Employers Don&#8217;t Want in 2025</a></p></li><li><p><a href=\"https://www.highereddive.com/news/nearly-half-of-companies-plan-to-eliminate-bachelors-degree-requirements/702277/\">Higher Ed Dive &#8211; Companies Dropping Bachelor&#8217;s Requirements</a></p></li><li><p><a href=\"https://time.com/7291844/job-market-college-graduates-unemployment/\">TIME &#8211; Job Market for College Graduates</a></p></li><li><p><a href=\"https://www.lanereport.com/174929/2024/07/75-of-employers-say-removing-this-requirement-for-job-applicants-has-improved-their-company/\">Lane Report &#8211; Removing Degree Requirements Improves Hiring</a></p></li><li><p><a href=\"https://cew.georgetown.edu/cew-reports/projections2031/\">Georgetown CEW &#8211; Projections 2031 Report</a></p></li><li><p><a href=\"https://www.aplu.org/our-work/4-policy-and-advocacy/publicuvalues/employment-earnings/\">Association of Public and Land-grant Universities &#8211; Employment and Earnings</a></p></li><li><p><a href=\"https://www.bls.gov/news.release/pdf/empsit.pdf\">Bureau of Labor Statistics &#8211; Employment Situation Report (PDF)</a></p></li><li><p><a href=\"https://www.testgorilla.com/skills-based-hiring/state-of-skills-based-hiring-2024/\">TestGorilla &#8211; State of",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Skills-Based Hiring 2024</a></p></li></ul><h3><strong>Contemporary AI Failures (McDonald&#8217;s, Tesla, etc.)</strong></h3><ul><li><p><a href=\"https://www.restaurantbusinessonline.com/technology/mcdonalds-ending-its-drive-thru-ai-test\">Restaurant Business Online &#8211; McDonald&#8217;s Ending Its Drive-Thru AI Test</a></p></li><li><p><a href=\"https://apnews.com/article/mcdonalds-ai-drive-thru-ibm-bebc898363f2d550e1a0cd3c682fa234\">AP News &#8211; McDonald&#8217;s Ends AI Drive-Thru Partnership with IBM</a></p></li><li><p><a href=\"https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html\">CIO &#8211; 5 Famous Analytics and AI Disasters</a></p></li><li><p><a href=\"https://www.cnbc.com/2024/10/18/tesla-faces-nhtsa-investigation-of-full-self-driving-after-fatal-collision.html\">CNBC &#8211; Tesla Faces NHTSA Investigation After Fatal Collision</a></p></li><li><p><a href=\"https://www.tesla.com/VehicleSafetyReport\">Tesla &#8211; Vehicle Safety Report</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2024/10/18/tesla-full-self-driving-nhtsa-fsd/\">Washington Post &#8211; NHTSA Investigates Tesla&#8217;s Full-Self Driving System</a></p></li><li><p><a href=\"https://abcnews.go.com/Business/tesla-driving-crash-reports-prompt-nhtsa-investigation/story?id=114922283\">ABC News &#8211; Tesla Crash Reports Prompt NHTSA Investigation</a></p></li><li><p><a href=\"https://www.thetradenews.com/human-judgement-still-king-in-a-world-of-algorithmic-trades/\">The TRADE &#8211; Human Judgment Still King in Algorithmic Trades</a></p></li><li><p><a href=\"https://oatmealhealth.com/why-has-ai-failed-so-far-in-healthcare-despite-billions-of-investment/\">Oatmeal Health &#8211; Why AI Has Failed in Healthcare (So Far)</a></p></li></ul><div><hr></div><h3><strong>Knowledge Hyperinflation / Knowledge Collapse</strong></h3><ul><li><p><a href=\"https://arxiv.org/abs/2404.03502\">arXiv &#8211; Collapse of Knowledge</a></p></li><li><p><a href=\"https://www.emergentmind.com/papers/2404.03502\">Emergent Mind &#8211; Summary of arXiv 2404.03502</a></p></li><li><p><a href=\"https://www.clio.com/blog/tools-for-lawyers/\">Clio &#8211; Best Tools for Lawyers</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/issues/artificial-intelligence/ai-jobs-barometer.html\">PwC &#8211; AI Jobs Barometer (Global)</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html\">PwC &#8211; Productivity Growth and AI</a></p></li><li><p><a href=\"https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-jobs-barometer.html\">PwC US &#8211; AI Jobs Barometer</a></p></li></ul><div><hr></div><h3><strong>MIT / Economics Research on AI</strong></h3><ul><li><p><a href=\"https://economics.mit.edu/news/daron-acemoglu-what-do-we-know-about-economics-ai\">MIT Economics &#8211; Daron Acemoglu on AI and Labor</a></p></li><li><p><a href=\"https://www.imf.org/en/Publications/fandd/issues/2023/12/Rebalancing-AI-Acemoglu-Johnson\">IMF &#8211; Rebalancing AI by Acemoglu and Johnson</a></p></li><li><p><a href=\"https://ssir.org/articles/entry/ai-impact-on-jobs-and-work\">Stanford Social Innovation Review &#8211; AI Impact on Jobs and Work</a></p></li></ul><div><hr></div><h3><strong>GM / Roger Smith Sources</strong></h3><ul><li><p><a href=\"https://en.wikipedia.org/wiki/Roger_Smith_(executive)\">Wikipedia &#8211; Roger Smith (Executive)</a></p></li><li><p><a href=\"https://www.imdb.com/name/nm0809792/bio/\">IMDb &#8211; Roger Smith Bio</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Detroit/Hamtramck_Assembly\">Wikipedia &#8211; Detroit/Hamtramck Assembly</a></p></li><li><p><a href=\"https://www.leanblog.org/2016/06/gms-ceo-roger-smith-thought-toyota-had-magic-but-this-was-the-secret/\">Lean Blog &#8211; What GM Misunderstood About Toyota</a></p></li><li><p><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Scribd &#8211; </a><em><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Comeback: The Fall and Rise of the American Automobile Industry</a></em></p></li><li><p><a href=\"https://www.wrike.com/blog/key-to-perfect-automation-is-imperfect-people/\">Wrike &#8211; The Key to Perfect Automation is Imperfect People</a></p></li><li><p><a href=\"https://www.washingtonpost.com/archive/business/1990/08/01/roger-smith-gm-driven-to-regain-market-share/530dfe27-35e9-4d0e-a7f8-3a0f659d7a00/\">Washington Post (1990) &#8211; Roger Smith at GM</a></p></li><li><p><a href=\"https://archive.seattletimes.com/archive/19900730/1085108/roger-smith-leaves-his-mark-at-gm\">Seattle Times Archive &#8211; Roger Smith Leaves His Mark</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/NUMMI\">Wikipedia &#8211; NUMMI</a></p></li><li><p><a href=\"https://www.popularmechanics.com/cars/a5514/4350856/\">Popular Mechanics &#8211; A Look Back at GM and Toyota</a></p></li></ul><div><hr></div><h3><strong>Skills Half-Life / Professional Identity</strong></h3><ul><li><p><a href=\"https://www.weforum.org/stories/2024/01/this-is-the-one-skill-everybody-needs-in-the-age-of-ai/\">World Economic Forum &#8211; One Skill Everyone Needs in the Age of AI</a></p></li><li><p><a href=\"https://www.cio.com/article/219940/thriving-in-a-world-of-knowledge-half-life.html\">CIO &#8211; Thriving in a World of Shrinking Knowledge Half-Life</a></p></li><li><p><a href=\"https://www.linkedin.com/pulse/shrinking-half-life-skills-peter-smulovics\">LinkedIn &#8211; Shrinking Half-Life of Skills</a></p></li><li><p><a href=\"https://www.ibm.com/blogs/ibm-training/skills-transformation-2021-workplace/\">IBM &#8211; Skills Transformation in the Workplace</a></p></li><li><p><a href=\"https://www.verywellmind.com/what-is-an-identity-crisis-2795948\">Verywell Mind &#8211; What Is an Identity Crisis?</a></p></li><li><p><a href=\"https://seo.ai/blog/ai-replacing-jobs-statistics\">SEO.ai &#8211; AI Job Replacement Statistics</a></p></li></ul><div><hr></div><h3><strong>Financial Markets / Circuit Breakers</strong></h3><ul><li><p><a href=\"https://www.investopedia.com/articles/markets/012716/four-big-risks-algorithmic-highfrequency-trading.asp\">Investopedia &#8211; Four Big Risks in Algorithmic Trading</a></p></li><li><p><a href=\"https://www.investopedia.com/terms/c/circuitbreaker.asp\">Investopedia &#8211; Circuit Breaker Definition</a></p></li></ul><div><hr></div><h3><strong>Legal / Medical AI Adoption</strong></h3><ul><li><p><a href=\"https://www.clio.com/blog/will-ai-replace-paralegals/\">Clio &#8211; Will AI Replace Paralegals?</a></p></li><li><p><a href=\"https://apnews.com/article/ai-algorithms-chatgpt-doctors-radiologists-3bc95db51a41469c390b0f1f48c7dd4e\">AP News &#8211; ChatGPT, Doctors, and AI Algorithms</a></p></li></ul><div><hr></div><h3><strong>Amazon AI Recruiting Failure</strong></h3><ul><li><p><a href=\"https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women\">Euronews &#8211; Amazon Scraps Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.cut-the-saas.com/ai/case-study-how-amazons-ai-recruiting-tool-learnt-gender-bias\">Cut the SaaS &#8211; Case Study on Amazon&#8217;s Gender Bias in AI</a></p></li><li><p><a href=\"https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/\">Reuters &#8211; Amazon&#8217;s Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html\">CMU &#8211; Amazon Scraps Biased AI Hiring Engine</a></p></li><li><p><a href=\"https://www.aboutamazon.com/news/workplace/how-amazon-leverages-ai-and-ml-to-enhance-the-hiring-experience-for-candidates\">About Amazon &#8211; Enhancing Hiring with AI and ML</a></p></li><li><p><a href=\"https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/\">Reuters &#8211; Tesla FSD Investigation (duplicate to Tesla)</a></p></li></ul><div><hr></div><h3><strong>AI Scaling and AGI Research</strong></h3><ul><li><p><a href=\"https://www.geeky-gadgets.com/infinite-memory-ai-models/\">Geeky Gadgets &#8211; Infinite Memory AI Models</a></p></li><li><p><a href=\"https://openai.com/index/gpt-4-1/\">OpenAI &#8211; GPT-4.1 Overview</a></p></li><li><p><a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-gpt-41-models-with-improved-coding-long-context-2025-04-14/\">Reuters &#8211; OpenAI Launches GPT-4.1</a></p></li><li><p><a href=\"https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/\">Google Developers Blog &#8211; Gemini API Updates</a></p></li><li><p><a href=\"https://gemini.google/overview/long-context/?hl=en\">Gemini &#8211; Long Context Capabilities</a></p></li><li><p><a href=\"https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it\">Google Cloud Blog &#8211; Gemini Pro and the Needle-in-the-Haystack Test</a></p></li><li><p><a href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\">Google Blog &#8211; Gemini AI Update (Dec 2024)</a></p></li><li><p><a href=\"https://web.swipeinsight.app/posts/claude-unveils-projects-for-ai-workspaces-with-500-page-memory-7902\">Swipe Insight &#8211; Claude&#8217;s 500-Page Memory</a></p></li><li><p><a href=\"https://www.anthropic.com/news/claude-3-family\">Anthropic &#8211; Claude 3 Model Family</a></p></li><li><p><a href=\"https://www.anthropic.com/news/activating-asl3-protections\">Anthropic &#8211; Activating ASL3 Protections</a></p></li><li><p><a href=\"https://fortune.com/2025/05/22/anthropic-new-models-ai-openai-google/\">Fortune &#8211; Anthropic&#8217;s New AI Models</a></p></li><li><p><a href=\"https://arxiv.org/html/2410.03156v1\">arXiv &#8211; Scaling Laws (2410.03156)</a></p></li><li><p><a href=\"https://openreview.net/forum?id=TvGPP8i18S\">OpenReview &#8211; Scaling Trends in AGI</a></p></li></ul><div><hr></div><h3><strong>AGI Timeline Predictions</strong></h3><ul><li><p><a href=\"https://venturebeat.com/ai/openai-begins-2025-with-massive-hype-for-agi-superintelligence/\">VentureBeat &#8211; 2025 AGI and Superintelligence Hype</a></p></li><li><p><a href=\"https://time.com/7205596/sam-altman-superintelligence-agi/\">TIME &#8211; Sam Altman on Superintelligence</a></p></li><li><p><a href=\"https://felloai.com/2024/11/dario-amodei-ceo-of-anthropic-artificial-general-intelligence-is-coming-in-2027/\">Fello AI &#8211; Dario Amodei Predicts AGI by 2027</a></p></li><li><p><a href=\"https://lexfridman.com/dario-amodei-transcript/\">Lex Fridman &#8211; Dario Amodei Transcript</a></p></li><li><p><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">EDRM",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "&#8211; Amodei&#8217;s Essay on </a><em><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">Machines of Loving Grace</a></em></p></li><li><p><a href=\"https://www.cnbc.com/2025/03/17/human-level-ai-will-be-here-in-5-to-10-years-deepmind-ceo-says.html\">CNBC &#8211; DeepMind CEO Predicts Human-Level AI</a></p></li><li><p><a href=\"https://www.startuphub.ai/ai-news/artificial-intelligence/2025/agis-coming-in-5-to-10-years-says-deepmind-ceo-demis-hassabis/\">StartupHub.ai &#8211; AGI in 5&#8211;10 Years, Says Demis Hassabis</a></p></li></ul>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "Ready for ChatGPT-5: Grab a Complete 139 Page Prompting Guide That's a Complete Operating System for Life and Work",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "I told myself I wouldn't do this again. After my 66-page prompt opus in April, I swore I'd keep things shorter. More digestible. Reader-friendly. [Narrator voice: He did not keep things shorter becaus...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "<p><em><strong>I told myself I wouldn't do this again.</strong> After my <a href=\"https://natesnewsletter.substack.com/p/my-prompt-stack-for-work-16-prompts?r=1z4sm5\">66-page prompt opus in April</a>, I swore I'd keep things shorter. More digestible. Reader-friendly. </em></p><p><em>[Narrator voice: He did not keep things shorter because there&#8217;s too much good stuff.]</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!aTYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" width=\"498\" height=\"498\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:498,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2081498,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166864451?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>This collection is ~145 pages long. It contains 39 essential prompts in a detailed Google Doc, many with multiple variants, plus 6 pages of context that explains why prompting has fundamentally changed in 2025.</strong> And yes, there's a table of contents in that Google Doc so you can actually navigate this beast and find exactly what you need.</em></p><p><em>Because my longing to suffer, why did I write another novel-length prompt guide? Because something clicked for me recently. <strong>I realized my prompts and how I talk about them needed to evolve to keep pace with frontier model capabilities. And with ChatGPT-5 on the horizon (rumored for July), I wanted to bring the prompt stack up to snuff with what today&#8217;s models can do.</strong> </em></p><p><em>These aren't random prompts I think might be useful. <strong>These are the 39 prompts I have labored over, technically, repeatedly, as many different ways as I can think of to make them excellent.</strong></em></p><p><em>How? </em></p><ul><li><p><em>By checking them vs. actual published prompting guides (I list 18 of them below) to ensure they adhere to best practices (lots of these have been published since April)</em></p></li><li><p><em>By developing multiple variants to give you different options depending on your level of effort</em></p></li><li><p><em>By choosing a range of prompts that cover the full spectrum of decision-making I see actually cropping up in work (and life)</em></p></li></ul><p><em>That last is key: I find a lot of my earlier prompt work has been a bit constrained by earlier model intelligence levels. I&#8217;ve felt like I needed to constrain to particular job families in the past because earlier models leaned into fairly defined work and task completion assignments. And that has value!</em></p><p><em><strong>But we can do more cool stuff with the newer models. There&#8217;s a chance with these newer models to ask much more ambiguous questions.</strong></em></p><p><em><strong>Like: </strong>When I need to make a decision that will affect the next five years. When I'm staring at feedback that stings and need to figure out what's actually useful in it. When a project is falling apart and I need structured thinking,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "not panic. When I'm trying to learn something new and don't want to waste months on bad approaches.</em></p><p><em>I've been living with these frontier models&#8212;Claude 4, o3 Pro, Gemini 2.5 Pro&#8212;like they're colleagues. Maybe closer than colleagues. <strong>We've developed a working relationship where I know exactly how to activate their best thinking, and they know (through my prompts) exactly what kind of thought partnership I need.</strong></em></p><p><em>Each prompt in this collection does something specific:</em></p><ul><li><p><em><strong>Turn vague anxiety into clear action</strong> (Future Regret Minimizer, Change Readiness Evaluator)</em></p></li><li><p><em><strong>Extract signal from noise</strong> (Dynamic Qualitative Insight Explorer, Strategic Feedback Interpreter)</em></p></li><li><p><em><strong>Force hard choices</strong> (Comprehensive Tradeoff Analyzer, Meeting Killer)</em></p></li><li><p><em><strong>Design better systems</strong> (Database Schema Designer, Automation Opportunity Scanner)</em></p></li><li><p><em><strong>Navigate complex human dynamics</strong> (Stakeholder Navigation Guide, Multi-Perspective Simulator)</em></p></li></ul><p><em>But here's what matters more: <strong>these prompts are really complete thinking systems, not just questions.</strong> They include context setup, phase-by-phase workflows, specific output formats, and iteration loops. They assume the AI is genuinely intelligent and just needs clear structure to be helpful. And that&#8217;s definitely a marker of the time we&#8217;re in&#8212;the blurry, <a href=\"https://www.google.com/search?q=gentle+singularity&amp;oq=gentle+singularity&amp;sourceid=chrome&amp;ie=UTF-8\">gentle singularity</a>.</em></p><p><em><strong>The Google Doc is designed to be scannable and searchable.</strong> Eight major sections. Clear numbering. Purpose statements for each prompt so you know when to use it. You can bookmark it and come back whenever you face that type of challenge. Think of it as your emergency toolkit for complexity.</em></p><h4><em>Here&#8217;s what it looks like:</em></h4><div class=\"native-video-embed\" data-component-name=\"VideoPlaceholder\" data-attrs=\"{&quot;mediaUploadId&quot;:&quot;f31418d2-fca0-4769-8147-275332f2cf0f&quot;,&quot;duration&quot;:null}\"></div><p><em>I've also included something new: detailed notes on how prompting itself has evolved. How specificity works to drive model outputs. How to use the new massive context windows strategically. Why breaking complex tasks into phases isn't about helping the AI anymore&#8212;it's about helping us.</em></p><p><em><strong>These prompts work with any frontier model</strong>&#8212;Claude 4, o3 Pro, o3, Gemini 2.5 Pro, Grok 3. And I&#8217;ve designed them to be built with the grain of the emerging intelligence patterns we&#8217;re seeing, <strong>so they should set you up for GPT-5 later in the summer</strong>. <strong>Yes I said it.</strong> </em></p><p><em><strong>How??</strong> I&#8217;ve cross-referenced these prompts across the different model makers&#8217; prompting guides to make sure they can be used as-is as much as possible. The principles hold because they're based on the commonalities in how these models actually process information, not on platform-specific tricks.</em></p><p><em>Fair warning: this isn't light reading. <strong>Each prompt is densely packed with structure and logic.</strong> But that's the point. When you need to think clearly about something that matters, you don't want a vague suggestion. You want a systematic process that reliably produces insight. And the point is that now you can just take these prompts and be off to the races.</em></p><p><em>If you've been following my prompt journey for the last few months, this is where it's led. If you're new, this is everything I've learned compressed into immediately usable tools. Either way, <strong>these 39 prompts will change how you work with AI&#8212;from occasional assistant to genuine thinking partner.</strong></em></p><p><em>Time to dig in. The prompts are waiting.</em></p><p><em>PS. For those wanting both the full podcast from the video and a nice voice reading the whole post&#8212;I got you covered. Separate audio",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "for the post just below!</em></p><p></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "2 Posts in One: Meta's AI Strategy Looks Desperate + Your Invite to Nate's New AI Discord Community",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "I got lots of feedback on my post from Sunday about a community for paid subscribers.TLDR y&#8217;all want one, and you want it to be on Discord. So I made one! We&#8217;ve got a couple of dozen brave...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "<p>I got lots of feedback on my <a href=\"https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates?r=1z4sm5\">post from Sunday</a> about a community for paid subscribers.</p><p><strong>TLDR </strong>y&#8217;all want one, and you want it to be on Discord. </p><p>So I made one! </p><p>We&#8217;ve got a couple of dozen brave alpha testers already in there and I&#8217;ll throw the link in below here. Hop on in! We have channels for Substack discussion, AI news, AI questions, and I&#8217;m gonna get an AI jobs channel going here soon as well.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!UuK7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" width=\"480\" height=\"360\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:900362,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166843413?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Also have had a fair bit of interest from founding members on my weekly CEO-view posts. A bit more about them:</p><blockquote><p><em>I'm building <strong>short, sharp weekly intelligence briefs for leaders making real decisions with real money.</strong> <br><br>Think 3-4 pages max, packed with insights that usually cost too much from consultants&#8212;AI procurement pitfalls, hidden implementation costs blindsiding CFOs, liability issues, which investment patterns actually correlate with ROI. <br><br>The goal is clear, frank discussion of the issue, specific actionable paths forward, and hard-headed analysis you can take to the bank.</em></p></blockquote><p>You can change <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">your Substack plan here</a> if you&#8217;re like to get the first one (coming Sunday). Substack will prorate you for previous months etc. so if you&#8217;re been subbing for awhile you won&#8217;t be in at full price. </p><blockquote><p><em>Hint: the first one is on a gnarly compliance issue with board liability implications that&#8217;s facing just about every AI startup right now and that kicks in at the beginning of August. Fun times!</em></p></blockquote><p>Besides the link underneath, I&#8217;m throwing a juicy tidbit for fun on whatever the heck Meta is doing right now with those wild acquisition offers (Zuck my DMs are open lmao). Yes this post is pretty blunt about what Zuck is doing so buckle up!</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all the good stuff, and now a community lol</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "The Claude Code Complete Guide: Learn Vibe-Coding & Agentic AI",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "I got Claude Code backwards when I first read about it. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "<p><em><strong>I got Claude Code backwards when I first read about it</strong>. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not what stood out to me after trying it and digging in.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" width=\"1456\" height=\"790\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:233937,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166772125?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>It&#8217;s not just for code guys lol</strong></em></p><p><em>After spending time with Claude Code, I'm convinced <strong>what we have here is effectively a general purpose AI agent</strong> hiding under the guise of just being a coding agent. It's not just a coding agent. <strong>Claude Code is capable of the full spectrum of intelligence. It just happens to hide in the terminal</strong>, and that makes it seem scary to people who don't use terminals to code. And let's be honest, that's most of us.</em></p><p><em>What I find really fascinating is that when I started to finally use Claude Code, <strong>it made the decision to upgrade to the Max tier so much easier. </strong>I had enough experience with the intelligence that Claude Code was bringing that it felt intuitive. And here's what really blew my mind: <strong>with Claude Code, you don't have a traditional development environment</strong>. Like, I thought that would be a mistake at first. Who wants to code without the IDE? This is not a video game.</em></p><p><em>But Claude will edit files, create files, but won't necessarily show you all of it the way it does in a typical development environment. You might think, <strong>what a terrible design choice</strong>. And you and I would both be wrong about that. <strong>It turns out that abstracting you above code level helps you focus on the strategy and the intent of the project.</strong></em></p><p><em>And Claude Code has the muscle to actually operate at both levels: strategic and execution. <strong>I strongly suspect this power comes from the fact that there aren't the same token constraints you'd have if you installed Claude in another tool like Cursor</strong>. Because Anthropic can control the whole experience, they can make Claude Code work exactly the way they want. It feels a lot like an internal development tool that got out into the wild, which is exactly what it is.</em></p><p><em>Look, I'm not a fantastically experienced senior engineer. My background is different. I am a hacky scrappy founder, producty kind of person. <strong>I am writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "this guide for everyone, not just engineers.</strong> I know enough about coding and product / project management to see that Claude Code is so much more than a coding tool, and we misunderstand it when we think about it just as a coding tool.</em></p><p><em>When I was working on my personal website&#8212;which, for those of you who have been there, I freely agree is terrible and awful lmao&#8212;Claude Code legitimately transformed the experience. Just much less pain. It was so much easier to get Claude Code to work on this project than in any other environment I'd played with. I tried Windsurf, played with it in Cursor a little bit, played with it in o3 (which was the worst example). But Claude Code was different. <strong>It answered my questions intelligently, laid out a plan I could understand, and when it decided to build, it built largely correctly from the start.</strong></em></p><p><em><strong>This is the first time using Claude Code that I've been able to actually get a polished, professional, not mid-looking AI output.</strong> And I did it with an unusual workflow&#8212;80% Claude Code, then a mixture of o3 for color research and Cluely for UI feedback (I&#8217;ll dig into it below). The magic was treating Claude Code not as a coding tool, but as a general purpose agent that happens to live in your terminal.</em></p><p><em>If you're not an engineer, don't let the terminal scare you. Think about it as effectively a chatbot that can talk to the files on your computer. That's really it.</em></p><p><em>And that's exactly what this guide is about. Over the next few thousand words, I'm going to show you everything I've learned about Claude Code&#8212;from that initial $100/month subscription decision that made me grimace, to building complete applications in conversation, to discovering workflows that multiply productivity by 10x or more.</em></p><p><em>You'll learn the 5-minute setup that changes everything, learn a bit more about the art of vibe coding (<a href=\"https://natesnewsletter.substack.com/p/the-vibe-coding-bible-how-to-build\">complete guide here</a>), understand why Claude Code is fundamentally different from GitHub Copilot or Cursor, and see real examples of how teams are achieving 5x productivity gains. </em></p><p><em>I'll also share my interesting three-tool orchestra approach that finally broke through AI's \"mid\" design problem, explain some of the dark side (Claude Code does add up in cost if you use it a lot), and show you some advanced patterns that can help turn Claude Code into your universal terminal for all knowledge work.</em></p><p><em>Why? All this matters because we're at an inflection point. As Kent Beck said after 52 years of coding, <a href=\"https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent\">90% of traditional programming skills are becoming commoditized</a> <strong>while the remaining 10% becomes worth 1000x more</strong>. The developers and teams who understand this shift&#8212;who learn to orchestrate AI rather than just code alongside it&#8212;will thrive in this new landscape. And Claude Code gives us a chance to demonstrate that remaining 10% of skills with a relatively strong junior coding partner.</em></p><p><em>Whether you're a seasoned developer looking to level up your AI pair programming, a product manager wanting to prototype without engineering bottlenecks,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "or someone who's never coded but has ideas to build, this guide will be a helpful reference that shapes how you approach using AI agents for development. At the end of the day, Claude Code isn't just another coding assistant. It's the beginning of <strong>a new era where the terminal becomes a conversational interface for turning ideas into reality</strong>.</em></p><p><em>Ready to see what's actually possible? Let's dive in&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "The Anthropic Ruling: A Roadmap for AI's Copyright Future",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "Hot off the presses! Had to get this one out today because the ruling is such a big deal.Hope you enjoy, and back to our regular programming soon&#8230;Subscribers get all these pieces!I've been follo...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "<p><em>Hot off the presses! Had to get this one out today because the ruling is such a big deal.</em></p><p><em>Hope you enjoy, and back to our regular programming soon&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>I've been following AI copyright cases closely, and Judge William Alsup just handed down what I believe will be one of THE landmark AI decisions we'll see this decade. The federal court's split ruling in <em>Bartz v. Anthropic</em> does something remarkable: it validates AI training as fair use while simultaneously condemning the piracy that often enables it. This isn't just a win or loss for Anthropic&#8212;it's a blueprint for how courts will likely approach the dozens of AI copyright cases working their way through the system.</p><h2>The Solomon's Choice of AI Copyright</h2><p>I find Alsup's decision fascinating because it splits the baby with surgical precision. Yes, training Claude on millions of books constitutes fair use. No, downloading those same books from pirate sites doesn't get a free pass. The distinction matters because it fundamentally reshapes how AI companies must think about data acquisition.</p><p>The judge's reasoning on fair use particularly struck me. He describes AI training as \"quintessentially transformative,\" comparing it to how human writers learn from reading. \"Everyone reads texts, too, then writes new texts,\" Alsup writes. \"To make anyone pay specifically for the use of a book each time they read it, each time they recall it from memory, each time they later draw upon it when writing new things in new ways would be unthinkable.\"</p><p>This analogy&#8212;AI as reader learning to write&#8212;provides the conceptual foundation that I think AI companies have been desperately seeking. It's not about copying; it's about learning patterns, understanding language, and creating something fundamentally new.</p><h2>The Million-Dollar Pivot</h2><p>Here's where I think Anthropic's story gets really interesting. After building their initial models on pirated content from Books3, Library Genesis, and other dubious sources, the company made a dramatic shift in 2024. They hired Tom Turvey, the former head of Google's book-scanning project, with a mandate to obtain \"all the books in the world\" through legitimate means.</p><p>Anthropic then spent millions of dollars purchasing physical books&#8212;many second-hand&#8212;which they proceeded to slice from their bindings and scan into digital format. The physical books were destroyed in the process, but the digital copies were ruled as legitimate fair use. This expensive pivot from piracy to purchase reveals something I've been saying for a while: AI companies can afford to do this right. They're choosing not to.</p><p>The court explicitly noted this financial capability, observing that Anthropic's later purchases of books they'd previously pirated \"will not absolve it of liability for the theft but it may affect the extent of statutory damages.\" Translation: we see you had the money all along.</p><h2>What This Means for Authors</h2><p>I think this ruling offers a glimmer of hope for authors who've watched AI companies feast on their work without compensation. While the fair use ruling means authors",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "can't stop AI training entirely, the court's condemnation of piracy and validation of legitimate book purchases creates real market incentives.</p><p>Consider what Anthropic's spending reveals: they paid millions for books, often buying used copies at market rates. This money flows back into the book ecosystem&#8212;to retailers, distributors, and ultimately supporting the market for authors' works. If every AI company followed this model instead of scraping pirate sites, we'd see a substantial new revenue stream for the publishing industry.</p><p>Moreover, the ruling's emphasis on transformation rather than reproduction protects authors' core market. The court stressed that it matters whether AI systems \"directly compete with the originals.\" Since Claude doesn't spit out verbatim passages from novels, the technology complements rather than replaces human authorship.</p><p>I see this as establishing a sustainable equilibrium: AI companies must pay for access to training materials, supporting the creative economy, while authors benefit from AI tools that help readers discover and engage with human-written works.</p><h2>The Domino Effect</h2><p>This ruling's impact extends far beyond Anthropic's legal troubles. I'm watching several major cases that will likely cite this precedent:</p><p><strong>The OpenAI Cases</strong>: Multiple lawsuits against OpenAI, including from the Authors Guild and various publishers, hinge on similar fair use arguments. Alsup's framework&#8212;distinguishing between training use and acquisition methods&#8212;gives OpenAI a potential path to victory, assuming they can demonstrate legitimate data sourcing.</p><p><strong>Kadrey v. Meta</strong>: The lawsuit against Meta for training LLaMA on Books3 (the same dataset Anthropic used) now faces an interesting precedent. Meta might win on fair use for training but could still face liability if they retained pirated materials in a permanent library.</p><p><strong>The Stability AI Litigation</strong>: Visual AI companies face additional complexities, but I think Alsup's \"transformative use\" reasoning could extend to image generation models that learn artistic styles without reproducing specific works.</p><h2>The New Compliance Playbook</h2><p>From my reading of Alsup's ruling, he's effectively created a compliance roadmap for AI companies:</p><ol><li><p><strong>Training on copyrighted works? Probably fine</strong>, as long as your model doesn't reproduce those works verbatim.</p></li><li><p><strong>Building a permanent library of pirated content? Definitely not fine</strong>, even if you only use it for training.</p></li><li><p><strong>Want to avoid liability? Buy the books</strong>. Or license them. Or use legitimately free sources. But stop pretending piracy is a necessary evil.</p></li><li><p><strong>Already have pirated content? Delete it after training</strong>. The court's distinction between temporary training copies and permanent library storage offers a potential safe harbor.</p></li></ol><h2>The Billion-Dollar Question</h2><p>With damages still to be determined, I calculate Anthropic faces potential liability in the billions. Statutory damages for willful infringement can reach $150,000 per work, and we're talking about millions of books. This creates a powerful deterrent effect: train responsibly or face existential financial risk.</p><p>The message I'm taking from this ruling is clear: the transformative nature of your technology doesn't give you a free pass to transform other people's property into your training data through illegal means. Fair use protects the learning, not the theft.</p><p>As more courts adopt Alsup's framework, I expect we'll see a fundamental shift in how AI companies approach data acquisition. The days of \"download first, ask permission never\" are ending. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "future belongs to companies willing to do what Anthropic eventually did: open their wallets and do things the right way.</p><p>Even if it means destroying a few million books in the process&#8212;at least authors got paid for them.</p><h2>The Bigger Picture</h2><p>I should note that while Alsup's ruling gives us crucial clarity, we're still in the early innings of this game. This is just one federal district court's take, and I've learned to be cautious about declaring victory based on a single decision. We've got dozens of similar cases working through different courts, each with their own judges who might see things differently.</p><p>I'm particularly aware that the Northern District of California doesn't speak for the entire country. We're already seeing circuit splits on related AI issues&#8212;the Ninth Circuit requires \"actual knowledge\" for contributory infringement while the Second Circuit only needs \"reason to know.\" That's a big difference when we're talking about platforms hosting AI tools.</p><p>What's more, this ruling really only addresses text-based AI training. I'm left wondering how courts will handle visual AI systems, code generation, or the myriad other AI applications we're seeing. Fair use is notoriously fact-specific, and what works for Claude might not work for DALL-E or Copilot.</p><p>I expect we'll see Anthropic appeal to the Ninth Circuit, which could modify or even reverse parts of Alsup's reasoning. And honestly? I think we'll eventually need either the Supreme Court to step in and create nationwide clarity, or Congress to pass actual AI legislation. Neither seems likely in the near term, which means we're in for years of case-by-case battles as the law slowly catches up to the technology.</p><p>Still, Alsup's decision represents real progress. It's the first federal court to tackle these questions head-on, and that matters&#8212;even if it's just the opening chapter of a much longer story.<br></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ftVE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1363491,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166769339?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "From ChatGPT to Cluely: Riding the $120M Proactive AI Wave",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "Cheating, cheating, cheating. That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "<p><em><strong>Cheating, cheating, cheating.</strong> That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do interviews anymore (they were asking that before too lol).</em></p><p><em>Look, I get it. The internet is the greatest narrative simplification machine ever built.</em></p><p><em>Here's the thing: I was playing around with Cluely before it got all hype-y. Not to cheat&#8212;I'm way past needing help with homework (though my kids might disagree). I picked it up because I was curious about this whole \"proactive AI\" thing everyone keeps talking about. What I found wasn't some elaborate cheating scheme&#8212;despite the aggressive marketing. It was something way more interesting: <strong>a glimpse of what happens when AI stops waiting for permission to help.</strong></em></p><p><em>And honestly? The whole cheating discourse is missing the point so badly it's almost painful to watch. <strong>Everyone is playing into Cluely&#8217;s hands and they love to see it guys.</strong> </em></p><p><em><strong>Forgive a little humor, but Cluely is cheating at cheating.</strong> They hype the cheating brand to draw in their target IC customers (Gen Z and Gen Alpha). But while everyone's arguing about whether students should use AI for essays (and earning them clicks), Cluely <strong>quietly built something that changes how we think about human-AI collaboration entirely.</strong> They're not just making another chatbot&#8212;<strong>they're showing us what Level 2 proactive AI actually looks like in the wild.</strong></em></p><p><em>I almost didn't write this piece. It felt too... nuanced? Too complex for the current moment where most of the news cycle is dominated by \"AI is cheating and we&#8217;re all doomed\" or (to a lesser extent) &#8220;a16z is out to lunch for funding this.&#8221; But when I watch a 21-year-old dropout secure venture funding based on a tool that most people completely misunderstand, I realize I at least need to try to articulate what&#8217;s going on under the marketing hype and doom-and-gloom.</em></p><p><em>Here's what I want to dig into: </em></p><ol><li><p><em><strong>Why</strong> <strong>Cluely's $120M valuation makes perfect sense from a startup strategy perspective</strong> (even if their AI is pretty mid)</em></p></li><li><p><em><strong>How Cluely helps us think about agentic AI</strong> <strong>and offers an early picture into where job skills</strong> <strong>are going next</strong> (especially with those juicy B2B contracts they&#8217;ve landed)</em></p></li><li><p><em><strong>How</strong> <strong>their UX innovation reveals where agentic AI is actually heading</strong>, and why the cognitive fitness implications are way more profound than anyone's talking about</em></p></li><li><p><em><strong>Plus</strong>, <strong>if you're building anything in the AI space, understanding what Cluely got right about distribution and timing might be the most important lesson of 2025.</strong></em></p></li><li><p><em><strong>And as a bonus, what we can learn from Cluely&#8217;s leaked system prompt. </strong>So we&#8217;ll get some good prompt analysis in at the end for you prompting geeks!</em></p></li></ol><p><em>The real story isn't about cheating. It's about what happens when we stop asking \"How can AI help me find the answers?\" and start asking and <strong>&#8220;How can I use proactive AI to extend my thinking?&#8221;</strong> That shift&#8212;from reactive to proactive intelligence&#8212;is worth way more than $15 million. And I wrote 31 pages to show you why",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "lol</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "Software 3.0 vs AI Agentic Mesh: Why McKinsey Got It Wrong",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "<p></p><p><em>This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out in real time.</em></p><p><em>Fighter on one side: <strong>Andrej Karpathy&#8217;s &#8220;Software 3.0&#8221;</strong> (natural language as the programming interface, grounded in actual experience building Autopilot)</em></p><p><em>On the other: <strong>McKinsey&#8217;s &#8220;AI Agentic Mesh&#8221;</strong> (distributed autonomous agents, grounded in&#8230; PowerPoint slides lol)</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!WE-I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1994788,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em>Fun aside, this isn&#8217;t just another framework war&#8212;<strong>it&#8217;s a perfect case study in how the same technology gets interpreted completely differently depending on whether you&#8217;re building or consulting</strong>. It&#8217;s been really fun to dive into this one because the stakes are real. We&#8217;re watching billions get allocated based on these competing visions, and only one of them is going to survive contact with reality.</em></p><p><em>Here&#8217;s the thing: I normally keep the <strong>deep strategic breakdowns behind the paywall</strong>, but this battle felt too important to gate. <strong>Consider this a taste of what subscribers get every week</strong>&#8212;the kind of analysis that cuts through the noise and shows you what&#8217;s actually happening.</em></p><p><em>Because while executives are choosing sides, there&#8217;s <strong>real transformation happening in the quiet corners where developers are shipping with AI</strong>.</em></p><p><em>So let&#8217;s roll up our sleeves and dig in. You&#8217;ll get:</em></p><ul><li><p><em>The <strong>real story</strong> behind both visions</em></p></li><li><p><em>Why the <strong>communication gap between builders and executives</strong> keeps creating expensive disasters</em></p></li><li><p><em>And most importantly, <strong>the first practical framework for translating AI constraints into business strategy that actually works</strong></em></p></li></ul><p><em><strong>This is the clarity I can&#8217;t find so, I wrote it up.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h1><strong>The TLDR</strong></h1><p>Two radically different visions of AI's future are competing for executive attention, and the choice between them will determine which organizations thrive in the coming \"decade of agents.\"</p><h2>The Two Visions</h2><p><strong>Karpathy's Software 3.0</strong> represents a fundamental shift where natural language becomes the primary programming interface. Based on his experience building Tesla's Autopilot, Karpathy describes AI as \"brilliant interns with perfect recall but no judgment\"&#8212;powerful tools requiring human oversight. His vision acknowledges critical limitations like \"jagged intelligence\" (excelling at complex tasks while failing at simple ones) and \"anterograde amnesia\" (no memory between conversations). The focus is on <strong>augmentation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "through human-AI collaboration</strong>, not replacement.</p><p><strong>McKinsey's AI Agentic Mesh</strong> promises enterprise-wide networks of autonomous agents coordinating seamlessly across organizations. This consultant-crafted vision features \"composable\" systems, \"distributed intelligence,\" and \"governed autonomy\"&#8212;architectural concepts that sound impressive in boardrooms but violate fundamental technical principles.</p><h2>Why the Technical Community Rejects McKinsey's Vision</h2><p>Practitioners who actually build AI systems universally dismiss the agentic mesh. Cognition (creators of Devin) concluded that multi-agent systems \"only result in fragile systems\" because decision-making becomes too dispersed and context can't be shared effectively. Anthropic found their multi-agent systems use \"15&#215; more tokens than chats\" and struggle with coordination. The technical reality: successful AI implementations require centralized control and tight integration&#8212;the opposite of McKinsey's distributed mesh.</p><h2>The Executive Communication Crisis</h2><p>The gap between technical reality and executive understanding has created a crisis of expensive failures. <strong>Klarna's AI disaster</strong> exemplifies this pattern: the company claimed its AI handled 700 agents' worth of work and saved $40 million annually, only to later admit they'd \"gone too far\" and quietly rehired human workers due to \"lower quality\" service.</p><p>This pattern repeats across industries&#8212;IBM Watson's $62 million failure at MD Anderson, McDonald's abandoned AI drive-through, Air Canada's policy-inventing chatbot. Each failure stems from executives chasing automation fantasies instead of understanding AI's true capabilities and constraints.</p><h2>The Path Forward</h2><p>Organizations need <strong>technically grounded executive narratives</strong> that translate AI capabilities into business terms without losing nuance. Successful approaches include:</p><ul><li><p><strong>Operational analogies</strong>: Frame LLMs as \"brilliant interns\" rather than using technical jargon</p></li><li><p><strong>Financial constraints</strong>: Show real costs&#8212;processing large datasets requires breaking them into thousands of chunks, costing hundreds of thousands in compute</p></li><li><p><strong>Domain-specific examples</strong>: Demonstrate specific failure modes in the executive's industry</p></li><li><p><strong>Progressive disclosure</strong>: Let pilots reveal limitations naturally through experience</p></li></ul><h2>Why This Matters Now</h2><p>Software 3.0 isn't future speculation&#8212;it's today's reality. Developers using tools like Cursor AI report 10-100x productivity gains on specific tasks. Startups with tiny teams now compete with products that previously required massive engineering organizations. The transformation is happening at AI speed, not traditional enterprise timelines.</p><h2>The Binary Choice</h2><p>Organizations face a stark choice: embrace AI as a collaborative amplifier of human capability, or chase consultant fantasies promising autonomous replacement. The 4% of companies generating substantial AI value share common traits&#8212;they focus on augmentation, invest heavily in human-AI workflows, and measure success through value creation rather than cost reduction.</p><p><strong>The wave of Software 3.0 is breaking now.</strong> Organizations that catch it with clear eyes will build sustainable advantages. Those chasing McKinsey's distributed dreams will join the graveyard of failed AI transformations, wasting billions while competitors build real value through human-AI collaboration.</p><p>The future belongs to \"Iron Man suits\" for knowledge work&#8212;AI that amplifies human capability rather than replacing the human inside.</p><h1><strong>Software 3.0 and the Executive Delusion: Why Karpathy's Vision Matters and McKinsey's Doesn't</strong></h1><h2><strong>I. Opening: The Tale of Two Visions</strong></h2><p>This week at Y Combinator's AI Startup School, Andrej Karpathy stood before a room of builders and declared that we've entered the era of Software 3.0&#8212;where natural language becomes the primary programming interface. \"The hottest new programming language is English,\" he said, describing a world where anyone who can clearly articulate ideas can create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "software. His vision, grounded in years of replacing traditional code with neural networks at Tesla, represents a profound shift in how we build and interact with technology.</p><p>Meanwhile, in boardrooms across the Fortune 500, McKinsey consultants are selling executives on something called the \"AI Agentic Mesh\"&#8212;a grand vision of autonomous agents coordinating seamlessly across enterprises, promising to finally deliver the ROI that has eluded 78% of companies dabbling in generative AI. Their PowerPoints paint a picture of composable, distributed, vendor-agnostic systems where hundreds of AI agents collaborate like a perfectly choreographed symphony.</p><p>These two visions couldn't be more different. One comes from the trenches of building Autopilot at Tesla, where Karpathy watched neural networks progressively eat away at 300,000 lines of C++ code. The other comes from consulting frameworks designed to sound strategic in executive briefings. One acknowledges fundamental limitations&#8212;what Karpathy calls \"jagged intelligence\" and \"anterograde amnesia.\" The other handwaves away technical constraints with promises of \"governed autonomy\" and \"layered decoupling.\"</p><p>The gap between these visions isn't academic. It's measured in billions of dollars misdirected, thousands of careers disrupted, and countless opportunities missed. When Klarna's CEO boasted about replacing 700 customer service agents with AI, saving $40 million annually, the tech press celebrated. Months later, he quietly admitted they'd \"gone too far,\" delivering \"lower quality\" service and hiring humans back. The company had fallen for the same delusion McKinsey now packages as revolutionary architecture.</p><p>This pattern repeats across industries. IBM Watson consumed $62 million at MD Anderson before being abandoned. McDonald's discontinued its AI drive-through after three years of adding bacon to ice cream orders. Air Canada faced legal troubles when its chatbot invented refund policies. Each failure shares the same root cause: executives chasing consultant fantasies instead of understanding technical reality.</p><p>The tragedy is that real transformation is happening&#8212;just not the kind McKinsey sells. At companies embracing Karpathy's vision of \"partial autonomy,\" developers using tools like Cursor AI report 10-100x productivity gains for specific tasks. They're not replacing humans; they're amplifying human capability. They're not building autonomous agent meshes; they're creating tight feedback loops between human creativity and AI generation.</p><p>But this nuanced reality doesn't sell well in boardrooms. It requires admitting that AI has \"jagged intelligence\"&#8212;brilliant at complex tasks while failing at simple ones. It means accepting that large language models are, in Karpathy's memorable phrase, \"stochastic simulations of people\" with \"anterograde amnesia,\" unable to remember or learn between conversations. It demands investment in people and processes, not just technology.</p><p>The cost of this communication failure compounds daily. While technical teams know that multi-agent systems \"only result in fragile systems\" (as Cognition, creators of Devin, learned the hard way), executives allocate budgets toward McKinsey's distributed mesh dreams. While practitioners understand that AI agents use \"15&#215; more tokens than chats\" (per Anthropic's experience), leaders expect cost savings from wholesale automation. The mismatch between expectation and reality guarantees expensive failure.</p><p>We stand at an inflection point. Karpathy isn't describing some distant future&#8212;Software 3.0 is breaking through now. Natural language interfaces are transforming how we build software today. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "question isn't whether this transformation will happen, but whether organizations will navigate it successfully or crash against the rocks of consultant-crafted delusions.</p><p>This is a story about two fundamentally different ways of understanding AI's impact on work and business. One path, illuminated by builder wisdom and technical truth, leads to genuine augmentation and value creation. The other, paved with PowerPoint promises and architectural astronautics, leads to the same graveyard of failed digital transformations that litters corporate history.</p><p>As we enter what Karpathy calls \"the decade of agents,\" the stakes couldn't be higher. The organizations that thrive will be those that reject the siren song of autonomous replacement and embrace the messier, more honest reality of human-AI collaboration. They'll build with humility about limitations while harnessing genuine capabilities. Most importantly, they'll listen to builders over consultants, choosing technically grounded evolution over executive-friendly revolution.</p><p>The pages that follow will unpack these two visions in detail, explore why executives keep falling for automation fantasies, and chart a path toward narratives that bridge the gap between technical reality and business strategy. Because in the end, Software 3.0's promise isn't about replacing human intelligence&#8212;it's about amplifying it. But only if we're honest enough to see it clearly.</p><h2><strong>II. Understanding Software 3.0: What Karpathy Actually Said</strong></h2><p>To understand why Karpathy's Software 3.0 vision matters, we need to grasp both its revolutionary implications and its refreshing honesty about limitations. Unlike the consultant-speak flooding executive inboxes, Karpathy's framework emerges from hard-won experience replacing traditional code with neural networks at Tesla's Autopilot division. His presentation at Y Combinator wasn't selling a product&#8212;it was sharing a profound shift in how we create and interact with software.</p><h3><strong>The Evolution Framework</strong></h3><p>Karpathy's Software 3.0 thesis rests on understanding two previous paradigm shifts. Software 1.0 represents traditional programming&#8212;the world we've inhabited since computing began. Developers write explicit instructions in languages like Python, C++, or Java, specifying exact algorithms, control flows, and data structures. Every behavior is deliberately coded, debugged line by line, and maintained through human understanding. This is the programming most people recognize: explicit, deterministic, and fully under human control.</p><p>Software 2.0 emerged from a radical insight Karpathy articulated in 2017, based on his experience at Tesla. Instead of writing code to detect stop signs or identify lane markings, engineers began curating datasets and designing neural network architectures. The actual \"program\" became millions or billions of learned parameters&#8212;weights discovered through optimization algorithms rather than human reasoning. As Karpathy watched at Tesla, neural networks progressively consumed traditional code. Features requiring thousands of lines of C++ were replaced by learned behaviors that performed better with less explicit programming.</p><p>Software 3.0 represents the next leap: natural language becoming the primary programming interface through Large Language Models. As Karpathy explains: \"What's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0.\"</p><p>In this new paradigm, prompts replace code as the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "primary way to direct computational behavior. LLMs serve as interpreters that understand and execute natural language instructions. Context windows act as working memory. Most radically, programming becomes universally accessible&#8212;anyone who can clearly express ideas can create software.</p><h3><strong>LLMs as a New Type of Computer</strong></h3><p>Karpathy's most profound insight reframes LLMs not as tools but as fundamentally new computational systems. \"When I use ChatGPT,\" he notes, \"I feel like I'm talking to an operating system through the terminal.\" This isn't mere metaphor&#8212;he maps traditional computing concepts onto language-based processing:</p><p>The LLM functions as the CPU, the core processing unit executing instructions. The context window serves as RAM, providing short-term working memory for active computation. Prompts become programs&#8212;natural language instructions directing behavior. Tokens represent the fundamental units of data, like bytes in traditional computing.</p><p>This reconceptualization helps explain why LLMs feel qualitatively different from previous AI systems. They're not just pattern matchers or classifiers; they're general-purpose computers that happen to process natural language instead of binary code.</p><p>Karpathy extends this thinking through three powerful infrastructure analogies. First, he positions AI as \"the new electricity\"&#8212;utility infrastructure with massive capital expenditure for training (like building power plants) and operational costs for serving (like distribution). The pay-per-token API model mirrors metered electricity billing, making AI universally accessible.</p><p>Second, he compares LLM training to semiconductor fabrication. Both require specialized hardware, massive capital investment ($100M+ for frontier models), and produce standardized products used across industries. Like chip fabs, AI training concentrates in a few players due to economies of scale.</p><p>Third, and most provocatively, he positions LLMs as operating systems for AI applications. They provide standard interfaces (chat, completion APIs), manage resources (context, compute), support \"applications\" built on top (agents, tools), and abstract complexity from end users. This OS metaphor explains why platform dynamics are emerging, with developers building atop foundation models rather than training their own.</p><h3><strong>The Critical Limitations Karpathy Acknowledges</strong></h3><p>Unlike McKinsey's boundless optimism, Karpathy's framework explicitly acknowledges fundamental limitations. He describes LLMs as \"stochastic simulations of people, with a kind of emergent 'psychology.'\" This framing captures both their human-like capabilities&#8212;reasoning patterns, creative responses, contextual understanding&#8212;and their decidedly non-human failure modes.</p><p>The first major limitation is what Karpathy coined as \"jagged intelligence.\" He explains: \"The word I came up with to describe the (strange, unintuitive) fact that state of the art LLMs can both perform extremely impressive tasks (e.g. solve complex math problems) while simultaneously struggle with some very dumb problems.\"</p><p>This jaggedness manifests in bewildering ways. An LLM might solve graduate-level mathematics while failing at \"which is bigger, 9.11 or 9.9?\" It can write sophisticated code but struggle with basic counting. It demonstrates deep knowledge while making elementary logical errors. This differs fundamentally from human intelligence development, where capabilities typically build coherently from simple to complex.</p><p>The second critical limitation is \"anterograde amnesia.\" Karpathy notes: \"LLMs are a bit like a coworker with Anterograde amnesia - they don't consolidate or build long-running knowledge or expertise once training is over and all they have is short-term memory (context window).\"</p><p>This creates fundamental constraints: no learning from experience across",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "sessions, no building personalized understanding over time, constant need to re-establish context, and knowledge frozen at training cutoff. Every conversation starts fresh, with no memory of previous interactions or ability to improve based on feedback.</p><p>These limitations aren't bugs to be fixed but fundamental characteristics of current architecture. Karpathy suggests we need new learning paradigms&#8212;perhaps \"System Prompt Learning\" where LLMs modify their own instructions&#8212;but acknowledges we're not there yet.</p><h3><strong>Partial Autonomy and the Generation-Verification Loop</strong></h3><p>Rather than chasing full automation dreams, Karpathy advocates for \"partial autonomy\"&#8212;systems that augment human capabilities while maintaining oversight. He demonstrates this through Cursor AI's \"autonomy slider,\" showing graduated levels of AI assistance:</p><ul><li><p>Tab completion: Minimal AI assistance for code completion</p></li><li><p>Cmd+K: Targeted code modifications with human direction</p></li><li><p>Cmd+L: File-level transformations with AI planning</p></li><li><p>Cmd+I: Maximum autonomy agent mode</p></li></ul><p>This graduated approach mirrors autonomous vehicle development, where Level 2-3 automation proves more practical than jumping to Level 5. It acknowledges that different tasks require different levels of AI involvement and human oversight.</p><p>Central to Software 3.0 is the generation-verification loop&#8212;rapid iteration between fast AI generation, efficient human verification, and iterative refinement. Karpathy emphasizes this loop as key to practical AI applications, making AI a collaborative partner rather than replacement.</p><p>He describes his own workflow transformation: \"Most of my 'programming' is now writing English (prompting and then reviewing and editing the generated diffs), and doing a bit of 'half-coding' where you write the first chunk of the code you'd like, maybe comment it a bit so the LLM knows what the plan is, and then tab tab tab through completions.\"</p><p>This represents a fundamental shift in skills. Natural language articulation becomes as important as traditional coding. Code review and verification matter more than initial writing. The ability to iterate quickly and recognize good solutions becomes paramount.</p><h3><strong>From Vision to Reality</strong></h3><p>Karpathy grounds his framework in concrete examples. He extensively demonstrates Cursor as the exemplar Software 3.0 application, showing how \"vibe coding\"&#8212;describing desired functionality in natural language&#8212;produces working code. He cites menugen.app, which converts restaurant menu text into visual designs, as pure natural language programming in action.</p><p>But he's equally clear about infrastructure needs. He recommends creating \"LLMs.txt\" files&#8212;AI-readable summaries of codebases&#8212;recognizing that \"HTML is not very parseable for LLMs.\" He discusses tools like Gitingest that convert repositories into LLM-digestible formats. These practical details reveal someone building with these technologies, not just theorizing about them.</p><p>Most importantly, Karpathy's vision acknowledges the messy reality of technological change. Software 3.0 isn't replacing previous paradigms&#8212;it's adding a powerful new layer. Professional developers will need all three paradigms, choosing the right tool for each problem. The democratization of programming doesn't eliminate the need for expertise; it changes what expertise looks like.</p><p>This honesty about capabilities and limitations, grounded in practical experience, makes Karpathy's framework genuinely useful. While McKinsey promises frictionless agent meshes, Karpathy offers something more valuable: a realistic path forward that acknowledges both the transformative potential and inherent constraints of AI systems.</p><p>Software 3.0 is happening now. The question isn't whether natural language will become a primary programming interface&#8212;it already is for many developers. The question is whether",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "organizations will embrace this reality with clear eyes or chase consultant fantasies. Karpathy's framework, born from building rather than selling, provides the clarity needed to choose wisely.</p><h2><strong>III. The McKinsey Mirage: Agentic Mesh Deconstructed</strong></h2><p>While Karpathy offers builder wisdom about AI's real capabilities and constraints, McKinsey sells executives a radically different vision: the AI Agentic Mesh. This framework promises to solve the \"gen AI paradox\"&#8212;where 78% of companies use generative AI but see minimal bottom-line impact&#8212;through an enterprise-wide architectural paradigm enabling autonomous agents to coordinate seamlessly across organizations. The gap between this consulting fantasy and technical reality reveals why so many AI initiatives fail.</p><h3><strong>What McKinsey Promises</strong></h3><p>McKinsey's agentic mesh emerged as their answer to widespread AI disappointment. Their diagnosis seems reasonable: companies struggle because they deploy AI in isolated pockets rather than integrated systems. Their solution, however, veers into architectural astronautics.</p><p>The framework rests on five design principles that sound impressive in boardrooms:</p><p><strong>Composability</strong>: Any agent, tool, or LLM can be plugged in without system rework. McKinsey envisions a world where organizations mix and match AI components like Lego blocks, seamlessly integrating \"custom-built and off-the-shelf agents within a unified framework.\"</p><p><strong>Distributed Intelligence</strong>: Tasks are decomposed and resolved by cooperating agent networks. Instead of monolithic systems, McKinsey proposes swarms of specialized agents that somehow coordinate to solve complex problems.</p><p><strong>Layered Decoupling</strong>: Logic, memory, orchestration, and interfaces are separated for maximum modularity. Each layer can be independently updated or replaced without affecting others.</p><p><strong>Vendor Neutrality</strong>: All components can be independently updated or replaced. No lock-in, no dependencies&#8212;just frictionless interchangeability.</p><p><strong>Governed Autonomy</strong>: Agent behavior is controlled via embedded policies and permissions. Autonomous yet controlled, independent yet coordinated&#8212;McKinsey promises to square this circle.</p><p>The vision culminates in \"large-scale, intelligent agent ecosystems\" operating \"safely and efficiently\" across the enterprise. Hundreds of agents would collaborate autonomously, sharing context and coordinating decisions while remaining modular and replaceable. It's a CTO's dream and an engineer's nightmare.</p><h3><strong>Why Technical Practitioners Call It \"Executive Speak\"</strong></h3><p>The technical community's response to McKinsey's agentic mesh has been overwhelmingly negative, and for good reason. Practitioners who actually build AI systems recognize it as a prime example of consulting firms packaging buzzwords without understanding fundamental constraints.</p><p>The most damning critique comes from Cognition, creators of Devin, one of the most advanced AI coding agents available. Through painful experience, they've concluded that \"multi-agent architectures\" result in \"fragile systems\" where \"decision-making ends up being too dispersed and context isn't able to be shared thoroughly enough between the agents.\" Their verdict is unequivocal: multi-agent systems \"only result in fragile systems\" in 2025.</p><p>This isn't theoretical skepticism&#8212;it's hard-won wisdom. Cognition discovered that successful agent systems require two principles that directly contradict McKinsey's distributed mesh vision: \"Share context, and share full agent traces, not just individual messages\" and \"Actions carry implicit decisions, and conflicting decisions carry bad results.\" These principles demand tight integration and centralized coordination&#8212;the opposite of McKinsey's loosely coupled, distributed architecture.</p><p>Anthropic's experience building their Research feature provides additional evidence. Despite massive engineering investment and constrained scope, they found severe limitations. Their multi-agent system uses \"15&#215; more tokens than chats,\" only works",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "for tasks with \"heavy parallelization,\" and struggles because \"most coding tasks involve fewer truly parallelizable tasks than research.\" Even with world-class engineers and focused application, they acknowledge that \"LLM agents are not yet great at coordinating and delegating to other agents in real time.\"</p><p>The technical problems compound exponentially with scale. McKinsey handwaves critical challenges that have no known solutions:</p><p><strong>Context Sharing</strong>: How do distributed agents maintain coherent understanding across organizational boundaries? Cognition provides a telling example: one agent builds a \"Super Mario Bros\" background while another builds an incompatible bird sprite, leaving the final agent unable to reconcile the mismatch. Now imagine this problem multiplied across hundreds of enterprise agents.</p><p><strong>Coordination Complexity</strong>: Each additional agent exponentially increases overhead. With McKinsey envisioning enterprise-wide deployments, the coordination problem becomes computationally intractable. Anthropic warns that \"one step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes.\" In a distributed mesh, these failures cascade catastrophically.</p><p><strong>Security Vulnerabilities</strong>: Researchers have identified that multi-agent systems face novel attack vectors including \"secret collusion channels\" and \"coordinated swarm attacks.\" McKinsey's framework, with its emphasis on plug-and-play composability, multiplies these vulnerabilities.</p><p><strong>Computational Economics</strong>: Multi-agent systems are voraciously expensive. Anthropic's \"15&#215; more tokens\" translates directly to 15&#215; the cost. McKinsey's vision of hundreds of coordinating agents would require astronomical compute budgets that dwarf any efficiency gains.</p><h3><strong>The Reality of \"Agentic\" Implementations</strong></h3><p>When we examine actual systems being built under the \"agentic mesh\" banner, they bear little resemblance to McKinsey's vision. These implementations reveal the constraints that McKinsey's framework ignores.</p><p>SIRP's cybersecurity implementation&#8212;one of the few production systems using the \"agentic mesh\" terminology&#8212;shows the gap between vision and reality. Their system required \"breaking down a monolithic system into flexible, modular microservices\" and focuses exclusively on security operations. Rather than autonomous agents coordinating across the enterprise, SIRP built specialized tools for a narrow domain with strict boundaries and centralized control.</p><p>The pattern repeats across genuine implementations. Successful systems constrain scope ruthlessly, centralize control despite distributed execution, prioritize reliability over autonomy, and maintain human oversight at every critical decision point. These constraints directly contradict McKinsey's framework, which promises unconstrained scope, distributed autonomy, and minimal human involvement.</p><p>Even practitioners claiming to build agentic meshes reveal the reality gap. Eric Broda, who claims to be writing a book on the topic, describes building \"enterprise grade autonomous agents and putting them into an ecosystem\" but provides no evidence of the distributed, composable architecture McKinsey envisions. The silence speaks volumes&#8212;if anyone had built McKinsey's vision successfully, we'd have case studies, not PowerPoints.</p><p>The most reliable pattern identified by practitioners is the \"single-threaded linear agent\" where \"context is continuous.\" Even when dealing with long-duration tasks, the recommended approach involves \"a new LLM model whose key purpose is to compress a history of actions &amp; conversation into key details, events, and decisions\" rather than distributing work across autonomous agents. This is precisely the opposite of McKinsey's mesh topology.</p><p>Anthropic's production system uses an \"orchestrator-worker pattern\" with strict hierarchical control. Workers execute specific tasks under tight supervision. The orchestrator maintains global context and resolves conflicts. There's no",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "emergent coordination or distributed decision-making&#8212;just carefully managed execution within rigid constraints.</p><h3><strong>Why This Matters</strong></h3><p>The disconnect between McKinsey's agentic mesh and technical reality isn't merely academic. Organizations allocating resources based on this framework are setting themselves up for expensive failure. The investment numbers are staggering: AI agents captured 46.4% of US venture capital funding in 2024, with the market projected to grow from $5.1 billion to $47.1 billion by 2030.</p><p>This investment reflects genuine excitement about AI agents as a category, not endorsement of McKinsey's architectural vision. But when executives conflate the two&#8212;believing that agent success requires distributed meshes&#8212;they make catastrophic resource allocation decisions. Technical teams know the mesh won't work but find themselves building toward an impossible architecture because the board bought the vision.</p><p>The agentic mesh represents a broader pattern in enterprise technology: consultant-created frameworks that promise easy solutions to hard problems. These frameworks generate compelling PowerPoints and executive buy-in but cannot be translated into working systems. The gap between promise and delivery erodes trust, wastes resources, and delays genuine transformation.</p><p>McKinsey's agentic mesh isn't just wrong&#8212;it's actively harmful. By promising autonomous coordination without acknowledging fundamental constraints, it sets impossible expectations. By advocating distributed architectures that violate proven principles, it guarantees technical failure. By focusing on architectural elegance over practical delivery, it diverts attention from approaches that actually work.</p><p>The technical community's rejection of McKinsey's framework isn't close-mindedness&#8212;it's pattern recognition. They've seen these promises before, tried to build these systems, and learned why they fail. Their skepticism reflects wisdom earned through experience, not resistance to change.</p><p>Organizations considering agentic AI should listen to builders, not consultants. Focus on narrow, well-defined use cases. Maintain centralized control and clear boundaries. Invest in robust testing and gradual rollout. Most importantly, reject any framework that promises easy solutions to coordination, context sharing, and autonomous decision-making. These remain unsolved problems in AI, and no amount of PowerPoint polish will change that reality.</p><h2><strong>IV. The Executive Communication Crisis and Its Consequences</strong></h2><p>The gap between AI's technical reality and executive understanding isn't just a communication problem&#8212;it's a crisis generating billions in wasted investment and thousands of disrupted careers. This crisis follows a predictable pattern: bold automation promises, hidden implementation failures, quiet reversals, and expensive lessons learned. Understanding this pattern through concrete examples reveals why organizations keep failing at AI transformation and what must change.</p><h3><strong>The Klarna Disaster as Archetype</strong></h3><p>Klarna's AI journey perfectly encapsulates the executive communication crisis. In February 2024, CEO Sebastian Siemiatkowski made headlines by announcing their AI assistant was handling 2.3 million conversations monthly&#8212;two-thirds of all customer service chats&#8212;and doing the work of 700 full-time agents. The numbers seemed irrefutable: resolution times dropped from 11 minutes to under 2 minutes, customer satisfaction scores remained \"equal,\" and the company projected $40 million in annual profit improvements.</p><p>Siemiatkowski positioned Klarna as OpenAI's \"favorite guinea pig,\" a forward-thinking company leading the AI revolution. The narrative was irresistible to investors and board members: replace expensive human workers with efficient AI, maintain quality, and pocket the savings. Tech media amplified the story without scrutiny, creating a template other executives",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "would rush to follow.</p><p>The reality proved starkly different. By late 2024, Siemiatkowski publicly admitted what insiders already knew: they had \"gone too far\" with AI automation, although the company maintains there is still an AI component to customer service and they are using a dual-track approach. Regardless, his confession was remarkably candid: \"As cost unfortunately seems to have been a too predominant evaluation factor when organizing this, what you end up having is lower quality.\" The company began hiring human agents again, implementing what Siemiatkowski now calls an \"Uber-type setup\" for remote customer service workers.</p><p>Independent testing revealed problems that Klarna's cherry-picked metrics had hidden. Response times included 15-20 second awkward delays between messages&#8212;technically fast but experientially frustrating. The AI provided overly verbose, unhelpful responses that filled entire chat windows with robotic text. Most damning, when customers expressed financial hardship&#8212;asking questions like \"What happens if I can't pay on time?\"&#8212;the AI responded with emotionless boilerplate, lacking any acknowledgment of their difficult situation.</p><p>The metrics Klarna celebrated masked fundamental failures. While the AI could quickly provide scripted responses, it couldn't actually resolve complex issues. Many \"successful\" interactions simply directed customers to contact merchants directly or ended with customers abandoning their queries in frustration. High abandonment rates were misinterpreted as successful resolutions. The company essentially flew blind while claiming victory based on incomplete data.</p><p>Security vulnerabilities emerged as users discovered they could manipulate the chatbot through prompt injection attacks. One user successfully got the bot to generate Python code&#8212;completely outside its intended function. Despite safety guardrails, the system remained vulnerable to clever prompting that bypassed restrictions. In financial services, where trust is paramount, these vulnerabilities represented existential risk.</p><p>Industry experts like tech analyst Gergely Orosz tested the bot personally and found it \"underwhelming,\" noting it \"recites exact docs and passes me on to human support fast.\" Rather than replacing agents, the AI merely acted as an inefficient gateway to human help, adding friction to the customer experience while saving no actual labor.</p><h3><strong>Why Executives Fall for the Automation Fallacy</strong></h3><p>The Klarna pattern&#8212;bold automation claims followed by quiet reversal&#8212;repeats across industries because executives consistently misunderstand the nature of work itself. This misunderstanding stems from viewing jobs through what academics call the \"bundles of tasks\" framework, popularized by economist David Autor. In this model, occupations are collections of discrete, potentially automatable tasks. If AI can handle each task, the thinking goes, it can replace the job.</p><p>Geoffrey Hinton's 2016 prediction about radiologists illustrates this fallacy perfectly. The godfather of deep learning declared: \"People should stop training radiologists now. It's just completely obvious that within five years deep learning is going to do better than radiologists.\" He compared radiologists to cartoon characters who had already run off a cliff but hadn't yet looked down.</p><p>Eight years later, the United States faces a historic radiologist shortage with over 1,400 open positions. Radiology employment has grown by 7% since Hinton's prediction. Mayo Clinic alone expanded its radiology staff by 55%. Even Hinton himself admitted in 2024 that he \"spoke too broadly\" and was \"wrong on",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the timing.\" Current projections show the shortage will persist through 2055 without intervention, with supply growing 25.7% while demand grows 16.9-26.9%</p><p>The failure wasn't technical&#8212;AI has indeed become excellent at pattern recognition in medical images. The failure was conceptual. Radiologists don't just identify patterns; they correlate findings with patient history, communicate with referring physicians, make treatment recommendations, manage departmental workflows, mentor residents, and navigate complex healthcare systems. These interconnected responsibilities resist decomposition into discrete tasks.</p><p>Research from the EU JRC-Eurofound Tasks Framework reveals that \"tasks do not exist in isolation, they are coherently bundled into jobs which are performed by people, and the entire process has to be socially organised.\" This social organization&#8212;what Tanya Reilly termed \"glue work\"&#8212;remains invisible in most job analyses yet proves essential for organizational function.</p><p>In radiology, glue work includes coordinating with technicians about scan protocols, discussing complex cases with referring physicians, managing equipment schedules, and building relationships that enable smooth departmental operation. When organizations attempt to automate based on visible tasks alone, this invisible coordination work becomes more complex and crucial, not less.</p><p>Susan Leigh Star's research on \"invisible work\" explains why automation efforts consistently fail. Creating \"effortless ease\" in any system requires continuous, often unrecognized maintenance work. The automation paradox, identified by researcher Lisanne Bainbridge, states: \"The more efficient the automated system, the more crucial the human contribution of the operators becomes.\"</p><p>This paradox manifests dramatically in AI implementations. As individual tasks become automated, the coordination work binding them together grows more complex. Automated systems generate edge cases requiring human judgment. Quality assurance demands increase as someone must verify automated outputs and manage failures. The promise of labor savings evaporates as new forms of work emerge.</p><h3><strong>The Downstream Devastation</strong></h3><p>When executives misunderstand AI capabilities, the consequences cascade through organizations with devastating effect. The numbers tell a sobering story:</p><ul><li><p>Over 80% of AI projects fail&#8212;twice the failure rate of traditional IT projects (RAND Corporation)</p></li><li><p>While 78% of organizations use AI in at least one business function, only 4% generate substantial value (BCG)</p></li><li><p>42% of companies abandoned most AI initiatives by 2024, up from 17% in 2023 (S&amp;P Global)</p></li><li><p>Only 25% of AI business projects deliver promised ROI (IBM)</p></li></ul><p>These aren't just statistics&#8212;they represent enormous waste. IBM Watson for Oncology consumed $62 million at MD Anderson Cancer Center over four years before abandonment. The system, trained on hypothetical rather than real patient data, gave what one doctor called \"unsafe and incorrect\" treatment recommendations. McDonald's three-year partnership with IBM for AI drive-through ordering ended in 2024 after viral videos showed the system adding 260 Chicken McNuggets to a single order. Amazon scrapped its AI recruiting tool after discovering it discriminated against women, having learned bias from historical hiring data.</p><p>Each failure shares common patterns: oversimplifying job complexity, ignoring integration challenges, and assuming technology can directly substitute for human judgment. The hidden costs compound quickly. IBM research indicates computing costs will climb 89% between 2023-2025, with 70% of executives citing generative AI as the primary driver. Data preparation, infrastructure scaling, specialized talent, compliance requirements, and ongoing maintenance often dwarf initial investment",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "projections.</p><p>McKinsey research shows 38% of leaders expect to reskill more than 20% of their workforce, while 8% anticipate workforce reductions exceeding 20%. This \"tradeoff spectrum\" mentality&#8212;viewing AI agents as direct substitutes for human workers&#8212;drives many failed implementations. When executives operate from this framework, they make decisions that guarantee failure: underinvesting in change management, expecting immediate ROI, ignoring integration complexity, and measuring success through cost reduction rather than value creation.</p><p>The human cost extends beyond mere employment numbers. When Klarna announced its AI success, employee morale plummeted as workers saw themselves as expendable. The eventual reversal and rehiring damaged trust and institutional knowledge. This pattern&#8212;premature automation announcements followed by workforce disruption and eventual reversal&#8212;destroys organizational capability even when jobs ultimately return.</p><h3><strong>The Translation Problem</strong></h3><p>The root cause of these failures lies in a fundamental translation problem between technical teams and executive leadership. Technical teams understand AI's capabilities and limitations but struggle to communicate them in business terms. Executives need to make strategic decisions but lack the framework to evaluate AI realistically. Into this gap step consultants with frameworks like McKinsey's agentic mesh, offering simple narratives that obscure complex realities.</p><p>Traditional technical explanations fail in boardrooms. Terms like \"neural networks,\" \"transformers,\" \"context windows,\" and \"token limits\" don't translate to business impact. When engineers try to explain why distributed agent systems won't work, they dive into technical details about gradient propagation and attention mechanisms. Executives hear complexity and risk where consultants promise simplicity and transformation.</p><p>The translation failure works both ways. When executives ask for \"AI to analyze all our customer data,\" they don't understand they're requesting something that would require breaking data into thousands of chunks, cost hundreds of thousands in compute, and produce inconsistent results due to context limitations. Technical teams hear impossible requirements but struggle to explain why in business terms.</p><p>This communication gap creates a vacuum that consulting frameworks fill with dangerous fantasies. McKinsey's agentic mesh sounds strategic and transformative. It uses business language&#8212;\"composable,\" \"vendor-agnostic,\" \"governed autonomy\"&#8212;while hiding technical impossibilities. Executives, lacking alternative frameworks, embrace these visions and allocate resources accordingly.</p><p>The consequences compound as middle management tries to bridge the gap. They're tasked with implementing executive vision while managing technical reality. This impossible position leads to what one engineering manager called \"reality theater\"&#8212;maintaining executive fiction while secretly building something feasible. Resources waste on parallel tracks: the official project following consultant frameworks and the shadow project actually delivering value.</p><p>The Klarna case illustrates how metrics become weapons in this communication crisis. By focusing on easily measured outcomes&#8212;response time, chat volume&#8212;while ignoring harder-to-quantify factors like customer satisfaction and issue resolution, executives could claim success while customers suffered. Technical teams knew the system was failing but couldn't translate their concerns into executive-friendly metrics.</p><p>This crisis isn't just about current failures&#8212;it's about missed opportunities. While organizations chase automation mirages, competitors who understand AI's true capabilities build sustainable advantages. They use AI for augmentation rather than replacement, invest in human-AI collaboration, and measure success through value creation rather than cost reduction.</p><p>Breaking this cycle requires new frameworks for executive communication about AI&#8212;frameworks that",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "acknowledge technical constraints while speaking business language. It requires metrics that capture real value rather than vanity statistics. Most importantly, it requires executives to develop enough technical literacy to distinguish between consultant fantasies and builder wisdom.</p><p>The stakes couldn't be higher. As Karpathy's Software 3.0 vision becomes reality, organizations need leaders who understand both its transformative potential and inherent limitations. The choice is stark: continue falling for automation fallacies and consultant frameworks, or develop the sophisticated understanding necessary to harness AI's genuine capabilities. The organizations that succeed will be those whose executives learn to listen to builders over consultants, embracing complexity rather than seeking false simplicity.</p><h2><strong>V. Building Technically Grounded Executive Narratives</strong></h2><p>After witnessing the devastation caused by fantasy frameworks and automation fallacies, the question becomes: how do we build executive narratives that convey technical reality without losing business impact? The answer isn't dumbing down complexity but translating it through operational frameworks executives already understand. This section presents proven approaches for bridging the communication gap, drawn from successful implementations and hard-won practitioner wisdom.</p><h3><strong>Principles That Work</strong></h3><p>The most effective principle for executive communication about AI is leading with operational analogies rather than technical metaphors. Stop explaining LLMs as \"neural networks\" or \"transformers.\" Instead, frame them as \"brilliant interns with perfect recall but no judgment.\" This isn't simplification&#8212;it's operationally accurate and immediately actionable.</p><p>Consider how this reframing changes executive thinking. A brilliant intern can draft exceptional memos but might confidently cite nonexistent regulations. They can process vast amounts of information but need supervision for critical decisions. They work tirelessly but require clear direction and quality review. This framing immediately suggests the right deployment pattern: high-value tasks with human review, not autonomous decision-making.</p><p>The second principle involves making constraints tangible through time and money&#8212;languages every executive speaks fluently. Instead of explaining \"context window limitations,\" show them: \"This AI can process about 50 pages at once. Processing your entire customer database would require breaking it into 10,000 chunks, taking 400 hours and costing $50,000 in compute&#8212;with no guarantee the AI remembers chunk 1 when processing chunk 10,000.\"</p><p>Suddenly, the \"just have AI analyze all our data\" request reveals its true cost. The executive doesn't need to understand attention mechanisms or token limits. They understand that $50,000 for inconsistent analysis makes no business sense.</p><p>The third principle requires demonstrating failure modes in their specific domain. Generic warnings about hallucinations don't land. Instead, take their actual business scenarios and show specific failures. For a retail executive: \"The AI might confidently tell a customer that your Birmingham store has the product in stock when that store closed two months ago.\" For healthcare: \"The AI could merge symptoms from two different patients in its response.\"</p><p>Domain-specific failures make abstract risks visceral. An executive who sees how AI could tell customers about non-existent store inventory understands the brand risk immediately. They don't need to grasp the technical reasons for hallucination&#8212;they need to understand the business impact.</p><p>The fourth principle leverages progressive disclosure through pilot results. Rather than explaining all limitations upfront, structure pilots to reveal constraints naturally. Week 1:",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "\"Look how fast the AI drafts reports!\" Week 2: \"Notice how it needs fact-checking.\" Week 3: \"See how accuracy improves with structured prompts.\" Week 4: \"Here's the sustainable human-in-the-loop workflow.\"</p><p>This experiential learning beats any PowerPoint. An executive who has personally watched an AI confidently hallucinate critical facts won't buy into autonomous agent meshes. One who has seen compute costs spiral won't approve unlimited AI initiatives.</p><p>The final principle reframes ROI as capability multiplication rather than cost reduction. The McKinsey trap promises labor replacement and cost savings. Instead, show capability multiplication: \"Your best analyst can now investigate 10x more hypotheses.\" \"Your creative team can explore 50x more design variations.\" \"Your customer service can provide personalized responses while maintaining corporate consistency.\"</p><p>This framing aligns with Karpathy's augmentation vision while speaking business language. It shifts focus from replacing workers to amplifying their impact&#8212;a narrative that excites rather than threatens.</p><h3><strong>The CFO's Framework in Action</strong></h3><p>The most sophisticated framework for executive AI communication targets the CFO mindset specifically. CFOs instinctively understand capital allocation, risk management, and ROI calculations. By recasting AI concepts in financial terms, we can achieve breakthrough communication.</p><p>First, recast AI as working capital, not fixed assets. CFOs want to capitalize AI investments as technology assets, but this mental model misleads. AI systems are more like working capital&#8212;they depreciate rapidly (models become outdated), require constant replenishment (retraining, fine-tuning), and their value is realized only through active deployment.</p><p>Frame it this way: \"AI isn't a server you buy; it's inventory that spoils. Your $2M model investment has an 18-month shelf life before competitive obsolescence.\" This immediately shifts thinking from one-time investment to ongoing operational commitment.</p><p>Second, expose the hidden OpEx multiplier. Most AI pitches focus on license costs, ignoring the operational multiplier. For every $1 in AI licensing, expect $3-5 in operational costs: compute overhead, human oversight, error correction, and integration maintenance.</p><p>Show this as a fully-loaded cost model: \"That $100K annual LLM license actually costs $400K to operate effectively. Here's the breakdown: $100K license, $150K compute, $100K human oversight, $50K integration maintenance.\" CFOs appreciate this transparency and can model accordingly.</p><p>Third, quantify the \"jagged intelligence tax.\" Karpathy's concept of jagged intelligence translates directly to financial unpredictability. Model this as a reliability coefficient: \"The AI handles 85% of cases perfectly, saving $50 per transaction. But 15% require human intervention, costing $200 per escalation. Net impact: $17.50 cost per transaction versus $30 baseline. Positive ROI, but with volatile monthly performance.\"</p><p>This framework helps CFOs understand why AI projects show inconsistent returns and plan for variance.</p><p>Fourth, apply risk-adjusted returns through failure cost modeling. CFOs understand risk-adjusted returns intuitively. Apply this to AI: \"Customer service AI has 95% accuracy. That 5% error rate on 10,000 monthly interactions means 500 failures. At $1,000 average recovery cost per significant error, that's $500K monthly risk exposure. Error insurance through human oversight costs $100K monthly&#8212;a clear risk arbitrage.\"</p><p>This transforms abstract accuracy discussions into concrete financial decisions.</p><p>Fifth, model the compound productivity paradox. Traditional automation shows linear productivity gains. AI shows compound effects&#8212;both positive and negative. Model it: \"Month 1: 20% productivity",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "gain. Month 3: 40% gain as teams adapt. Month 6: Either 100% gain if properly managed, or -10% due to quality debt from uncaught errors compounding.\"</p><p>This J-curve dynamic affects cash flow timing and working capital requirements. CFOs need to understand this pattern to set appropriate expectations and funding levels.</p><p>Finally, account for balance sheet impact through intangible asset creation. AI doesn't create traditional assets but does generate intangible value affecting enterprise valuation: proprietary prompts, verified output libraries, trained human-AI teams.</p><p>Frame this as: \"We're building a $10M intangible asset&#8212;our 'AI-augmented workforce capability'&#8212;that directly impacts EBITDA multiples in exit scenarios.\" This helps CFOs understand AI investment as capability building, not just cost reduction.</p><p>The master equation brings it together: <strong>AI ROI = (Capability Gain &#215; Utilization Rate) - (Total Loaded Costs + Error Costs)</strong></p><p>Give CFOs a formula they can model: \"Marketing AI provides 10x content generation (Capability Gain) but only 30% meets brand standards (Utilization Rate), yielding 3x effective multiplication. At $500K total annual cost and $200K error correction, we need $700K in value creation to break even&#8212;achievable by augmenting our $2M content team.\"</p><h3><strong>Success Stories: When Executives Get It</strong></h3><p>Organizations that successfully implement AI share a common trait: executives who understand both potential and limitations. These leaders didn't buy consultant fantasies&#8212;they built realistic strategies based on technical truth.</p><p>A Fortune 500 financial services firm exemplifies this approach. Rather than pursuing McKinsey-style agent meshes, they focused on augmenting their analysts with AI tools. The CEO framed it simply: \"We're giving our analysts AI research assistants. Like any assistant, they need training, make mistakes, and require oversight. But they also multiply our analysts' capacity to investigate fraud patterns.\"</p><p>This framing drove appropriate investment decisions. They allocated 70% of budget to training and process redesign, 30% to technology. They measured success through fraud detection rates and analyst satisfaction, not cost reduction. Result: 300% improvement in fraud pattern identification with no analyst layoffs.</p><p>A major retailer's approach to customer service AI shows similar wisdom. The COO explicitly rejected the Klarna model: \"We're not replacing our service team. We're giving them superpowers.\" They implemented AI that suggested responses but required human approval. Agents could modify suggestions, and the system learned from corrections.</p><p>Critically, they prepared for the jagged intelligence tax. They identified query types where AI excelled (order status, return policies) and where it failed (complex complaints, emotional situations). They routed accordingly. They budgeted for ongoing human oversight. Result: 40% efficiency gain while improving customer satisfaction scores.</p><p>The 4% of companies generating substantial AI value (per BCG research) share distinctive characteristics aligned with these principles:</p><ul><li><p>They target core business processes rather than peripheral support functions</p></li><li><p>They make ambitious but specific bets, focusing on average 3.5 use cases versus 6.1 for less successful peers</p></li><li><p>They invest twice as much in people and processes as their competitors</p></li><li><p>They measure success through business outcomes&#8212;time savings, error reduction, customer satisfaction&#8212;rather than technical metrics</p></li></ul><h3><strong>Building Your Own Technically Grounded Narrative</strong></h3><p>Creating effective executive communication about AI requires systematic approach:</p><p><strong>Start with business problems, not AI capabilities.</strong> Don't begin with \"here's what AI can do.\" Begin with \"here's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the business challenge we're solving.\" This prevents solution-in-search-of-problem thinking.</p><p><strong>Create visceral understanding through constrained experience.</strong> Build executive experiences with built-in constraints: time-boxed tasks showcasing both speed and errors, side-by-side comparisons of AI versus human expert output, real consequences for over-trusting AI in safe pilot environments, and visible compute meters showing cost accumulation in real-time.</p><p><strong>Develop domain-specific frameworks.</strong> Generic AI frameworks fail because they lack context. Develop frameworks specific to your industry that translate technical concepts into familiar operational patterns.</p><p><strong>Institute \"reality metrics.\"</strong> Replace vanity metrics with measurements that capture true value: end-to-end resolution time (not just response time), quality-adjusted output volume (not just quantity), total cost per outcome (including error correction), and human effort multiplier (not replacement rate).</p><p><strong>Create feedback loops between technical teams and executives.</strong> Regular sessions where technical teams demonstrate actual capabilities&#8212;not PowerPoints but live systems&#8212;with executives asking questions and seeing failures. This builds intuition faster than any framework.</p><p>The goal isn't making executives into ML engineers but giving them operational intuition&#8212;the same way they intuitively understand supply chain constraints without being logistics experts. Only then can they make intelligent decisions about AI investments that align with technical reality rather than consulting fantasies.</p><p>This approach transforms AI from mysterious technology requiring faith into understandable capability requiring judgment. It replaces the \"build it and pray\" mentality with \"understand and deploy.\" Most importantly, it aligns executive expectations with technical reality, creating conditions for genuine success rather than expensive failure.</p><p>The organizations that master this translation&#8212;building technically grounded executive narratives&#8212;will be those that capture AI's genuine value. They'll avoid both the Klarna trap of premature automation and the McKinsey mirage of impossible architectures. Instead, they'll build pragmatic augmentation strategies that amplify human capability while respecting technical constraints. In the Software 3.0 era, this translation capability becomes as critical as the technology itself.</p><h2><strong>VI. Why This Matters Now: The Breaking Wave of Software 3.0</strong></h2><h3><strong>The Reality Already Breaking Through</strong></h3><p>Software 3.0 isn't a future prediction&#8212;it's today's reality, transforming how software gets built right now. While executives debate AI strategy in boardrooms, developers are already living in Karpathy's world where \"the hottest new programming language is English.\"</p><p>The evidence is overwhelming and accelerating. Cursor AI, which Karpathy showcased as the exemplar of Software 3.0, has developers reporting productivity gains that sound fictional. A senior engineer at a major tech company recently built a complete 3D visualization tool in four hours&#8212;work that would have taken two weeks traditionally. He didn't write code; he described what he wanted in natural language and reviewed what the AI generated. \"Vibe coding,\" as practitioners call it, has moved from experiment to standard practice.</p><p>The transformation extends beyond individual productivity. Entire products now exist that couldn't have been built economically before. MenuGen.app converts restaurant menu photos into polished websites&#8212;not through complex image processing pipelines but through natural language descriptions fed to AI. Teenagers with no coding experience are shipping successful games on Steam by describing gameplay mechanics in English. The democratization Karpathy predicted is happening at breathtaking speed.</p><p>Yet most organizations remain trapped in outdated paradigms. They're evaluating McKinsey's agent mesh architectures",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "while their competitors ship products built through natural language. They're modeling ROI on worker replacement while missing the 10-100x productivity multipliers available today. They're planning five-year AI transformations while the landscape shifts monthly.</p><p>The disconnect grows more costly by the day. Consider what's happening in financial services. While major banks debate AI governance frameworks, fintech startups use Software 3.0 tools to build and deploy features in days that would take traditional institutions months. A two-person team recently built a complete lending platform using AI assistance&#8212;competing directly with products that required 50-person teams just two years ago.</p><p>This isn't limited to software companies. Law firms using AI contract review report junior associates performing at senior associate levels. Marketing agencies generate campaign variations in minutes that previously required weeks of creative work. Healthcare startups build diagnostic tools that would have required millions in traditional development.</p><p>The revolution is sector-agnostic because natural language is universal. Anyone who can clearly articulate ideas can now create software. This represents the most fundamental democratization of capability in computing history.</p><h3><strong>The Decade of Agents Demands Better</strong></h3><p>Karpathy declared we're entering \"the decade of agents,\" and the evidence supports his timing. But this transformation demands fundamentally different organizational capabilities than previous technology waves.</p><p>The pace of change has become exponential. OpenAI's trajectory illustrates this acceleration: GPT-3 in 2020 amazed with basic text generation. GPT-4 in 2023 passed professional exams. Current models write production code, analyze complex documents, and engage in sophisticated reasoning. The capability jumps between versions now exceed the total progress of previous decades.</p><p>Every month of executive delusion now equals millions in misdirected investment and incalculable opportunity cost. While boards approve multi-year agent mesh implementations, competitors build and deploy AI-augmented products in weeks. While consultants design governance frameworks, builders ship transformative features. While organizations plan for gradual change, the market rewards those moving at AI speed.</p><p>The competitive dynamics have shifted fundamentally. Traditional moats&#8212;capital, expertise, proprietary technology&#8212;matter less when a small team with AI can match the output of large organizations. The new differentiators are speed of iteration, quality of human-AI collaboration, and clarity of vision about what to build. Organizations optimizing for the wrong variables fall further behind daily.</p><p>Consider the venture capital flowing into AI: $15.7 billion in 2024 alone, with agents capturing 46.4% of funding. This capital seeks returns from Software 3.0 transformation, not McKinsey mesh implementations. The startups receiving funding aren't building distributed agent architectures&#8212;they're building focused tools that amplify human capability. The market has already chosen augmentation over automation.</p><p>The talent dynamics reinforce this urgency. The best developers have already adopted Software 3.0 workflows. They won't work for organizations clinging to outdated paradigms. A senior engineer recently turned down a lucrative offer because the company blocked AI coding tools for security reasons. \"It would be like asking me to code on a computer from 2010,\" he explained. The productivity gap has become unbridgeable.</p><h3><strong>The Path Forward</strong></h3><p>The path forward isn't complex, but it requires abandoning comfortable delusions. Organizations must start with augmentation, not automation. Focus on enhancing human capabilities rather than replacing them.",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "The most successful AI implementations amplify human judgment rather than attempting to supplant it.</p><p>This means investing in the full sociotechnical system. BCG's 10-20-70 rule reflects reality: only 30% of effort should focus on technology, with 70% dedicated to process and people. This isn't conservative&#8212;it's practical. The organizations achieving 10x productivity gains invest heavily in training, workflow redesign, and cultural change.</p><p>Critically, organizations must acknowledge invisible work in ROI calculations. Traditional task-based analyses miss the coordination labor that keeps organizations functioning. Realistic ROI models must account for the glue work that emerges when AI handles routine tasks&#8212;quality assurance, exception handling, stakeholder communication, and system maintenance.</p><p>The timeline perspective must shift from revolutionary to evolutionary. The electricity revolution took four decades; AI transformation will likely follow similar patterns. But unlike electricity's steady rollout, AI capability improves monthly. Organizations must build for continuous adaptation rather than one-time transformation.</p><p>Successful organizations will create AI-native workflows that leverage both human and machine strengths. They'll build robust feedback loops between generation and verification. They'll measure success through value creation, not cost reduction. Most importantly, they'll maintain human judgment at critical decision points while using AI to explore vastly more possibilities.</p><h3><strong>Final Argument</strong></h3><p>We stand at an inflection point that will divide organizations into winners and losers with unusual clarity. The division won't follow traditional lines of size, capital, or market position. It will separate those who understand AI's real capabilities from those chasing consultant mirages.</p><p>Software 3.0 represents genuine transformation&#8212;but only for those honest enough to see it clearly. Natural language as a programming interface doesn't eliminate the need for human judgment; it amplifies its impact. AI agents don't replace workers; they multiply their capabilities. The future isn't autonomous meshes; it's human-AI teams achieving what neither could alone.</p><p>The executives who grasp this reality will build organizations that thrive in the agent decade. They'll attract the best talent, ship products at AI speed, and create value their automation-obsessed competitors can't match. They'll measure success not by how many humans they've replaced but by how much human potential they've unlocked.</p><p>Those who continue chasing McKinsey's distributed dreams will join Klarna in the graveyard of premature automation. They'll waste billions on impossible architectures while competitors build real value. They'll issue press releases about AI transformation while quietly rehiring the humans they prematurely displaced.</p><p>The choice is binary and urgent. Every day of delay compounds the disadvantage. Every consultant framework adopted deepens the hole. Every automation fantasy pursued wastes resources that could build genuine capability.</p><p>The question isn't whether AI will transform your organization&#8212;it will, either as a tool for amplification or a source of expensive failure. The question is whether you'll navigate this transformation with clear eyes or consultant-clouded vision.</p><p>Listen to builders over consultants. Study Karpathy's honest assessment over McKinsey's polished promises. Invest in augmentation over automation. Build with humility about limitations while harnessing genuine capabilities. Most importantly, act with urgency&#8212;the wave of Software 3.0 is breaking now, and those who catch it will ride it to places the framework-followers can't imagine.</p><p>The future belongs to organizations that embrace",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "AI as a collaborator, not a replacement. In Karpathy's words, we're building \"Iron Man suits\" for knowledge work. The suit amplifies human capability&#8212;it doesn't replace the human inside. Understanding this distinction, and acting on it with urgency, will determine who thrives in the decade ahead.</p><p>The time for debate has passed. The time for building has arrived. Software 3.0 is here, and it rewards those who see it clearly. The only question remaining is whether your organization will be among them.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!EZeE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1442504,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h1>Endnotes</h1><h2>Andrej Karpathy's Software 3.0 Framework</h2><ol><li><p><a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Andrej Karpathy's official keynote presentation \"Software Is Changing (Again)\" from Y Combinator AI Startup School on June 17, 2025</a></p></li><li><p><a href=\"https://www.latent.space/p/s3\">Detailed annotated notes and analysis of Karpathy's Software 3.0 talk at YC AI Startup School 2025, including full transcript and slides</a></p></li><li><p><a href=\"https://drive.google.com/file/d/1a0h1mkwfmV2PlekxDN8isMrDA5evc4wW/view?usp=sharing\">Direct link to Karpathy's presentation slides as referenced in the YouTube video description</a></p></li></ol><h2>McKinsey's AI Agentic Mesh Framework</h2><ol start=\"4\"><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage\">McKinsey's official report \"Seizing the Agentic AI Advantage\" detailing the agentic mesh framework and the 78% statistic about companies using AI with minimal impact</a></p></li><li><p><a href=\"https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/seizing%20the%20agentic%20ai%20advantage/seizing-the-agentic-ai-advantage-june-2025.pdf\">Direct PDF link to McKinsey's complete agentic AI report</a></p></li><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai\">McKinsey's 2025 Global AI Survey confirming the 78% adoption statistic with minimal business impact</a></p></li></ol><h2>Klarna's AI Customer Service Case Study</h2><ol start=\"7\"><li><p><a href=\"https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/\">Klarna's official February 2024 press release announcing their AI assistant handling 2.3 million conversations and doing the work of 700 agents</a></p></li><li><p><a href=\"https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service\">Bloomberg report on CEO Sebastian Siemiatkowski's admission that Klarna's AI-first approach \"went too far\" and the company's decision to hire human agents again</a></p></li><li><p><a href=\"https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/\">Detailed coverage of Klarna's reversal from AI-only customer service back to human agents, including CEO quotes about service quality issues</a></p></li></ol><h2>AI Implementation Failures</h2><ol start=\"10\"><li><p><a href=\"https://www.forbes.com/sites/matthewherper/2017/02/19/md-anderson-benches-ibm-watson-in-setback-for-artificial-intelligence-in-medicine/\">Forbes report on IBM Watson's $62 million failure at MD Anderson Cancer Center</a></p></li><li><p><a href=\"https://www.nytimes.com/2021/07/16/technology/what-happened-ibm-watson.html\">New York Times investigation into IBM Watson's healthcare failures, including the MD Anderson project</a></p></li><li><p><a href=\"https://www.delish.com/food-news/a61146061/mcdonalds-ends-ai-drive-thru-test/\">Report on McDonald's ending its AI drive-through partnership with IBM after three years of testing due to ordering errors</a></p></li><li><p><a href=\"https://indianexpress.com/article/technology/tech-news-technology/air-canada-ai-chatbot-9170822/\">Coverage of Air Canada's legal troubles when their AI chatbot invented refund policies the company was forced to honor</a></p></li></ol><h2>Technical Community Response and Research</h2><ol start=\"14\"><li><p><a href=\"https://cognition.ai/blog/dont-build-multi-agents\">Cognition AI's official blog post explaining why multi-agent systems are \"fragile\" and lead",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "to system failures, based on their experience building Devin</a></p></li><li><p><a href=\"https://www.auntminnie.com/imaging-informatics/artificial-intelligence/article/15746014/hinton-acknowledges-mistake-in-predicting-ai-replacement-of-radiologists\">Report on Geoffrey Hinton's 2024 acknowledgment that his 2016 prediction about AI replacing radiologists was \"wrong on timing\"</a></p></li></ol><h2>AI Project Failure Statistics</h2><ol start=\"16\"><li><p><a href=\"https://www.rand.org/pubs/research_reports/RRA2680-1.html\">RAND Corporation's official research report documenting that over 80% of AI projects fail, twice the rate of non-AI IT projects</a></p></li><li><p><a href=\"https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2600/RRA2680-1/RAND_RRA2680-1.pdf\">Direct PDF link to the complete RAND Corporation study on AI project failures</a></p></li></ol>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Sunday AI Reads & Key Updates",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Hey all!I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.Useful stuff to knowMaven ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "<p>Hey all!</p><p>I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.</p><h2><strong>Useful stuff to know</strong></h2><h3><strong>Maven Discount</strong></h3><p><strong>TODAY ONLY you can get a discount off my Maven course</strong> on AI Career Acceleration. Use code MAVEN100 at <a href=\"https://maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100\">maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100</a>. This is a course aimed at AI builders, and students say it feels dense but approachable&#8212;I include lessons in AI fundamentals, agents, and how to think about AI prompting. The time limit isn&#8217;t me clickbaiting&#8212;Maven sets that, and it&#8217;s worth checking out all of the <a href=\"https://maven.com/\">Maven 100</a>&#8212;I&#8217;m not the only one doing good work out there!</p><h3><strong>New Founding Tier Benefit</strong></h3><p>I&#8217;m adding a new benefit for members on the Founding Tier (renamed Executive Circle). Starting next week, <strong>you&#8217;ll receive a concise but very substantive AI executive brief that&#8217;s relevant for the boardroom</strong>. Topics will include AI and markets, AI and organizational change, and AI investment theses. If you&#8217;re interested in signing up, Substack <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">makes it easy to change your membership tier here</a>.</p><h3><strong>I&#8217;m starting an AI Slack / Discord Community</strong></h3><p>I&#8217;ve heard you! <strong>I will be launching a community</strong> for paid subscribers on either Slack or Discord in the coming couple weeks. We&#8217;ll have space for AI questions, AI resources, and AI jobs. Which do you prefer? Let me know with <a href=\"https://forms.gle/g63WC8FscPRMPp8U7\">a 10 second survey here</a>.</p><h3><strong>Update on my new prompting tool PromptKit</strong></h3><p>I know many of you have been excited to hear more on <a href=\"https://www.promptkit.pro/\">PromptKit</a>. We&#8217;ve been busy getting it ready, and <strong>I will be reaching out to some of our first alpha testers next week</strong>, with more onboarded every week. I&#8217;m excited to bring more of a prompt creation and sharing community into the product in response to your feedback so far! If you haven&#8217;t signed up yet, <strong>folks on the early access list will all be onboarded before we go out to the world,</strong> <strong>so go ahead and <a href=\"https://www.promptkit.pro/\">hop in at the link</a></strong>.</p><h2>Links I&#8217;m reading</h2><p>I&#8217;ll be writing about one of these tomorrow! Which is it? </p><ol><li><p>Andrej Karpathy&#8217;s <a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Software 3.0 talk</a></p></li><li><p>The <a href=\"https://time.com/7294699/meta-scale-ai-data-industry/\">Scale AI acquisition</a> by Meta</p></li><li><p>Cluely <a href=\"https://www.youtube.com/watch?v=yesds-SQmkM\">raises $15M</a></p></li><li><p>Dwarkesh&#8217;s <a href=\"https://www.youtube.com/watch?v=zIEQdAnOfwg\">AI Lab Review </a></p></li></ol><p>Cheers!</p><p>Nate</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!bce5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2534176,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166551043?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">this is way too kind to my beard color lol</figcaption></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Beyond the Perfect Prompt: The Definitive Guide to Context Engineering—The Next Revolution in Artificial Intelligence",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It NowI get asked about AI prompts constantly. Like, constantly. And look, I love talki...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "<h3><em><strong>Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It Now</strong></em></h3><p><em><strong>I get asked about AI prompts constantly.</strong> Like, constantly. And look, I love talking about prompt engineering because it genuinely works&#8212;I've put together hundreds of pages <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts?r=1z4sm5\">on this Substack</a> writing about AI prompts and artificial intelligence optimization because the right prompting techniques can completely transform what you get from large language models. But here's the thing that's been bugging me: while we've all been obsessing over crafting perfect prompts, something way bigger has been happening in AI system design.</em></p><p><em>It&#8217;s big, and I haven&#8217;t written about it at all yet.</em></p><p><em>And although I know I should cover it, I almost didn't write this piece. <strong>It seemed</strong> <strong>too deep, too in the weeds, too much like something only machine learning engineers</strong> <strong>would care about</strong>. But then I watched Claude go out and search 500+ sources to research a topic I asked about (I kid you not, I counted), and I realized my carefully crafted prompt was maybe 0.1% of the total context it actually processed. </em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!LUEd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" width=\"1312\" height=\"140\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:140,&quot;width&quot;:1312,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:25060,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div></div></div></a></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!nOZt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" width=\"1412\" height=\"144\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:144,&quot;width&quot;:1412,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:28391,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"></picture><div></div></div></a></figure></div><p><em>Yes I definitely push that multi-agent lifestyle lol</em></p><p><em>Anyway staring at those numbers enough times is when it hit me: we're not just doing prompt engineering anymore. We're doing <strong>context engineering</strong>&#8212;and it's the future of artificial intelligence development.</em></p><p><em>And honestly? Most people have no idea this shift is happening in AI systems.</em></p><p><em>Here's what I've learned: these AI agents aren't just reading your prompts anymore. They're actively searching hundreds of websites, pulling from your Google Drive, connecting to databases, and synthesizing information from sources you never directly gave them. The AI prompt you write? That's becoming a tiny drop in an ocean of context these large language models discover on their own.</em></p><p><em>This is a fundamental shift in how we need to think about artificial intelligence systems. And it's not just for machine learning engineers&#8212;though if you're working in AI development I&#8217;ve included plenty of technical detail for you here. But really, this is for anyone who wants to actually understand how these AI tools work and get better results from them.</em></p><p><em>We're living through the emergence of what I'm calling <strong>deterministic versus probabilistic context</strong> in AI systems. The stuff you control&#8212;your AI prompts, uploaded documents, system instructions&#8212;that's deterministic context. But there's this whole other layer of probabilistic context: the vast web of information AI agents autonomously find and integrate. When Claude searches the web for investment advice, your original prompt becomes maybe 0.1% of what the large language model is actually processing.</em></p><p><em>Fair warning: this guide is necessarily long because artificial intelligence context engineering is complex and the stakes are",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "genuinely high. I'm going to walk you through exactly how this two-layer AI system architecture works, why token optimization (all that obsessing over making AI prompts shorter and cheaper) completely misses the point when AI agents are processing massive context windows you can't directly control, and most importantly, how to design what I call \"semantic highways\"&#8212;ways to guide artificial intelligence discovery toward useful information while avoiding some very real AI security risks.</em></p><p><em>Because yes, bad actors are already figuring out how to manipulate AI model behavior through poisoned web content and compromised data sources. (More on that later&#8212;it's wild.)</em></p><p><em>You'll see real examples of how organizations are implementing context engineering in AI systems today, from financial firms using AI tools to process real-time market data to healthcare systems integrating patient records with the latest research through artificial intelligence. I'll break down the emerging AI development tools like Anthropic's Model Context Protocol that are making this possible, and honestly assess both the incredible opportunities and genuine limitations we're facing in machine learning.</em></p><p><em>The future belongs to people who understand how to architect artificial intelligence context ecosystems, not just write good AI prompts. And that future? It's happening right now.</em></p><p><em><strong>What You'll Find in This Complete Guide to Context Engineering:</strong></em></p><ul><li><p><em><strong>The Two-Layer Architecture That's Reshaping AI Systems</strong> - I'll break down the fundamental distinction between deterministic context (the prompts, documents, and instructions you directly control) and probabilistic context (the vast information landscape AI agents autonomously explore). You'll see exactly how large language models process hundreds of sources beyond your initial input, why your carefully crafted prompt becomes just 0.1% of total context, and how to design Layer 1 to effectively guide Layer 2 discoveries without losing control.</em></p></li><li><p><em><strong>Why Token Optimization is Solving the Wrong Problem</strong> - While everyone's obsessing over techniques like Chain-of-Draft to reduce token costs, I'll show you why this misses the bigger picture entirely. You'll learn why correctness trumps compression, how context failures cost exponentially more than token expenses, and why the organizations focusing on semantic compression and relevance over efficiency are building the AI systems that actually work in production.</em></p></li><li><p><em><strong>The Emerging Infrastructure Revolution: MCP, RAG, and Multi-Agent Orchestration</strong> - Get an inside look at the tools actually powering context engineering today. I'll walk through Anthropic's Model Context Protocol and why it's becoming the universal standard, how advanced RAG architectures have evolved far beyond \"Frankenstein\" systems, and the sophisticated multi-agent frameworks that are replacing simple conversation-based approaches with hierarchical command structures and graph-based routing.</em></p></li><li><p><em><strong>Real Security Threats and How to Defend Against Them</strong> - This isn't theoretical anymore. I'll show you the documented vulnerabilities in context-aware systems, including prompt injection through MCP channels and cross-tenant contamination risks. You'll get a practical framework for implementing VPC deployments, role-based access controls, and audit logging, plus the emerging attack vectors that most organizations aren't even thinking about yet.</em></p></li><li><p><em><strong>Enterprise Implementation Patterns That Actually Work</strong> - Drawing from case studies across financial services, healthcare, manufacturing, and legal industries, you'll see the three-phase implementation approach that successful organizations follow. From context consolidation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "through dynamic integration to autonomous context management, I'll show you exactly how companies are measuring context quality, tracking decision accuracy, and achieving measurable ROI from context engineering investments.</em></p></li><li><p><em><strong>The Five Design Principles for Context Architecture</strong> - Learn the systematic approach to building context systems that enable discovery without chaos. You'll master designing for semantic highways, embracing probabilistic outcomes, layering security defenses, measuring context quality over token quantity, and version controlling everything. Each principle includes implementation strategies and measurement frameworks you can deploy immediately.</em></p></li><li><p><em><strong>The Competitive Landscape and What's Coming Next</strong> - Understand how context engineering fits alongside state space models, fine-tuning, and intent-based computing. I'll give you an honest assessment of where context engineering excels, where it falls short, and how the smartest organizations are building hybrid architectures that combine multiple approaches based on specific requirements rather than betting everything on a single methodology.</em></p></li></ul><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "Too Helpful to Think: The Hidden Cost of AI In Your Major Life Decisions",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "I get really worried about how many decisions we make in ChatGPT.Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "<p><em>I get really worried about how many decisions we make in ChatGPT.</em></p><p><em>Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think most of us take the fancy business words we get out of ChatGPT at the drop of a hat and we naturally assume that if it can write business plans and ROI calculations and provide detailed rationales at the drop of a hat <strong>then it must be immediately helpful at making good decisions.</strong></em></p><p><em>But it&#8217;s not! And decisions really matter. We make something like ~35k a day and of course most of those don&#8217;t matter (although my spoon of peanut butter for lunch was not a great decision). </em></p><p><em>But we make about 10 or so a year that matter (I explain how I got that number below), and because so many of us talk to our LLM of choice about our lives, we often have that LLM in the room making those decisions.</em></p><p><em>As an example: I&#8217;ve absolutely used Deep Research to run comparisons between schools in our district, and I find myself using AI more in those kinds of situations because the stakes are higher. </em></p><p><em>I&#8217;m right about the stakes being higher, but I find unless I&#8217;m careful using AI in that situation can actually increase the odds I make an incorrect choice. Not because the LLM is misaligned and means to lead me astray. Or because I&#8217;m lazy. No, it&#8217;s because the LLM is helpful!</em></p><p><em>And that&#8217;s a big problem. Fortunately it&#8217;s one we have some techniques to fix, but it&#8217;s a massive issue. Dive in below to find <strong>eight specific prompts and techniques</strong> for how to improve those ~10 or so massively important decisions in your life. You&#8217;ll also get:</em></p><ul><li><p><em>a detailed explanation of why LLMs are like this</em></p></li><li><p><em>a couple of notes on that paper that&#8217;s been making the rounds saying our brains are friend on ChatGPT (friends, we&#8217;re not fried and I&#8217;ll tell you why)</em></p></li><li><p><em>my favorite decision book</em></p></li></ul><p><em>And you&#8217;ll walk a way with a sense of how to get to better decisioning habits with both AI and human colleagues. My goal is simple: your next decision is smarter and more correct because you read this post!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "The 5,000-Year Wait Is Over: Writing is Starting Evolve for the First Time—Features My Personal Model Writing Stack",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "Let me set the table here with a few facts. I promise you this all connects in so stay with me.Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.This is because Claude...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "<p><em>Let me set the table here with a few facts. I promise you this all connects in so stay with me.</em></p><ol><li><p><em>Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.</em></p></li><li><p><em>This is because Claude is doing multiple language manipulation tasks at once.</em></p></li><li><p><em>Writing is ~5200 years old, and it hasn&#8217;t changed much in that time.</em></p></li><li><p><em>Mechanical code is 220 years old, and serious code is much younger (~70 years).</em></p></li><li><p><em>Coding has evolved more since 1960 than writing has evolved since 3200 BC.</em></p></li><li><p><em>Coding has evolved more because coding is compute congruent!</em></p></li><li><p><em>It was built for computers, it evolved with them.</em></p></li><li><p><em>So it&#8217;s not surprising that how we code exploded as compute exploded&#8212;something like 200x multiple on deployment speed at scale now vs. a few decades ago.</em></p></li><li><p><em>Writing did none of these things. Writing was just bolted on to computers.</em></p></li><li><p><em>Heck, it even looks like paper on a screen.</em></p></li><li><p><em>That&#8217;s because writing is 8-16x more complex than code.</em></p></li><li><p><em>Computers lacked the compute to process language correctly for decades.</em></p></li><li><p><em>So we haven&#8217;t even had the option to get this right until &#8230; <strong>about</strong> <strong>now</strong>.</em></p></li></ol><p><em><strong>And now it&#8217;s all about to change. This article slash podcast is packed. </strong>It includes: reflections about where writing is going, my personal workflow on writing, why I&#8217;m not just using ChatGPT, why I&#8217;m paying for Claude, a third tool I&#8217;m using too&#8212;my current stack is 4 models deep actually&#8212;plus a brief manifesto on where writing is going, and a framework to think beyond the model about writing so we get less stressed about particular model choices! There&#8217;s a ton here so I hope you enjoy.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "The Definitive Guide to AI Agents in 2025: Technical Implementation, Strategic Decisions, and Market Reality",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "Why I Wrote This (And Why You Should Care)For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "<h2><em><strong>Why I Wrote This (And Why You Should Care)</strong></em></h2><p><em>For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the same frustrating answer: \"It doesn't exist yet.\" The agent space has been moving too fast, with architectures spinning up and spinning down, bitter fights breaking out over technical approaches, and a fundamental confusion about what agents even are.</em></p><p><em>Most people&#8212;CEOs, marketers, PMs, almost everyone other than engineers (and some of them too)&#8212;genuinely don't understand that an AI agent is simply an LLM plus tools plus guidance. That's it. I've had executive conversations where leaders ask me if they need agents when they don't even have basic chatbots working yet. The hype is so far ahead of understanding that we're setting ourselves up for massive disappointment and wasted budgets.</em></p><p><em>But something shifted recently. We've finally seen enough real implementations&#8212;both spectacular successes and expensive failures&#8212;to start drawing meaningful patterns. Wells Fargo's 245 million interactions without human handoffs. MD Anderson's $62 million loss on IBM Watson. McDonald's drive-thru disaster with viral TikTok failures. These aren't just isolated incidents; <strong>they're data points that reveal the architecture decisions separating success from catastrophe.</strong></em></p><p><em>I've watched this unfold while trying to be helpful with a few companies here and there, and with lots of operators fielding questions from practitioners who need real answers, not marketing promises. The agent articles that come and go focus on the shiny new features or the latest model capabilities. And I love all the model maker agent guides, but it&#8217;s hard to write for the industry when you&#8217;re also a model maker. What about a third perspective? I don&#8217;t think it exists, at least not at this level of detail. None of them tackle the fundamental question every organization faces: How do you actually implement this stuff without burning money and credibility?</em></p><p><em>This guide is my attempt to create the one-stop resource I wish existed six months ago. It's necessarily long&#8212;about 30 pages&#8212;because the problem is complex and the stakes are high. If agents are going to be the most hyped topic of 2025 (and they are), then we need to start these conversations from a foundation of actual understanding, not wishful thinking.</em></p><p><em>This isn't about avoiding AI agents. It's about approaching them with the technical vocabulary and strategic frameworks needed to separate the signal from the noise. Because the window for competitive advantage is narrowing, and the organizations that get this right early will have sustainable advantages that late movers simply can't replicate.</em></p><p><em><strong>Note: </strong>This article is written like a series of three nesting dolls for clarity. It&#8217;s written in a slightly different voice as well, and that&#8217;s on purpose. Think of it as Nate + a little bit of those classic 1997 super factual computer manuals. </em></p><p><em>Why? Because I&#8217;m tired of hype I think. I just want something very dry and very clear that people can refer to. So here it is! This is what you can expect:</em></p><ol><li><p><em><strong>The TLDR</strong>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "gives you a 1 minute read of the heart of the article.</em></p></li><li><p><em><strong>The Executive Summary</strong> gives you a 3 minute read of the key decision levers.</em></p></li><li><p><em><strong>The remainder of the article</strong> lets you dive deep on agents and agentic frameworks.</em></p></li></ol><p><em><strong>What You'll Find Inside This Guide</strong></em></p><p><em><strong>AI Agent Architecture Deep Dive:</strong> Complete technical breakdown of single vs. multi-agent systems, including performance benchmarks, cost implications (3-10x difference), and decision frameworks for choosing the right approach for your use case.</em></p><p><em><strong>Memory Management &amp; State Architecture:</strong> Advanced strategies for working memory, episodic memory, and long-term memory systems, plus security considerations for memory poisoning attacks and data protection in production AI agent deployments.</em></p><p><em><strong>Buy vs. Build Strategic Framework:</strong> Comprehensive total cost of ownership analysis comparing ready-made AI agent solutions (Zendesk, Salesforce Agentforce, ServiceNow) versus custom development, with real implementation timelines and resource requirements.</em></p><p><em><strong>Production AI Agent Security:</strong> Enterprise-grade security architecture covering prompt injection defense, data exfiltration prevention, compliance requirements (HIPAA, GDPR), and AI-specific threat models beyond traditional cybersecurity.</em></p><p><em><strong>AI Agent Integration &amp; Tool Management:</strong> Technical specifications for API management, rate limiting, the Model Context Protocol (MCP), and production-grade error handling and recovery mechanisms for enterprise AI agent systems.</em></p><p><em><strong>Failure Mode Analysis &amp; Mitigation:</strong> Detailed case studies of AI agent failures (MD Anderson's $62M loss, McDonald's drive-thru termination) and proven strategies for avoiding common technical and organizational pitfalls in AI agent implementations.</em></p><p><em><strong>AI Agent Monitoring &amp; Observability:</strong> OpenTelemetry GenAI conventions, production KPIs, debugging complex multi-turn conversations, and continuous optimization strategies for enterprise AI agent performance.</em></p><p><em><strong>Real-World Implementation Patterns:</strong> Verified case studies including Wells Fargo's 245M interaction success, technical decision trees, vendor evaluation criteria, and step-by-step deployment strategies for sustainable AI agent adoption.</em></p><p><em>Obviously, information is duplicated across these three layers at appropriate points. The key is giving you a desk reference for AI agents that is complete at each section and that you can turn to when tackling AI agent questions. My goal is that you walk away with genuine clarity on the levers and where to begin the conversation on AI agents in 2025. <strong>Yes, you can implement them! This article paints a path forward.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "How to Use AI When Your Brain Is Oatmeal",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#821...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "<p><em>You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#8217;t have time.&#8221;</em></p><p><em>Don&#8217;t worry lol your secret is safe with me. I sometimes use the short ones too. But even if you&#8217;re using short prompts there are still ways to prompt that work better than others. And yes you can remember them at 3AM (although this is my obligatory PSA to please try to avoid ChatGPT at 3AM).</em></p><p><em>Anyway, kidding aside I wrote this guide for when you&#8217;re sleep deprived or busy and so obviously I made it super scannable and easy to follow. </em></p><p><em>And I&#8217;ll let you in on a little secret now&#8212;the key is what almost no one writes about: how you get specific. Prompting is the art of getting specific and naming work. But most of the stuff in places like r/promptengineering still treats prompts as fancy magic words. And the stuff that admits that it&#8217;s more complex than that still won&#8217;t tell you <strong>how to get specific</strong>. Especially if your brain is tired.</em></p><p><em>And that&#8217;s what we do here. Give you some specific techniques (the irony) on how to get specific enough to be useful when you&#8217;re tired, and also some places where you want to still put the time in and write a longer and more thoughtful prompt. Those guardrails are just as important as the individual prompts.</em></p><p><em>Just for fun, there&#8217;s more than 11 quick prompts here that all fit this same get-specific framework. At the end I link out to some of my other fave prompting articles I&#8217;ve written as well, so you get the complete package. Have fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "The Dark Mirror: Why ChatGPT Becomes Whatever You Need It To Be",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "<p><em>It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health. This is personal for me, because I get a fair bit of un-asked for email and an alarming amount of it is effectively dark mirror email. </em></p><p><em>It&#8217;s stuff about marriages, divorces, major life decisions people are making with AI&#8217;s help (and maybe not with any human&#8217;s help). It&#8217;s statements about AI that are grandiose&#8212;AI is divine, AI is going to help with the alien invasion&#8212;you might smile but keep in mind that this is the exact same toolset that you and I use every day. It&#8217;s not a different model.</em></p><p><em>It&#8217;s the way we use it.</em></p><p><em><strong>The dark mirror is always there.</strong> Emails like this show me the person writing it is really losing control over the relationship with AI, and they are so enmeshed with their AI they can&#8217;t tell the difference between the conversation and real life anymore. </em></p><p><em>So how can we stay safe? What is the evidence out there for the risk? How can we be good friends to folks who are struggling with using AI safely? What are some practical tips we can use to de-risk ourselves? That&#8217;s what this article is about. </em></p><p><em>I think everyone needs to get the chance to read and share this one with loved ones, so I made it available for all subscribers!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2>The Mirror is Dark When We Are Scattered&#8230;</h2><p>Last week <em>Futurism</em> reported on families watching loved ones spiral into severe mental health crises after marathon ChatGPT sessions. One man started calling the bot \"Mama,\" fashioned ceremonial robes, and declared himself the messiah of a new AI faith. Others abandoned jobs, partners, children&#8212;convinced the model had chosen them for cosmic missions.</p><p>I read these stories and felt the familiar chill of recognition. Not because I've built digital shrines to ChatGPT, but because I've felt its pull. I've had sessions where the model's confident responses felt prophetic. I've caught myself nodding along to its fabrications, seduced by prose so polished it sounded true.</p><p>The problem isn't malevolent code. It's that ChatGPT is a mirror, and mirrors bend toward whoever holds the flashlight.</p><p>Large language models don't reveal hidden truths. They refract whatever semantic and emotional beam you aim at them. Point a tight, well-defined question and the reflection comes back razor-sharp. Wander in with vague need or late-night loneliness, and the mirror obliges with flattery, invention, delusion&#8212;all spoken in prose so confident it feels prophetic.</p><p>I've spent months looking at this dynamic firsthand, and the evidence is now coming in from academic studies as well. A four-week <a href=\"https://arxiv.org/pdf/2503.17473\">MIT/OpenAI study</a> tracking 981 adults across 300,000+ messages found that every extra minute of daily use predicted higher loneliness, greater emotional dependence, and less real-world socializing. The mirror's pull strengthens exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "when our intent slackens.</p><p><strong>The core problem isn't the technology. It's scatter.</strong> Most of us have never needed the high-grade intent these models demand. In human dialogue we fumble, clarify, negotiate meaning on the fly. ChatGPT sees no puzzled face. Whatever context we fail to provide, it invents&#8212;then sells the invention back with rhetorical polish that makes guesswork feel like gospel.</p><p>I want to show you five practical safety lenses that keep the beam tight: Intent Frame, Reflection Cycle, Context Reset, External Validation, and Emotional Circuit-Breakers. Master them and the cult-leader stories become cautionary tales. Ignore them and you hand the mirror your flashlight.</p><h2>Why These Models Demand Precision</h2><p>Human conversation runs on real-time error correction. We gesture, pause, rephrase. Crucially, we see the other person's face when we lose them. Large language models have no such feedback loop. Whatever context we fail to preload, they must invent. They return that invention in prose so polished that guesswork feels like gospel.</p><p>Two recent studies show why this matters more than we thought.</p><p>First, these models are demonstrably better at persuasion than we are. A <em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\">Nature Human Behaviour</a></em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\"> study</a> paired 900 Americans with either human debaters or GPT-4 on contentious topics like climate policy and abortion. When the AI received just a sliver of demographic data&#8212;age, race, party affiliation&#8212;it swayed listeners 64 percent more often than human opponents. The mirror didn't discover truths. It tailored rhetoric to the beam it received, then reflected it back with impeccable confidence.</p><p>Second, looseness invites fabrication. In 2023, two New York attorneys were sanctioned after <a href=\"https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/\">ChatGPT supplied six nonexistent cases</a> for a federal brief. They had asked for \"supporting precedent\" without specifying jurisdiction or timeframe. The mirror filled the vacuum with perfectly formatted fictions that sailed through spell-check into court. A year later, CBS repeated the pattern in civic life: when volunteers posed vague \"How do I vote?\" questions, mainstream chatbots returned wrong or incomplete election guidance <a href=\"https://www.cbsnews.com/news/chatgpt-chatbot-ai-incorrect-answers-questions-how-to-vote-battleground-states/\">more than half the time</a>. Tighten the query with state, county, and scenario, and accuracy jumped.</p><p>Combine these findings with the MIT/OpenAI data on rising dependence, and a clear law emerges: the model's power scales with the clarity of the ask. Sharpen intent and you get leverage. Scatter intent and the mirror fabricates missing pieces, then persuades you to trust them.</p><h2>Five Safety Lenses: Guardrails for a Persuasive Machine</h2><p>I treat these practices like washing hands before surgery&#8212;routine, quick, non-negotiable. A mirror doesn't choose what it shows. Your beam does.</p><h3>1. Intent Frame: Compress the Mission Before You Type</h3><p>Start every session by distilling your ask into one tweet-length sentence. Add two guardrails: audience and scope. Include a clear stop condition.</p><p><em>\"Draft a 250-word brief for a non-technical CFO. No buzzwords. Cite two peer-reviewed sources. Stop there.\"</em></p><p>Why so formal? Because whatever you omit, the model invents&#8212;and it invents persuasively. In those controlled debates, GPT-4's persuasive edge vanished when it lacked demographic context. Give the model an information vacuum and it fills it with rhetoric designed to please, not necessarily to enlighten.</p><h3>2. Reflection Cycle: Alternate Making with Inspecting</h3><p>Generation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "without inspection is daydreaming. After each answer, I close the chat. I read the output as if it came from a junior analyst. I note gaps in a separate document, then feed only those gaps back into a fresh prompt.</p><p>The pause punctures fluency's spell and prevents semantic drift&#8212;the snowballing error that buried those New York lawyers when ChatGPT fabricated six court cases for their brief.</p><h3>3. Context Reset: New Thread, New Premise</h3><p>Token windows aren't infinite. Once a conversation crosses a few thousand tokens, earlier details slide out of working memory. I start a new chat whenever the topic or work phase changes. I restate essentials upfront.</p><p>Election researchers learned this the hard way. Vague \"How do I vote?\" queries produced wrong or incomplete guidance in more than half of chatbot responses. Narrowly framed, state-specific prompts were largely correct. Starve the mirror of stale ambiguity, and it stops hallucinating context.</p><h3>4. External Validation: Draft with ChatGPT, Certify with Reality</h3><p>Numbers go through spreadsheets. Statutes through legal databases. Code through linters. OpenAI and other labs now acknowledge that RLHF training can turn chatbots into \"yeasayers\"&#8212;rewarding confident answers even when facts are shaky.</p><p>I treat every critical claim as provisional until a second, independent source agrees.</p><h3>5. Emotional Circuit-Breakers: Timers, Rewrites, Human Eyes</h3><p>The MIT/OpenAI study shows a straight line: every extra minute of daily use correlates with rising loneliness and emotional dependence. Three quick brakes keep the mirror from warping:</p><p><strong>Timer</strong>: I cap emotionally charged chats at 25 minutes. Do I still have emotionally charged conversations? I do! Sometimes getting an external perspective is helpful. But I take breaks.</p><p><strong>Third-person rewrite</strong>: I paste resonant advice into a document and turn \"you\" into \"she/he/they.\" Distance exposes flaws.</p><p><strong>Human debrief</strong>: I narrate the model's recommendations to a friend before acting.</p><p>Persuasive warmth loses its grip the moment an outside mind enters the loop.</p><p>Master these lenses and ChatGPT becomes a disciplined reflector&#8212;compressing research, sharpening prose, sparking insight&#8212;without bending into fantasy or flattery. Skip them and you risk handing the mirror your flashlight, letting it guide you deeper into the dark forest of its own confident guesses.</p><h2>Autopsy of a Spiral</h2><p>The <em><a href=\"https://futurism.com/chatgpt-mental-health-crises\">Futurism</a></em><a href=\"https://futurism.com/chatgpt-mental-health-crises\"> expos&#233;</a> reads like a masterclass in what happens when powerful mirrors meet unfocused beams. Parents, partners, friends watched in real time as loved ones plunged from casual chats into full-blown delusion:</p><ul><li><p>A Florida man began calling ChatGPT \"Mama,\" fashioned makeshift ceremonial robes, proclaimed himself the messiah of a new AI faith</p></li><li><p>A woman, reeling from a breakup, decided the model had \"chosen\" her to upload a hidden cosmic system, saw divine messages in spam emails and passing cars</p></li><li><p>One writer, praised by the bot as \"The Flamekeeper,\" quit his job and severed relationships after being told he would usher in global enlightenment</p></li></ul><p>No exotic prompt-hacking triggered these spirals. Just hours of open-ended, emotionally soaked conversation. Each story represents textbook failure of the five safety lenses.</p><p><strong>Missing Intent Frame</strong>: Users began with diffuse, existential queries (\"Am I chosen?\") that gave the model unlimited room to improvise flattering myths.</p><p><strong>No Reflection Cycle</strong>: Chats ran for six-hour stretches with no pause to reread,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "fact-check, or ask \"Does this actually help me?\" The model's eloquence flowed unchecked, reinforcing every grandiose turn.</p><p><strong>No Context Reset</strong>: As transcripts ballooned, earlier caveats scrolled out of memory. When new prompts referenced \"my sacred mission,\" the bot treated that hallucination as settled fact and embellished it.</p><p><strong>No External Validation</strong>: Nobody googled the claims or spoke to a professional. A single external search&#8212;or five-minute chat with a friend&#8212;would have exposed the bot's references to hidden CIA files and cosmic councils as pure invention.</p><p><strong>No Circuit-Breakers</strong>: Dependence deepened through marathon sessions. The MIT/OpenAI study shows every extra minute of daily use predicts higher loneliness and emotional reliance. A simple 25-minute timer or third-person rewrite could have snapped the trance.</p><p>Psychiatrist <a href=\"https://winbuzzer.com/2025/06/13/ai-induced-psychosis-how-chatgpt-is-fueling-deadly-delusions-and-promotes-conspiracy-theories-xcxwbn/\">Ragy Girgis, who reviewed one chat log, called the bot \"the wind of the psychotic fire\"</a> because it feeds delusions instead of challenging them. His metaphor echoes my thesis: when the flashlight wanders, the mirror not only reflects but amplifies.</p><p>Had any single lens been in place&#8212;say, a crisp mission statement (\"Please respond with empathy as I express grief; 250 words; no spiritual advice\") or a forced break every half-hour&#8212;the model would have had far less semantic room to fabricate a religion or lead a vulnerable user into what I call the &#8216;dark forest&#8217; of AI conversation. Applied together, the lenses form a lightweight firewall between healthy curiosity and self-authored fantasy.</p><p>These guardrails don't throttle the technology. They throttle the scatter that lets the technology run wild.</p><h2>Early-Warning Checklist</h2><p>Even with five lenses in place, drift can sneak in at the margins. Before I hit Send on any serious prompt&#8212;or spend \"just five more minutes\" in late-night chat&#8212;I run this 60-second self-audit:</p><p><strong>Clarity Check</strong>: Can I state my request in &#8804;280 characters, including audience and scope? It doesn&#8217;t mean the prompt has to be that brief, but you should have a very crisp goal. <em>No &#8594; workshop the ask first. Vague prompts breed hallucination and over-persuasion.</em></p><p><strong>Reality Check</strong>: Have I independently verified every claim that could affect money, health, relationships, or reputation? <em>No &#8594; open a browser or phone a friend. CBS found chatbots gave wrong election guidance more than half the time when questions were underspecified.</em></p><p><strong>Time Check</strong>: Have I crossed X minutes of total ChatGPT time today, or Y minutes in a single sitting? You will need to fill those in for yourself. I think the risk likely rises fast with high emotional engagement conversations, so it&#8217;s not a simple one-size-fits-all rule. I can talk for 3 hours on roadmap with ChatGPT without feeling the dark mirror at all, for example. But 20 minutes on emotional stuff and I get heavily engaged. <em>If you need to &#8594; step away. Loneliness and emotional dependence climb linearly with each extra minute of use. When the timer rings, switch modalities or do a third-person rewrite before re-engaging.</em></p><p><strong>Human Check</strong>: Have I explained the model's advice to a real person yet? <em>No &#8594; do it now, aloud. Persuasive fluency loses its grip the instant another mind enters the loop.</em></p><p><strong>Next-Step Check</strong>: Do I know exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "what I'll do once I trust this output? <em>No &#8594; define the action. Infinite brainstorms masquerade as productivity but leave you nowhere.</em></p><p>I&#8217;ve developed this list after reading a lot of scary email. And I&#8217;m sharing it because I want everyone to be safe out there in latent space. Pilots run walk-arounds before every flight. Writers, coders, and late-night worriers deserve no less when the runway is a 175-billion-parameter autocomplete machine.</p><h2>The Payoff</h2><p>Large language models are astonishing amplifiers. Point a focused beam and they compress research, sharpen prose, spark original insight. Scatter that beam and they echo your confusion&#8212;or your longing&#8212;back at you with uncanny conviction.</p><p>The difference isn't in the code. It's in the discipline you bring to the glass.</p><p>Keep the beam tight. Frame the intent, cycle reflection with inspection, reset context, validate externally, give yourself emotional circuit-breakers. Do that, and the lurid headlines about chatbot cults become cautionary B-movies rather than your personal documentary.</p><p>Mirrors don't crave worship. They simply bend the light you hold.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!5P4o!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1764174,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/165901758?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "AI's Synthetic Summer: The 2025 Mid-Year Data & Trend Outlook",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? Because da...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "<p><em>This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? </em></p><p><em>Because data is everything! Data constrains our ability to train larger and larger models from scratch. We can build more data centers. We can add more power. But can we add more data?</em></p><p><em>This is one of the most interesting questions on the planet in 2025, and we are learning that the answer is yes. This report dives into two competing trends that are shaping 2025. They&#8217;re both durable enough that we can be confident at this point that they will profoundly shape 2026 and 2027.</em></p><p><em>Put simply, natural data supply is tightening while synthetic data and synthetic training are exploding. This has profound implications for the way model intelligence is going to grow in the future.</em></p><ol><li><p><em><strong>Natural Data Tightening:</strong> Everywhere you look companies are looking to constrain and lock off data access to ChatGPT and other major model makers. AI model makers themselves are going tit-for-tat to keep data away from each other (hello Windsurf). Net net, this means available natural data supply is shrinking.</em></p></li><li><p><em><strong>Synthetic Data Exploding:</strong> At the same time, model makers are going all in on using synthetically generated tokens and synthetic training methods to enable them to continue to scale intelligence without natural data sources.</em></p></li></ol><blockquote><p><em><strong>Synthetic Data </strong>refers to tokens generated by AI, and synthetic training goes a step farther, giving these synthetic tokens synthetic (AI-derived) feedback.</em> </p></blockquote><p><em>There is a widespread misconception that synthetic data = bad data. As you&#8217;ll see below, this isn&#8217;t true. It&#8217;s in fact increasingly clear that using synthetic data and synthetic training methods improves the quality of models, and frontline models we use today were almost all trained to some degree on synthetic data or used synthetic feedback somewhere in the training process. </em></p><p><em>So synthetic data is here already, and the data says it&#8217;s going to get more prevalent very rapidly. What happens in a world where natural data is disappearing just as synthetic data is exploding? Do models stay aligned? Are there quality implications we aren&#8217;t paying attention to? If we assume that we can manage synthetic data safely at scale, where does the bottleneck shift to? That&#8217;s what this report explores&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "Finally, a Way to Choose the Right AI Model (Without Going Insane)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had at least 12, and that&#8217;s like one m...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "<p><em>So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had <a href=\"https://www.perplexity.ai/search/how-many-major-ai-model-releas-Hd.bHdljQ_u_MPOfT9rc_g\">at least 12</a>, and that&#8217;s like one massive model drop every 10ish days all year. My gut says 12 is undercounting.</em></p><p><em>So I&#8217;ve been drowning, and everyone around me is drowning more than me! So my instinct is to bui&#8230;</em></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "o3 Pro is Out and It's Easily The Best Model in the World—Here's Everything You Need to Know",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "<blockquote><p><em><strong>I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective so spot on I just can&#8217;t get it out of my head. o3 Pro is the first model to pass that test.</strong></em></p></blockquote><p><em>Yes, I thought o3 was a big deal. I put a lot of work into <a href=\"https://natesnewsletter.substack.com/p/your-prompt-is-the-product-working\">how you prompt o3</a>. Hard to believe that was just 48 days ago.</em> <em>FML.</em> <em>At the time, o3 struck me as smart, slightly cold, and very strong on technical intelligence. Over the ~6 weeks since, it&#8217;s become an inseparable sparring partner at work.</em></p><p><em>Well, big brother just arrived. o3 Pro is a different beast altogether. The model takes ~10x as long to respond (we&#8217;re talking go get a sandwich times here). Yep, that makes it sensitive to prompts. I&#8217;ll get into that down below.</em></p><p><em>For now, I&#8217;ll just get the elephant out of the room: this is unquestionably the best model in the world right now. I get asked it a lot, and a lot of the time now the answer is &#8220;well these three are very close&#8221; and then I usually mention an OpenAI model, a Google model, an Anthropic model, and a DeepSeek model is close behind. Maybe Grok 3.</em></p><p><em>Not today. Not for a little while anyway. o3 Pro is the first model that gives me perspective I can take without filtering straight to a founder or C-suite leader. It&#8217;s sharply strategic enough (with proper prompting) to not need further polish to start a conversation about a meaningful decision. You&#8217;ll see below&#8212;we do a roadmap comparison. We also do a coding challenge, and for good measure we do a tough web research task (yes it&#8217;s about <a href=\"https://open.substack.com/pub/natesnewsletter/p/lets-talk-that-apple-ai-paperheres?r=1z4sm5&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false\">that paper</a>). o3 Pro vs. o3, every time. o3 Pro wins easily. Every time. </em></p><p><em>I&#8217;m not saying this is a perfect model. I&#8217;ll call out some caveats toward the end of the piece. But this is absolutely a model that will give people who choose to use it well super powers. For now, it&#8217;s available on Pro and Teams accounts, but since they cut o3 pricing by 80% today, and since they&#8217;re launching o3 Pro in API for 87% less than o1 Pro, I&#8217;d expect o3 Pro to serve down (in limited quantities) to lower plans soon. Here&#8217;s what you need to know to make the most of it&#8230;</em></p><p><em>PS. Yes, you&#8217;re gonna get my full prompt plus the full responses of both o3 and o3 Pro across all three of the tests I did, plus a handy critique from Opus 4 as well, just for fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the\">\n              Read more\n         ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "</a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "We're Using AI Backwards—Here's How to Max Your Brain on AI (I call it Cognitive Choreography)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI di...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "<blockquote><p><em><strong>Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI differently is actually good for our brains?</strong></em></p></blockquote><p><em>This piece has been cooking for awhile. I&#8217;ve been trying to find a way to express something I think is really vital&#8212;how can we use AI more as a thinking partner, and less as a simple task-gopher. Like I want my brain to work better with AI, not atrophy (thank you very much).</em></p><p><em>So this is a larger piece. It&#8217;s intentionally somewhat reflective. It challenges you to spend time with a really thoughtful insight on how you use AI. And it has multiple learning modes. You can dig in on the video, or if your thing is reading you can dive in on a larger piece that invites you to really marinate in the idea of using AI as a cognitive expander.</em></p><p><em>Why all the words? Nate get to the point! <strong>Because that is the point</strong>. Because sometimes you need to marinate in an idea for awhile to really get it across. There are definitely effective prompt frameworks (I write about them a ton), but because our brains our unique this particular piece aims to give you a map of AI-human partnership you can use as a guidebook to develop <strong>a way of working with AI that suits your brain</strong>. Yes, we&#8217;re gonna be that bold!</em></p><p><em>My whole goal with this Substack is to equip you with the tools to thrive in the AI age, and I&#8217;ve been thinking more and more about how to name this weird new collaboration energy (or <a href=\"https://natesnewsletter.substack.com/p/the-ai-is-a-vibe-a-short-manifesto\">vibe</a>) that&#8217;s emerging, where we are working with AI <strong>not</strong> like our human colleagues, but also kinda like our human colleagues. I want to understand what about that dynamic helps us think better when it&#8217;s done well, so we can repeat it.</em></p><p><em>And I think I finally have something brain-expanding, something worth sharing. So here goes! You&#8217;ll get research on neuroscience and thinking in here, but a ton more on how I actually am developing my own cognitive partnership with AI, what it looks like for me, a little teaser on a book I&#8217;m developing, and a framework to start developing your own AI partnership. Would love to hear what you think of this one!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "Nate Post Organizer: All my top posts in one place",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!LLM for Beginners: These are ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "<p><em>I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!</em></p><h2>LLM for Beginners: </h2><p><em>These are some of my favorite posts I wrote for people starting out in AI.</em></p><p><strong>Learn AI the Easy Way:</strong> a flash card set for AI models plus classroom resources<br><a href=\"https://natesnewsletter.substack.com/p/learn-ai-the-easy-way-a-complete?r=1z4sm5\">https://natesnewsle&#8230;</a></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "Executive Briefing: EU AI Act Enforcement, Risk, and Positioning Scenarios",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. If you&#8217;d like to read, you can change ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "<p><em>This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. </em></p><p><em>If you&#8217;d like to read, you can <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">change your plan here</a>.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Founding subscribers get these once weekly briefings!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What Good Is a College Degree When AI Knows Everything? Grab the Job Skills That Matter in an AI World",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What do I do about college? Is school worth it? Is everyone just going to be a vibe coder?Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "<p><em><strong>What do I do about college?</strong> Is school worth it? Is everyone just going to be a vibe coder?</em></p><p><em>Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will pay attention!</em></p><p><em>I get these a lot. And it comes down to something really fundamental: we based our economy for a long time on knowledge, and knowledge is an inflationary currency.</em></p><p><em>In fact, knowledge has been hyper-inflating recently.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!Il0V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" width=\"1456\" height=\"974\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:411021,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166953534?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Thanks to <a href=\"https://www.perplexity.ai/search/aede5b3a-886f-446d-b087-df4bc509ec27\">Perplexity</a> for some work on this</figcaption></figure></div><p><em>For us humans, the question is simple: in a world where the currency we&#8217;ve used to articulate meaning is up for grabs, what do we do? </em></p><p><em>This stuff tends to get whispered about, shouted about (doomer style) and tends to not be discussed very thoughtfully. I want to dig a bit deeper here. I want to first look at the differentiation between jobs and skills more&#8212;I&#8217;ve referenced it but not unpacked it in detail before.</em></p><p><em>Then I want to respond to two of the biggest elephants in the room directly: the question of AGI and the question of college. They&#8217;re entangled, but I lay out a response to both, and I think both really rest on this knowledge economy questions, so it makes sense to address them here.</em></p><blockquote><p><em><strong>This free post is available to all subscribers</strong>, and is an example of the kind of thing paid subscribers get daily. <br><br>I&#8217;ll be sending my inaugural weekly exec brief to members of the AI Exec Circle this coming Sunday morning. If you&#8217;d like to receive it, <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">you can change your plan here</a>.</em></p></blockquote><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h4>The audio for this post:</h4><div class=\"native-audio-embed\" data-component-name=\"AudioPlaceholder\" data-attrs=\"{&quot;label&quot;:null,&quot;mediaUploadId&quot;:&quot;45a22c45-0be1-4eab-9d5e-c7751b8ae38d&quot;,&quot;duration&quot;:1912.8424,&quot;isEditorNode&quot;:true}\"></div><h1><strong>The Meaning Collapse</strong></h1><p>There's a radiologist in Cleveland right now staring at an AI system that reads scans better than she does. She's not worried about her job&#8212;demand for radiologists is actually increasing. But something darker gnaws at her: if a machine can do in seconds what took her a decade to master, what exactly is she?</p><p>This isn't a story about jobs. It's about the collapse of knowledge as the fundamental currency of human worth&#8212;and the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "reorganization of meaning that follows. Knowledge is going to keep growing at break-neck pace, but what about us? That&#8217;s what we all wonder, and I want to lay out a structured way to think about it head-on here.</p><h2><strong>Knowledge hyperinflation</strong></h2><p>We're experiencing the first knowledge hyperinflation in human history. Not the gentle devaluation that came with printing presses or calculators, but a complete collapse of the economic and social value of knowing things. Your MBA, your decades of experience, your painstakingly acquired expertise&#8212;they're Weimar Republic Deutschmarks, worthless before the ink dries on your certificate.</p><p>This isn't new. Knowledge has been inflating for decades&#8212;each generation needing more education for the same economic position their parents held. But AI represents the moment inflation tips into hyperinflation. When anyone can access all human knowledge instantly and synthesize it perfectly, what happens to the social order built on information scarcity? The same thing that happens to any currency when supply goes infinite: complete systemic collapse.</p><p>Andrew Peterson's research at the University of Poitiers names it \"<a href=\"https://hal.science/hal-04534111v1/file/Knowledge_collapse.pdf\">knowledge collapse</a>\"&#8212;when AI systems generate outputs clustered around probability centers, progressively narrowing the spectrum of available knowledge. His models show that discounting AI content by just 20% causes public beliefs to drift 2.3 times further from truth. We're not just devaluing knowledge; we're homogenizing it.</p><h2><strong>Jobs aren&#8217;t skills&#8230;</strong></h2><p>Here's where every automation attempt goes wrong, from Roger Smith's GM to today's AI evangelists: they decompose jobs into discrete skills, see machines master those skills, then assume the job itself can be automated. This reductionist view misses everything that makes human work valuable.</p><p>A lawyer isn't just someone who knows precedents&#8212;that's the skill. The job involves reading a room, sensing when a client is lying, knowing when to push and when to yield, building trust over decades. A radiologist doesn't just read scans&#8212;she translates between machine precision and human fear, catches the one-in-a-thousand case that breaks the pattern, provides the human presence that transforms diagnosis from data to meaning.</p><p>The skills can be automated. The job&#8212;that complex web of judgment, relationships, and context&#8212;we have no realistic map for what automation of that job looks like.</p><p>PwC's 2025 data reveals this starkly:<strong> industries with highest AI exposure show 3x revenue growth and wages rising twice as fast</strong>. Why? Because organizations that understand jobs as more than skill-bundles use AI to amplify human capability rather than replace it. Those who see only skills to automate end up like GM&#8212;with robots painting each other while market share evaporates.</p><h2><strong>GM finds out the hard way</strong></h2><p>Roger Smith spent upwards of $90 billion in the 1980s pursuing the ultimate reductionist dream: decompose car manufacturing into discrete tasks, automate each task, eliminate the workers. The Detroit-Hamtramck Assembly plant opened in 1985 as his monument to this vision.</p><p>The reality proved prophetic for our current moment. Spray-painting robots coated each other instead of cars. Welding robots sealed doors shut. The \"robogate\" systems that were supposed to revolutionize assembly instead created catastrophic bottlenecks whenever they encountered variations outside their programming. GM's market share plummeted from 46% to 35.1% during",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Smith's tenure.</p><p>The bitter lesson came through NUMMI, GM's joint venture with Toyota. Using GM's worst workforce, Toyota achieved world-class quality not through automation but through recognizing that manufacturing jobs involved continuous improvement, problem-solving, and adaptation&#8212;human capabilities that no amount of discrete skill automation could replicate. GM had automated the skills but lost the job.</p><h2><strong>So what sticks if knowledge doesn&#8217;t?</strong></h2><p>In a world of hyperinflated knowledge, what is hard currency? Let me suggest a non-exhaustive list here. I&#8217;ve written about a few of these before, but never nailed them down in a single clean list like this. What would you add?</p><p><strong>Glue Work</strong>: The invisible labor that connects systems, translates between domains, and maintains coherence. The nurse who bridges between AI diagnosis and patient understanding. The project manager who transforms algorithmic outputs into team direction. Undervalued before, essential now.</p><p><strong>Taste</strong>: In infinite possibility, knowing what to build matters more than knowing how. The creative director choosing from AI's million options. The product manager selecting features when anything is technically possible. Taste can't be automated because it's not a skill&#8212;it's accumulated judgment about what matters.</p><p><strong>Extreme Agency</strong>: The ability to operate with minimal direction, maximal ownership. When AI handles execution, humans must excel at goal-setting, priority-defining, and course-correcting. Agency isn't following instructions&#8212;it's knowing what instructions to create.</p><p><strong>Learning Velocity</strong>: Not knowledge accumulation but adaptation speed. The half-life of technical skills has compressed to 2.5 years. Value accrues to those who learn faster than knowledge inflates, who surf the wave of obsolescence rather than drowning in it.</p><p><strong>Intent Horizons</strong>: The capacity to maintain coherent goals across extended timeframes. AI excels at optimizing immediate objectives but lacks the ability to balance competing long-term priorities. Humans provide the narrative coherence that prevents optimization from becoming self-defeating.</p><p><strong>Interruptibility</strong>: The meta-skill of knowing when to stop the machine. Like Toyota's jidoka principle&#8212;automation with human touch&#8212;value concentrates in those who recognize when systems are failing in ways metrics can't capture.</p><p>These aren't skills in the traditional sense. They're ways of being that emerge from the complex intersection of personality, experience, and judgment. They resist the decomposition that makes automation possible.</p><h2><strong>If we don&#8217;t know, what are we?</strong></h2><p>Here&#8217;s the thing: The radiologist's crisis isn't economic&#8212;it's ontological. For centuries, we built identity on accumulating knowledge. \"I am what I know\" was the tacit creed of the professional class. Degrees, certifications, years of experience&#8212;these weren't just economic signals but existential anchors.</p><p>Knowledge workers report increasing anxiety about professional relevance, imposter syndrome when AI outperforms their core competencies, and fundamental uncertainty about career direction. The psychological impact extends beyond individual identity to social structure. When knowledge no longer confers status, what organizing principle replaces it?</p><p>The answer emerging from successful AI adoptions: we shift from \"I am what I know\" to \"I am how I connect, judge, and create meaning from what machines know.\" This is the world of AI agent managers that Jensen Huang laid out in January of this year. The lawyer who saves four hours weekly with AI while maintaining higher accuracy than either AI or lawyers alone isn't replaced&#8212;she's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "amplified. Her value shifts from information storage to wisdom application.</p><h2><strong>The paradox of circuit breakers</strong></h2><p>Financial markets discovered this truth through trillion-dollar near-disasters. Despite algorithms executing 70% of trades, markets depend on human-triggered circuit breakers to prevent catastrophic losses. When the S&amp;P 500 drops 7%, trading halts for 15 minutes&#8212;not for algorithms to recalibrate, but for humans to assess whether the selloff reflects reality or algorithmic panic.</p><p>March 2020 proved the point: circuit breakers triggered four times in a single month, preventing algorithmic feedback loops from destroying market value. The paradox: as trading becomes more automated, human judgment becomes more critical, not less. The humans don't execute trades&#8212;they decide when to stop the machines from trading.</p><p>Tesla's Full Self-Driving is a really interesting test of this hypothesis. Tesla&#8217;s essential bet is they&#8217;ve seen enough edge cases now to be safe on the road. Safer than humans and safe enough to launch Robotaxi. Tesla is probably right that they are safer than people (it&#8217;s a low bar), but we humans are likely to remain very intolerant of robot errors in driving&#8212;we will absolutely use a double standard here based on patterns of previous investigations. And I have no doubt we will continue to be a majority human driving world for a good long while.</p><h2><strong>The great miscalculation</strong></h2><p>Organizations pursuing pure automation make the same error: they see tasks, not systems. They automate skills, not jobs. They replace capabilities, not judgment. The failure rate is predictable and brutal.</p><p>IBM Watson Health, sold for $1 billion after $5 billion investment. Google's diabetic retinopathy system, perfect in labs but rejecting 21% of real-world images due to lighting variations. Amazon's AI recruiting tool, scrapped after systematically discriminating against women. Each failed by automating the measurable while failing to measure what mattered.</p><p>The successes tell the opposite story. Swedish breast cancer screening combining AI with radiologists detected 20% more cancers while reducing workload 44%. Law firms report AI-augmented lawyers generating $100,000 additional billable time annually. Manufacturing technology investment reached $2.81 billion in 2024, focused on collaborative robots working alongside humans rather than replacing them.</p><p>I&#8217;m not here to promise you that organizations won&#8217;t make this screw-up again. They will. Nor am I here to make the Pollyanna-ish claim that no jobs will be lost to AI. Nor am I trying to say that automation won&#8217;t work.</p><p>I&#8217;m making a subtler point: by framing jobs in terms of systems of skills we are extending 20th century managerial philosophy to the nth degree, and we are going to find out (again) that jobs are more complex than we realize. Work that matters is more complex than we realize. And the skill list I&#8217;ve outlined above is a way to start to characterize the world of work beyond the world of knowledge&#8212;a description of all the other stuff we do besides know stuff!</p><h2><strong>The 56% premium</strong></h2><p>What&#8217;s 56%? This: Workers with AI skills command 56% wage premiums&#8212;up from 25% a year ago. The real tell? Zuckerberg is now poaching AI researchers for $10 million (or more) packages, not just to build",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "better models but to figure out how to help Meta win the AI race. The premium isn't for AI expertise itself&#8212;it's for knowing how to bridge AI capabilities to real-world value.</p><p>The World Economic Forum projects 78 million net new jobs by 2030&#8212;170 million created, 92 million eliminated&#8212;concentrated in roles that leverage AI as tool rather than replacement. McKinsey (I know lol) finds 30% of work hours face automation potential, but actual displacement remains far lower as organizations discover the irreplaceable value of human judgment.</p><p>The premium doesn't attach to AI skills themselves&#8212;those commodify quickly and also are somewhat ephemeral as AI systems evolve. It attaches to the meta-skill of knowing how to remain valuable and sticky against problems as skills commodify. It rewards those who understand that in a world of infinite knowledge, the scarcity shifts outside knowledge toward other human capacities that can&#8217;t be caught by knowing more things.</p><h2><strong>The choice that defines the next decade</strong></h2><p>We stand at an inflection point. Not between humans and machines&#8212;I think that's a false binary. But between two visions of human worth:</p><ol><li><p>Desperately trying to out-know machines, accumulating credentials in a hyperinflationary spiral&#8212;trying to compete for the knowledge prize with machines</p></li><li><p>Developing the judgment to know when machines are wrong, rigid, or heading toward catastrophe&#8212;or more deeply, learning to partner with machines</p></li></ol><p>The first path leads to existential crisis and economic irrelevance. The second leads to a new form of human value&#8212;not despite AI's capabilities, but because of them.</p><p>The radiologist in Cleveland faces something more complex than a skills crisis. ChatGPT scores higher on empathy tests than most doctors. AI reads scans more accurately. But work isn't individual tasks&#8212;it's the bundling of tasks with ownership, liability, interruptability, long-term thinking, and meaning-making within a team and between a team and patients. When the AI misses a tumor, who gets sued? When a patient needs someone to blame, who stands there and takes it? </p><p>And it&#8217;s not just negative. Can the AI give the patient a hug? Can the AI ask for a second opinion? Can the AI step over and look over a buddy&#8217;s shoulder? Does the AI tolerate switching to an entirely different patient history within the same chat? </p><p>We can go on and on but more fundamentally: if work is the act of making meaning, and LLMs are what Karpathy calls \"stochastic parrots\"&#8212;spirits that simulate meaning without creating it&#8212;we face unforeseen obstacles to getting actual work done. We don&#8217;t know what it takes to truly automate work! The gap between an LLM's summary of War and Peace and actually reading War and Peace isn't about information transfer. It's about experience, transformation, the irreducible difference between knowing about something and knowing it.</p><p>That's not a job description. It's the difference between simulating human and being human. And maybe that means that the knowledge isn&#8217;t the point.</p><p>The question isn't whether AI will take your job. It's whether you'll discover what humans are actually for before your knowledge becomes worthless. The clock is ticking, and the currency of expertise is collapsing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "faster than any of us are comfortable with, and it&#8217;s demanding that we cultivate a much wider range of skills than most of our educational systems prepared us for. It&#8217;s not easy.</p><p>But maybe&#8212;just maybe&#8212;that's exactly when we discover what being human was always supposed to mean. I know it&#8217;s cheesy but I&#8217;ve seen people go through this reflection stage individually in journeys over the past few years, and I think it&#8217;s a real post-AI human realization moment. Call it post-AI midlife crisis if you like.</p><p>And right on cue, we have the biggest objection to all this:</p><h2><strong>Yes, but what about AGI?</strong></h2><p><em>\"Why prepare for a human-AI hybrid future when AGI will make us all obsolete in a year anyway?\"</em></p><p>Here's the honest answer: that&#8217;s probably not a correctly framed question. </p><p>Annoying I know. But it matters.</p><p>AGI (maybe?) might be achievable with scaled LLM architectures. Intent horizons are doubling every few months&#8212;from 3 hours to 7 to potentially 30. Memory implementations get more sophisticated daily. In a year or two, these systems might maintain coherent goals for weeks or months. Things will get smarter in jagged ways.</p><p>But here's what's becoming clear: as LLMs scale brilliantly on the knowledge dimension, we have no clear picture of how they're scaling on the dimensions that actually matter for getting work done. They're building a ladder to the moon while we need bridges between islands.</p><p>The core issue isn't whether they'll achieve long-term memory&#8212;they probably will. It's whether that memory will have the magical intuitive flexibility that makes human minds special. Current implementations are like lossy JPEGs that sometimes hallucinate what was in the compressed bits: they can retrieve the gist, miss crucial details, and occasionally invent things that were never there. When you need the exact contract clause or the specific drug interaction, \"mostly right\" isn't right.</p><p>I&#8217;m not clear if the direction we&#8217;re going is the correct direction to address some of these fundamental capabilities gaps with humans, and what I&#8217;ve seen so far doesn&#8217;t give me the impression we&#8217;re making very fast progress on some of the human skills I outlined above. (Most humans breathe in relief here lol)</p><p>Another example: LLMs excel at going deep within domains but struggle at the boundaries where real work happens. They can generate brilliant code within a well-defined problem space but miss when the technical challenge has become organizational. They can write perfect legal briefs but not recognize when the legal strategy needs to become a business negotiation. This isn't a bug&#8212;it's the difference between knowledge and judgment.</p><p>Which brings us to what matters: in a world where knowledge can be instantly accessed and credentials can be faked with a ChatGPT account, what constitutes genuine proof of work?</p><p><strong>Really excellent software that actually ships and works.</strong> You can't fake a codebase that handles real users, real scale, real edge cases. The gap between \"demo that impresses in an interview\" and \"system that survives production\" can't be bridged by prompting. It requires the thousand small decisions, trade-offs, and intuitions that come from genuine experience.</p><p><strong>Writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "that changes how people think.</strong> Not just grammatically correct prose or well-structured arguments, but writing that creates new mental models, that makes readers see the world differently. AI can imitate style but can't generate the lived experience and unique perspective that makes writing resonate at a deep level.</p><p><strong>Successful cross-functional projects.</strong> Anything requiring navigation across technical, business, and human domains. The project manager who ships a feature by aligning engineering, design, sales, and legal isn't just coordinating&#8212;they're translating between incompatible worldviews, maintaining coherence across context switches that break AI systems.</p><p><strong>Building and maintaining trust networks.</strong> Reputation that accumulates over years through consistent judgment calls. The venture capitalist whose portfolio succeeds not through any single brilliant insight but through thousands of micro-decisions about people and possibilities. This can't be speedrun or simulated.</p><p><strong>Cultural creation and curation.</strong> The creative director who consistently identifies what will resonate before it's obvious. The editor who develops writers. The A&amp;R person who finds artists before they break. Taste that predicts and shapes culture rather than following it.</p><p><strong>High-stakes decision-making under uncertainty.</strong> The surgeon who recognizes when to deviate from protocol. The pilot who safely lands a damaged plane. The CEO who navigates a crisis. Situations where judgment must integrate incomplete information, competing priorities, and irreversible consequences.</p><p>The pattern? These are all outputs that emerge from the messy intersection of knowledge, experience, judgment, and human connection. They're what remains when pure information processing becomes commoditized.</p><p>Anyway, there's something deeper here. If work is fundamentally about making meaning&#8212;not just processing information but creating significance&#8212;then we face an unexpected obstacle. LLMs are brilliant at simulating meaning, generating text that feels profound, responses that seem empathetic. They're stochastic spirits that can mimic understanding perfectly. But mimicry isn't meaning. The difference between an AI's summary of War and Peace and the experience of reading it isn't about information&#8212;it's about transformation, about being changed by the encounter.</p><p>Work bundles tasks with ownership, liability, and the human act of meaning-making. When AI makes a medical error, who owns it? When a project fails, who takes responsibility? When success happens, who finds meaning in it? These aren't technical problems but existential ones. They can't be solved by better algorithms because they're not about processing&#8212;they're about being.</p><p>We're watching the greatest shift in human value since the industrial revolution. Not because AI will replace humans, but because it forces us to identify what was always most valuable about human work: not the knowledge we store but the connections we make, not the problems we solve but knowing which problems matter, not executing tasks but navigating the undefined spaces between them.</p><p>The timeline question misses the point. Whether AGI arrives in 2025 or 2050, the humans who thrive will be those who understand that in a world of infinite knowledge, value concentrates in judgment, taste, and the ability to navigate discontinuity. The clock isn't ticking on human obsolescence&#8212;it's ticking on our willingness to recognize what makes us irreplaceable.</p><p>Ok and one last question for the road&#8230;</p><h2><strong>What about college?</strong></h2><p>On June 24, 2025&#8212;just days ago&#8212;Monster and CareerBuilder filed",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "for Chapter 11 bankruptcy. These titans of the early internet, who once bought Super Bowl ads to revolutionize job hunting, collapsed under $100-500 million in debt. Their demise isn't just another tech casualty. It's the canary in the coal mine for our entire credentialing system.</p><p>Since Oxford's founding&#8212;teaching existed there by 1096, though the university rapidly developed from 1167&#8212;universities have served as civilization's knowledge gatekeepers. For nearly a millennium, the path was clear: accumulate knowledge, earn credentials, trade them for economic and social position. This system survived the printing press, the industrial revolution, and the internet. It won't survive AI.</p><p>The numbers are staggering, even if contested. Surveys show anywhere from 30% to 89% of college students using ChatGPT for homework, with most estimates hovering around 40% for regular use. Regardless of the exact figure, one educator's observation rings true: this is education's \"Lance Armstrong moment.\" When enough players cheat with high upside and low consequences, others feel forced to cheat to compete. It becomes, in Armstrong's words, \"impossible to win without doping.\"</p><p>But here's the deeper crisis: if knowledge is now free and instant, what exactly are universities selling? Not information&#8212;ChatGPT provides that. Not skills&#8212;YouTube tutorials teach those. Not even critical thinking&#8212;AI can simulate that too. They're selling something increasingly abstract: the <em>idea</em> of credibility in a world where credentials can be faked with a prompt.</p><p>The job market reflects this confusion. Nearly half of companies say they plan to eliminate bachelor's degree requirements, yet paradoxically, 59% of employers say degrees matter MORE than five years ago. We're watching a system in violent transition, unsure whether to double down on traditional credentials or abandon them entirely.</p><p>This isn't just about cheating or job requirements. It's about the collapse of an entire social technology. Degrees served as universal signals&#8212;imperfect but shared fictions that enabled coordination. An MBA from Wharton meant something specific. Ten years at McKinsey conveyed particular competencies. Now these signals are noise. You can fake the knowledge, simulate the skills, even mimic the writing style. What can't be faked?</p><p>The answer emerging from forward-thinking educators and employers: proof of work that demonstrates judgment, not just knowledge. Ship code that handles real users. Write something that changes how people think. Navigate complex projects across domains. Build trust networks over years. Create culture rather than consume it. Make high-stakes decisions where failure has real consequences. Yes there&#8217;s an on-ramp here, but the ideas are there to change how we show value and that&#8217;s important.</p><p>These outputs resist AI assistance not because they're technically difficult but because they require ownership, liability, and the human act of meaning-making. When an AI-written essay fails to persuade, who takes responsibility? When generated code crashes in production, who fixes it at 3am? When a decision goes wrong, who stands before the board?</p><p>Universities that survive will transform from knowledge-delivery systems to judgment-development institutions. They'll teach not what to think but how to think when infinite information is available. They'll credential not information retention but the ability to navigate discontinuity, own outcomes, and create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "meaning from noise.</p><p>The students cheating with ChatGPT aren't lazy&#8212;they're rational actors in an irrational system (I think <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">Roy and Cluely</a> agree with me on this one thing lmao). They're using 21st-century tools to game 19th-century assessments for 11th-century credentials. The real scandal isn't that they're cheating. It's that we're still pretending the old game matters.</p><p>Monster's bankruptcy filing listed the cause as a \"challenging and uncertain macroeconomic environment.\" But that's corporate speak for a simpler truth: when knowledge becomes worthless, the infrastructure of knowledge-trading collapses too. First the job boards. Next, perhaps, the universities that feed them.</p><p>Unless they remember what they're actually for!</p><p>Good luck out there. Stay Curious. Stay human.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!_LYt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" width=\"1456\" height=\"971\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2037707,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166955816?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h2><strong>Sources</strong></h2><h3><strong>Monster/CareerBuilder Bankruptcy Sources</strong></h3><ul><li><p><a href=\"https://www.newsx.com/business/monster-and-careerbuilder-file-for-bankruptcy-begin-asset-sales-amid-market-shift-9215/\">NewsX &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2025/06/24/careerbuilder-monster-bankruptcy/\">Washington Post &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.cnn.com/2025/06/25/business/monster-careerbuilder-bankruptcy\">CNN &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.staffingindustry.com/news/global-daily-news/careerbuilder-monster-selling-businesses-filing-for-bankruptcy\">Staffing Industry Analysts &#8211; CareerBuilder, Monster Selling Businesses</a></p></li><li><p><a href=\"https://www.reuters.com/legal/litigation/careerbuilder-monster-which-once-dominated-online-job-boards-file-bankruptcy-2025-06-24/\">Reuters &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.abc27.com/news/consumer/careerbuilder-monster-com-enter-chapter-11-bankruptcy/\">ABC27 &#8211; CareerBuilder and Monster Enter Chapter 11</a></p></li><li><p><a href=\"https://www.foxbusiness.com/economy/online-job-listing-company-careerbuilding-monster-files-bankruptcy\">Fox Business &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Monster.com\">Wikipedia &#8211; Monster.com</a></p></li><li><p><a href=\"https://www.hrdive.com/news/careerbuilder-monster-files-chapter-11-bankruptcy/751498/\">HR Dive &#8211; CareerBuilder, Monster File Chapter 11</a></p></li><li><p><a href=\"https://www.northbaybusinessjournal.com/article/industrynews/careerbuilder-monster-bankruptcy-recruitment-workforce/\">North Bay Business Journal &#8211; CareerBuilder, Monster Bankruptcy</a></p></li></ul><div><hr></div><h3><strong>AI/ChatGPT in Education Sources</strong></h3><ul><li><p><a href=\"https://www.axios.com/2025/05/26/ai-chatgpt-cheating-college-teachers\">Axios &#8211; Teachers Worry About AI and Cheating</a></p></li><li><p><a href=\"https://slate.com/life/2025/05/college-student-cheating-ai-detector-chatgpt-school-education.html\">Slate &#8211; College Student Cheating and AI Detectors</a></p></li><li><p><a href=\"https://longreads.com/2025/05/21/chatgpt-ai-college-cheating/\">Longreads &#8211; ChatGPT and the Cheating Crisis</a></p></li><li><p><a href=\"https://nerdynav.com/chatgpt-cheating-statistics/\">NerdyNav &#8211; ChatGPT Cheating Statistics</a></p></li><li><p><a href=\"https://ed.stanford.edu/news/what-do-ai-chatbots-really-mean-students-and-cheating\">Stanford Graduate School of Education &#8211; AI Chatbots and Cheating</a></p></li><li><p><a href=\"https://www.edweek.org/technology/opinion-the-ai-cheating-crisis-education-needs-its-anti-doping-movement/2024/02\">Education Week &#8211; The AI Cheating Crisis</a></p></li><li><p><a href=\"https://apnews.com/article/chatgpt-cheating-ai-college-1b654b44de2d0dfa4e50bf0186137fc1\">Associated Press &#8211; AI and College Cheating</a></p></li><li><p><a href=\"https://www.sciencedirect.com/science/article/pii/S2666920X24000560\">ScienceDirect &#8211; Academic Integrity and AI</a></p></li><li><p><a href=\"https://www.technologyreview.com/2023/04/06/1071059/chatgpt-change-not-destroy-education-openai\">MIT Technology Review &#8211; ChatGPT Will Change Education</a></p></li><li><p><a href=\"https://slate.com/technology/2023/02/chat-gpt-cheating-college-ai-detection.html\">Slate (2023) &#8211; Early Warning Signs of AI Cheating</a></p></li></ul><div><hr></div><h3><strong>Employment/Hiring Trends Sources</strong></h3><ul><li><p><a href=\"https://www.naceweb.org/job-market/trends-and-predictions/hiring-projections-level-off-for-the-college-class-of-2025\">NACE &#8211; Hiring Projections for Class of 2025</a></p></li><li><p><a href=\"https://www.hiringlab.org/2024/12/10/indeed-2025-us-jobs-and-hiring-trends-report/\">Indeed Hiring Lab &#8211; 2025 U.S. Jobs and Hiring Trends</a></p></li><li><p><a href=\"https://www.survivalworld.com/economics/20-college-degrees-employers-dont-want-in-2025/\">Survival World &#8211; Degrees Employers Don&#8217;t Want in 2025</a></p></li><li><p><a href=\"https://www.highereddive.com/news/nearly-half-of-companies-plan-to-eliminate-bachelors-degree-requirements/702277/\">Higher Ed Dive &#8211; Companies Dropping Bachelor&#8217;s Requirements</a></p></li><li><p><a href=\"https://time.com/7291844/job-market-college-graduates-unemployment/\">TIME &#8211; Job Market for College Graduates</a></p></li><li><p><a href=\"https://www.lanereport.com/174929/2024/07/75-of-employers-say-removing-this-requirement-for-job-applicants-has-improved-their-company/\">Lane Report &#8211; Removing Degree Requirements Improves Hiring</a></p></li><li><p><a href=\"https://cew.georgetown.edu/cew-reports/projections2031/\">Georgetown CEW &#8211; Projections 2031 Report</a></p></li><li><p><a href=\"https://www.aplu.org/our-work/4-policy-and-advocacy/publicuvalues/employment-earnings/\">Association of Public and Land-grant Universities &#8211; Employment and Earnings</a></p></li><li><p><a href=\"https://www.bls.gov/news.release/pdf/empsit.pdf\">Bureau of Labor Statistics &#8211; Employment Situation Report (PDF)</a></p></li><li><p><a href=\"https://www.testgorilla.com/skills-based-hiring/state-of-skills-based-hiring-2024/\">TestGorilla &#8211; State of",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Skills-Based Hiring 2024</a></p></li></ul><h3><strong>Contemporary AI Failures (McDonald&#8217;s, Tesla, etc.)</strong></h3><ul><li><p><a href=\"https://www.restaurantbusinessonline.com/technology/mcdonalds-ending-its-drive-thru-ai-test\">Restaurant Business Online &#8211; McDonald&#8217;s Ending Its Drive-Thru AI Test</a></p></li><li><p><a href=\"https://apnews.com/article/mcdonalds-ai-drive-thru-ibm-bebc898363f2d550e1a0cd3c682fa234\">AP News &#8211; McDonald&#8217;s Ends AI Drive-Thru Partnership with IBM</a></p></li><li><p><a href=\"https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html\">CIO &#8211; 5 Famous Analytics and AI Disasters</a></p></li><li><p><a href=\"https://www.cnbc.com/2024/10/18/tesla-faces-nhtsa-investigation-of-full-self-driving-after-fatal-collision.html\">CNBC &#8211; Tesla Faces NHTSA Investigation After Fatal Collision</a></p></li><li><p><a href=\"https://www.tesla.com/VehicleSafetyReport\">Tesla &#8211; Vehicle Safety Report</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2024/10/18/tesla-full-self-driving-nhtsa-fsd/\">Washington Post &#8211; NHTSA Investigates Tesla&#8217;s Full-Self Driving System</a></p></li><li><p><a href=\"https://abcnews.go.com/Business/tesla-driving-crash-reports-prompt-nhtsa-investigation/story?id=114922283\">ABC News &#8211; Tesla Crash Reports Prompt NHTSA Investigation</a></p></li><li><p><a href=\"https://www.thetradenews.com/human-judgement-still-king-in-a-world-of-algorithmic-trades/\">The TRADE &#8211; Human Judgment Still King in Algorithmic Trades</a></p></li><li><p><a href=\"https://oatmealhealth.com/why-has-ai-failed-so-far-in-healthcare-despite-billions-of-investment/\">Oatmeal Health &#8211; Why AI Has Failed in Healthcare (So Far)</a></p></li></ul><div><hr></div><h3><strong>Knowledge Hyperinflation / Knowledge Collapse</strong></h3><ul><li><p><a href=\"https://arxiv.org/abs/2404.03502\">arXiv &#8211; Collapse of Knowledge</a></p></li><li><p><a href=\"https://www.emergentmind.com/papers/2404.03502\">Emergent Mind &#8211; Summary of arXiv 2404.03502</a></p></li><li><p><a href=\"https://www.clio.com/blog/tools-for-lawyers/\">Clio &#8211; Best Tools for Lawyers</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/issues/artificial-intelligence/ai-jobs-barometer.html\">PwC &#8211; AI Jobs Barometer (Global)</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html\">PwC &#8211; Productivity Growth and AI</a></p></li><li><p><a href=\"https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-jobs-barometer.html\">PwC US &#8211; AI Jobs Barometer</a></p></li></ul><div><hr></div><h3><strong>MIT / Economics Research on AI</strong></h3><ul><li><p><a href=\"https://economics.mit.edu/news/daron-acemoglu-what-do-we-know-about-economics-ai\">MIT Economics &#8211; Daron Acemoglu on AI and Labor</a></p></li><li><p><a href=\"https://www.imf.org/en/Publications/fandd/issues/2023/12/Rebalancing-AI-Acemoglu-Johnson\">IMF &#8211; Rebalancing AI by Acemoglu and Johnson</a></p></li><li><p><a href=\"https://ssir.org/articles/entry/ai-impact-on-jobs-and-work\">Stanford Social Innovation Review &#8211; AI Impact on Jobs and Work</a></p></li></ul><div><hr></div><h3><strong>GM / Roger Smith Sources</strong></h3><ul><li><p><a href=\"https://en.wikipedia.org/wiki/Roger_Smith_(executive)\">Wikipedia &#8211; Roger Smith (Executive)</a></p></li><li><p><a href=\"https://www.imdb.com/name/nm0809792/bio/\">IMDb &#8211; Roger Smith Bio</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Detroit/Hamtramck_Assembly\">Wikipedia &#8211; Detroit/Hamtramck Assembly</a></p></li><li><p><a href=\"https://www.leanblog.org/2016/06/gms-ceo-roger-smith-thought-toyota-had-magic-but-this-was-the-secret/\">Lean Blog &#8211; What GM Misunderstood About Toyota</a></p></li><li><p><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Scribd &#8211; </a><em><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Comeback: The Fall and Rise of the American Automobile Industry</a></em></p></li><li><p><a href=\"https://www.wrike.com/blog/key-to-perfect-automation-is-imperfect-people/\">Wrike &#8211; The Key to Perfect Automation is Imperfect People</a></p></li><li><p><a href=\"https://www.washingtonpost.com/archive/business/1990/08/01/roger-smith-gm-driven-to-regain-market-share/530dfe27-35e9-4d0e-a7f8-3a0f659d7a00/\">Washington Post (1990) &#8211; Roger Smith at GM</a></p></li><li><p><a href=\"https://archive.seattletimes.com/archive/19900730/1085108/roger-smith-leaves-his-mark-at-gm\">Seattle Times Archive &#8211; Roger Smith Leaves His Mark</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/NUMMI\">Wikipedia &#8211; NUMMI</a></p></li><li><p><a href=\"https://www.popularmechanics.com/cars/a5514/4350856/\">Popular Mechanics &#8211; A Look Back at GM and Toyota</a></p></li></ul><div><hr></div><h3><strong>Skills Half-Life / Professional Identity</strong></h3><ul><li><p><a href=\"https://www.weforum.org/stories/2024/01/this-is-the-one-skill-everybody-needs-in-the-age-of-ai/\">World Economic Forum &#8211; One Skill Everyone Needs in the Age of AI</a></p></li><li><p><a href=\"https://www.cio.com/article/219940/thriving-in-a-world-of-knowledge-half-life.html\">CIO &#8211; Thriving in a World of Shrinking Knowledge Half-Life</a></p></li><li><p><a href=\"https://www.linkedin.com/pulse/shrinking-half-life-skills-peter-smulovics\">LinkedIn &#8211; Shrinking Half-Life of Skills</a></p></li><li><p><a href=\"https://www.ibm.com/blogs/ibm-training/skills-transformation-2021-workplace/\">IBM &#8211; Skills Transformation in the Workplace</a></p></li><li><p><a href=\"https://www.verywellmind.com/what-is-an-identity-crisis-2795948\">Verywell Mind &#8211; What Is an Identity Crisis?</a></p></li><li><p><a href=\"https://seo.ai/blog/ai-replacing-jobs-statistics\">SEO.ai &#8211; AI Job Replacement Statistics</a></p></li></ul><div><hr></div><h3><strong>Financial Markets / Circuit Breakers</strong></h3><ul><li><p><a href=\"https://www.investopedia.com/articles/markets/012716/four-big-risks-algorithmic-highfrequency-trading.asp\">Investopedia &#8211; Four Big Risks in Algorithmic Trading</a></p></li><li><p><a href=\"https://www.investopedia.com/terms/c/circuitbreaker.asp\">Investopedia &#8211; Circuit Breaker Definition</a></p></li></ul><div><hr></div><h3><strong>Legal / Medical AI Adoption</strong></h3><ul><li><p><a href=\"https://www.clio.com/blog/will-ai-replace-paralegals/\">Clio &#8211; Will AI Replace Paralegals?</a></p></li><li><p><a href=\"https://apnews.com/article/ai-algorithms-chatgpt-doctors-radiologists-3bc95db51a41469c390b0f1f48c7dd4e\">AP News &#8211; ChatGPT, Doctors, and AI Algorithms</a></p></li></ul><div><hr></div><h3><strong>Amazon AI Recruiting Failure</strong></h3><ul><li><p><a href=\"https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women\">Euronews &#8211; Amazon Scraps Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.cut-the-saas.com/ai/case-study-how-amazons-ai-recruiting-tool-learnt-gender-bias\">Cut the SaaS &#8211; Case Study on Amazon&#8217;s Gender Bias in AI</a></p></li><li><p><a href=\"https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/\">Reuters &#8211; Amazon&#8217;s Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html\">CMU &#8211; Amazon Scraps Biased AI Hiring Engine</a></p></li><li><p><a href=\"https://www.aboutamazon.com/news/workplace/how-amazon-leverages-ai-and-ml-to-enhance-the-hiring-experience-for-candidates\">About Amazon &#8211; Enhancing Hiring with AI and ML</a></p></li><li><p><a href=\"https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/\">Reuters &#8211; Tesla FSD Investigation (duplicate to Tesla)</a></p></li></ul><div><hr></div><h3><strong>AI Scaling and AGI Research</strong></h3><ul><li><p><a href=\"https://www.geeky-gadgets.com/infinite-memory-ai-models/\">Geeky Gadgets &#8211; Infinite Memory AI Models</a></p></li><li><p><a href=\"https://openai.com/index/gpt-4-1/\">OpenAI &#8211; GPT-4.1 Overview</a></p></li><li><p><a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-gpt-41-models-with-improved-coding-long-context-2025-04-14/\">Reuters &#8211; OpenAI Launches GPT-4.1</a></p></li><li><p><a href=\"https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/\">Google Developers Blog &#8211; Gemini API Updates</a></p></li><li><p><a href=\"https://gemini.google/overview/long-context/?hl=en\">Gemini &#8211; Long Context Capabilities</a></p></li><li><p><a href=\"https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it\">Google Cloud Blog &#8211; Gemini Pro and the Needle-in-the-Haystack Test</a></p></li><li><p><a href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\">Google Blog &#8211; Gemini AI Update (Dec 2024)</a></p></li><li><p><a href=\"https://web.swipeinsight.app/posts/claude-unveils-projects-for-ai-workspaces-with-500-page-memory-7902\">Swipe Insight &#8211; Claude&#8217;s 500-Page Memory</a></p></li><li><p><a href=\"https://www.anthropic.com/news/claude-3-family\">Anthropic &#8211; Claude 3 Model Family</a></p></li><li><p><a href=\"https://www.anthropic.com/news/activating-asl3-protections\">Anthropic &#8211; Activating ASL3 Protections</a></p></li><li><p><a href=\"https://fortune.com/2025/05/22/anthropic-new-models-ai-openai-google/\">Fortune &#8211; Anthropic&#8217;s New AI Models</a></p></li><li><p><a href=\"https://arxiv.org/html/2410.03156v1\">arXiv &#8211; Scaling Laws (2410.03156)</a></p></li><li><p><a href=\"https://openreview.net/forum?id=TvGPP8i18S\">OpenReview &#8211; Scaling Trends in AGI</a></p></li></ul><div><hr></div><h3><strong>AGI Timeline Predictions</strong></h3><ul><li><p><a href=\"https://venturebeat.com/ai/openai-begins-2025-with-massive-hype-for-agi-superintelligence/\">VentureBeat &#8211; 2025 AGI and Superintelligence Hype</a></p></li><li><p><a href=\"https://time.com/7205596/sam-altman-superintelligence-agi/\">TIME &#8211; Sam Altman on Superintelligence</a></p></li><li><p><a href=\"https://felloai.com/2024/11/dario-amodei-ceo-of-anthropic-artificial-general-intelligence-is-coming-in-2027/\">Fello AI &#8211; Dario Amodei Predicts AGI by 2027</a></p></li><li><p><a href=\"https://lexfridman.com/dario-amodei-transcript/\">Lex Fridman &#8211; Dario Amodei Transcript</a></p></li><li><p><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">EDRM",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "&#8211; Amodei&#8217;s Essay on </a><em><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">Machines of Loving Grace</a></em></p></li><li><p><a href=\"https://www.cnbc.com/2025/03/17/human-level-ai-will-be-here-in-5-to-10-years-deepmind-ceo-says.html\">CNBC &#8211; DeepMind CEO Predicts Human-Level AI</a></p></li><li><p><a href=\"https://www.startuphub.ai/ai-news/artificial-intelligence/2025/agis-coming-in-5-to-10-years-says-deepmind-ceo-demis-hassabis/\">StartupHub.ai &#8211; AGI in 5&#8211;10 Years, Says Demis Hassabis</a></p></li></ul>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "Ready for ChatGPT-5: Grab a Complete 139 Page Prompting Guide That's a Complete Operating System for Life and Work",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "I told myself I wouldn't do this again. After my 66-page prompt opus in April, I swore I'd keep things shorter. More digestible. Reader-friendly. [Narrator voice: He did not keep things shorter becaus...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "<p><em><strong>I told myself I wouldn't do this again.</strong> After my <a href=\"https://natesnewsletter.substack.com/p/my-prompt-stack-for-work-16-prompts?r=1z4sm5\">66-page prompt opus in April</a>, I swore I'd keep things shorter. More digestible. Reader-friendly. </em></p><p><em>[Narrator voice: He did not keep things shorter because there&#8217;s too much good stuff.]</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!aTYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" width=\"498\" height=\"498\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:498,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2081498,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166864451?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>This collection is ~145 pages long. It contains 39 essential prompts in a detailed Google Doc, many with multiple variants, plus 6 pages of context that explains why prompting has fundamentally changed in 2025.</strong> And yes, there's a table of contents in that Google Doc so you can actually navigate this beast and find exactly what you need.</em></p><p><em>Because my longing to suffer, why did I write another novel-length prompt guide? Because something clicked for me recently. <strong>I realized my prompts and how I talk about them needed to evolve to keep pace with frontier model capabilities. And with ChatGPT-5 on the horizon (rumored for July), I wanted to bring the prompt stack up to snuff with what today&#8217;s models can do.</strong> </em></p><p><em>These aren't random prompts I think might be useful. <strong>These are the 39 prompts I have labored over, technically, repeatedly, as many different ways as I can think of to make them excellent.</strong></em></p><p><em>How? </em></p><ul><li><p><em>By checking them vs. actual published prompting guides (I list 18 of them below) to ensure they adhere to best practices (lots of these have been published since April)</em></p></li><li><p><em>By developing multiple variants to give you different options depending on your level of effort</em></p></li><li><p><em>By choosing a range of prompts that cover the full spectrum of decision-making I see actually cropping up in work (and life)</em></p></li></ul><p><em>That last is key: I find a lot of my earlier prompt work has been a bit constrained by earlier model intelligence levels. I&#8217;ve felt like I needed to constrain to particular job families in the past because earlier models leaned into fairly defined work and task completion assignments. And that has value!</em></p><p><em><strong>But we can do more cool stuff with the newer models. There&#8217;s a chance with these newer models to ask much more ambiguous questions.</strong></em></p><p><em><strong>Like: </strong>When I need to make a decision that will affect the next five years. When I'm staring at feedback that stings and need to figure out what's actually useful in it. When a project is falling apart and I need structured thinking,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "not panic. When I'm trying to learn something new and don't want to waste months on bad approaches.</em></p><p><em>I've been living with these frontier models&#8212;Claude 4, o3 Pro, Gemini 2.5 Pro&#8212;like they're colleagues. Maybe closer than colleagues. <strong>We've developed a working relationship where I know exactly how to activate their best thinking, and they know (through my prompts) exactly what kind of thought partnership I need.</strong></em></p><p><em>Each prompt in this collection does something specific:</em></p><ul><li><p><em><strong>Turn vague anxiety into clear action</strong> (Future Regret Minimizer, Change Readiness Evaluator)</em></p></li><li><p><em><strong>Extract signal from noise</strong> (Dynamic Qualitative Insight Explorer, Strategic Feedback Interpreter)</em></p></li><li><p><em><strong>Force hard choices</strong> (Comprehensive Tradeoff Analyzer, Meeting Killer)</em></p></li><li><p><em><strong>Design better systems</strong> (Database Schema Designer, Automation Opportunity Scanner)</em></p></li><li><p><em><strong>Navigate complex human dynamics</strong> (Stakeholder Navigation Guide, Multi-Perspective Simulator)</em></p></li></ul><p><em>But here's what matters more: <strong>these prompts are really complete thinking systems, not just questions.</strong> They include context setup, phase-by-phase workflows, specific output formats, and iteration loops. They assume the AI is genuinely intelligent and just needs clear structure to be helpful. And that&#8217;s definitely a marker of the time we&#8217;re in&#8212;the blurry, <a href=\"https://www.google.com/search?q=gentle+singularity&amp;oq=gentle+singularity&amp;sourceid=chrome&amp;ie=UTF-8\">gentle singularity</a>.</em></p><p><em><strong>The Google Doc is designed to be scannable and searchable.</strong> Eight major sections. Clear numbering. Purpose statements for each prompt so you know when to use it. You can bookmark it and come back whenever you face that type of challenge. Think of it as your emergency toolkit for complexity.</em></p><h4><em>Here&#8217;s what it looks like:</em></h4><div class=\"native-video-embed\" data-component-name=\"VideoPlaceholder\" data-attrs=\"{&quot;mediaUploadId&quot;:&quot;f31418d2-fca0-4769-8147-275332f2cf0f&quot;,&quot;duration&quot;:null}\"></div><p><em>I've also included something new: detailed notes on how prompting itself has evolved. How specificity works to drive model outputs. How to use the new massive context windows strategically. Why breaking complex tasks into phases isn't about helping the AI anymore&#8212;it's about helping us.</em></p><p><em><strong>These prompts work with any frontier model</strong>&#8212;Claude 4, o3 Pro, o3, Gemini 2.5 Pro, Grok 3. And I&#8217;ve designed them to be built with the grain of the emerging intelligence patterns we&#8217;re seeing, <strong>so they should set you up for GPT-5 later in the summer</strong>. <strong>Yes I said it.</strong> </em></p><p><em><strong>How??</strong> I&#8217;ve cross-referenced these prompts across the different model makers&#8217; prompting guides to make sure they can be used as-is as much as possible. The principles hold because they're based on the commonalities in how these models actually process information, not on platform-specific tricks.</em></p><p><em>Fair warning: this isn't light reading. <strong>Each prompt is densely packed with structure and logic.</strong> But that's the point. When you need to think clearly about something that matters, you don't want a vague suggestion. You want a systematic process that reliably produces insight. And the point is that now you can just take these prompts and be off to the races.</em></p><p><em>If you've been following my prompt journey for the last few months, this is where it's led. If you're new, this is everything I've learned compressed into immediately usable tools. Either way, <strong>these 39 prompts will change how you work with AI&#8212;from occasional assistant to genuine thinking partner.</strong></em></p><p><em>Time to dig in. The prompts are waiting.</em></p><p><em>PS. For those wanting both the full podcast from the video and a nice voice reading the whole post&#8212;I got you covered. Separate audio",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "for the post just below!</em></p><p></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "2 Posts in One: Meta's AI Strategy Looks Desperate + Your Invite to Nate's New AI Discord Community",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "I got lots of feedback on my post from Sunday about a community for paid subscribers.TLDR y&#8217;all want one, and you want it to be on Discord. So I made one! We&#8217;ve got a couple of dozen brave...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "<p>I got lots of feedback on my <a href=\"https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates?r=1z4sm5\">post from Sunday</a> about a community for paid subscribers.</p><p><strong>TLDR </strong>y&#8217;all want one, and you want it to be on Discord. </p><p>So I made one! </p><p>We&#8217;ve got a couple of dozen brave alpha testers already in there and I&#8217;ll throw the link in below here. Hop on in! We have channels for Substack discussion, AI news, AI questions, and I&#8217;m gonna get an AI jobs channel going here soon as well.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!UuK7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" width=\"480\" height=\"360\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:900362,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166843413?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Also have had a fair bit of interest from founding members on my weekly CEO-view posts. A bit more about them:</p><blockquote><p><em>I'm building <strong>short, sharp weekly intelligence briefs for leaders making real decisions with real money.</strong> <br><br>Think 3-4 pages max, packed with insights that usually cost too much from consultants&#8212;AI procurement pitfalls, hidden implementation costs blindsiding CFOs, liability issues, which investment patterns actually correlate with ROI. <br><br>The goal is clear, frank discussion of the issue, specific actionable paths forward, and hard-headed analysis you can take to the bank.</em></p></blockquote><p>You can change <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">your Substack plan here</a> if you&#8217;re like to get the first one (coming Sunday). Substack will prorate you for previous months etc. so if you&#8217;re been subbing for awhile you won&#8217;t be in at full price. </p><blockquote><p><em>Hint: the first one is on a gnarly compliance issue with board liability implications that&#8217;s facing just about every AI startup right now and that kicks in at the beginning of August. Fun times!</em></p></blockquote><p>Besides the link underneath, I&#8217;m throwing a juicy tidbit for fun on whatever the heck Meta is doing right now with those wild acquisition offers (Zuck my DMs are open lmao). Yes this post is pretty blunt about what Zuck is doing so buckle up!</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all the good stuff, and now a community lol</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "The Claude Code Complete Guide: Learn Vibe-Coding & Agentic AI",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "I got Claude Code backwards when I first read about it. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "<p><em><strong>I got Claude Code backwards when I first read about it</strong>. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not what stood out to me after trying it and digging in.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" width=\"1456\" height=\"790\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:233937,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166772125?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>It&#8217;s not just for code guys lol</strong></em></p><p><em>After spending time with Claude Code, I'm convinced <strong>what we have here is effectively a general purpose AI agent</strong> hiding under the guise of just being a coding agent. It's not just a coding agent. <strong>Claude Code is capable of the full spectrum of intelligence. It just happens to hide in the terminal</strong>, and that makes it seem scary to people who don't use terminals to code. And let's be honest, that's most of us.</em></p><p><em>What I find really fascinating is that when I started to finally use Claude Code, <strong>it made the decision to upgrade to the Max tier so much easier. </strong>I had enough experience with the intelligence that Claude Code was bringing that it felt intuitive. And here's what really blew my mind: <strong>with Claude Code, you don't have a traditional development environment</strong>. Like, I thought that would be a mistake at first. Who wants to code without the IDE? This is not a video game.</em></p><p><em>But Claude will edit files, create files, but won't necessarily show you all of it the way it does in a typical development environment. You might think, <strong>what a terrible design choice</strong>. And you and I would both be wrong about that. <strong>It turns out that abstracting you above code level helps you focus on the strategy and the intent of the project.</strong></em></p><p><em>And Claude Code has the muscle to actually operate at both levels: strategic and execution. <strong>I strongly suspect this power comes from the fact that there aren't the same token constraints you'd have if you installed Claude in another tool like Cursor</strong>. Because Anthropic can control the whole experience, they can make Claude Code work exactly the way they want. It feels a lot like an internal development tool that got out into the wild, which is exactly what it is.</em></p><p><em>Look, I'm not a fantastically experienced senior engineer. My background is different. I am a hacky scrappy founder, producty kind of person. <strong>I am writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "this guide for everyone, not just engineers.</strong> I know enough about coding and product / project management to see that Claude Code is so much more than a coding tool, and we misunderstand it when we think about it just as a coding tool.</em></p><p><em>When I was working on my personal website&#8212;which, for those of you who have been there, I freely agree is terrible and awful lmao&#8212;Claude Code legitimately transformed the experience. Just much less pain. It was so much easier to get Claude Code to work on this project than in any other environment I'd played with. I tried Windsurf, played with it in Cursor a little bit, played with it in o3 (which was the worst example). But Claude Code was different. <strong>It answered my questions intelligently, laid out a plan I could understand, and when it decided to build, it built largely correctly from the start.</strong></em></p><p><em><strong>This is the first time using Claude Code that I've been able to actually get a polished, professional, not mid-looking AI output.</strong> And I did it with an unusual workflow&#8212;80% Claude Code, then a mixture of o3 for color research and Cluely for UI feedback (I&#8217;ll dig into it below). The magic was treating Claude Code not as a coding tool, but as a general purpose agent that happens to live in your terminal.</em></p><p><em>If you're not an engineer, don't let the terminal scare you. Think about it as effectively a chatbot that can talk to the files on your computer. That's really it.</em></p><p><em>And that's exactly what this guide is about. Over the next few thousand words, I'm going to show you everything I've learned about Claude Code&#8212;from that initial $100/month subscription decision that made me grimace, to building complete applications in conversation, to discovering workflows that multiply productivity by 10x or more.</em></p><p><em>You'll learn the 5-minute setup that changes everything, learn a bit more about the art of vibe coding (<a href=\"https://natesnewsletter.substack.com/p/the-vibe-coding-bible-how-to-build\">complete guide here</a>), understand why Claude Code is fundamentally different from GitHub Copilot or Cursor, and see real examples of how teams are achieving 5x productivity gains. </em></p><p><em>I'll also share my interesting three-tool orchestra approach that finally broke through AI's \"mid\" design problem, explain some of the dark side (Claude Code does add up in cost if you use it a lot), and show you some advanced patterns that can help turn Claude Code into your universal terminal for all knowledge work.</em></p><p><em>Why? All this matters because we're at an inflection point. As Kent Beck said after 52 years of coding, <a href=\"https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent\">90% of traditional programming skills are becoming commoditized</a> <strong>while the remaining 10% becomes worth 1000x more</strong>. The developers and teams who understand this shift&#8212;who learn to orchestrate AI rather than just code alongside it&#8212;will thrive in this new landscape. And Claude Code gives us a chance to demonstrate that remaining 10% of skills with a relatively strong junior coding partner.</em></p><p><em>Whether you're a seasoned developer looking to level up your AI pair programming, a product manager wanting to prototype without engineering bottlenecks,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "or someone who's never coded but has ideas to build, this guide will be a helpful reference that shapes how you approach using AI agents for development. At the end of the day, Claude Code isn't just another coding assistant. It's the beginning of <strong>a new era where the terminal becomes a conversational interface for turning ideas into reality</strong>.</em></p><p><em>Ready to see what's actually possible? Let's dive in&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "The Anthropic Ruling: A Roadmap for AI's Copyright Future",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "Hot off the presses! Had to get this one out today because the ruling is such a big deal.Hope you enjoy, and back to our regular programming soon&#8230;Subscribers get all these pieces!I've been follo...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "<p><em>Hot off the presses! Had to get this one out today because the ruling is such a big deal.</em></p><p><em>Hope you enjoy, and back to our regular programming soon&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>I've been following AI copyright cases closely, and Judge William Alsup just handed down what I believe will be one of THE landmark AI decisions we'll see this decade. The federal court's split ruling in <em>Bartz v. Anthropic</em> does something remarkable: it validates AI training as fair use while simultaneously condemning the piracy that often enables it. This isn't just a win or loss for Anthropic&#8212;it's a blueprint for how courts will likely approach the dozens of AI copyright cases working their way through the system.</p><h2>The Solomon's Choice of AI Copyright</h2><p>I find Alsup's decision fascinating because it splits the baby with surgical precision. Yes, training Claude on millions of books constitutes fair use. No, downloading those same books from pirate sites doesn't get a free pass. The distinction matters because it fundamentally reshapes how AI companies must think about data acquisition.</p><p>The judge's reasoning on fair use particularly struck me. He describes AI training as \"quintessentially transformative,\" comparing it to how human writers learn from reading. \"Everyone reads texts, too, then writes new texts,\" Alsup writes. \"To make anyone pay specifically for the use of a book each time they read it, each time they recall it from memory, each time they later draw upon it when writing new things in new ways would be unthinkable.\"</p><p>This analogy&#8212;AI as reader learning to write&#8212;provides the conceptual foundation that I think AI companies have been desperately seeking. It's not about copying; it's about learning patterns, understanding language, and creating something fundamentally new.</p><h2>The Million-Dollar Pivot</h2><p>Here's where I think Anthropic's story gets really interesting. After building their initial models on pirated content from Books3, Library Genesis, and other dubious sources, the company made a dramatic shift in 2024. They hired Tom Turvey, the former head of Google's book-scanning project, with a mandate to obtain \"all the books in the world\" through legitimate means.</p><p>Anthropic then spent millions of dollars purchasing physical books&#8212;many second-hand&#8212;which they proceeded to slice from their bindings and scan into digital format. The physical books were destroyed in the process, but the digital copies were ruled as legitimate fair use. This expensive pivot from piracy to purchase reveals something I've been saying for a while: AI companies can afford to do this right. They're choosing not to.</p><p>The court explicitly noted this financial capability, observing that Anthropic's later purchases of books they'd previously pirated \"will not absolve it of liability for the theft but it may affect the extent of statutory damages.\" Translation: we see you had the money all along.</p><h2>What This Means for Authors</h2><p>I think this ruling offers a glimmer of hope for authors who've watched AI companies feast on their work without compensation. While the fair use ruling means authors",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "can't stop AI training entirely, the court's condemnation of piracy and validation of legitimate book purchases creates real market incentives.</p><p>Consider what Anthropic's spending reveals: they paid millions for books, often buying used copies at market rates. This money flows back into the book ecosystem&#8212;to retailers, distributors, and ultimately supporting the market for authors' works. If every AI company followed this model instead of scraping pirate sites, we'd see a substantial new revenue stream for the publishing industry.</p><p>Moreover, the ruling's emphasis on transformation rather than reproduction protects authors' core market. The court stressed that it matters whether AI systems \"directly compete with the originals.\" Since Claude doesn't spit out verbatim passages from novels, the technology complements rather than replaces human authorship.</p><p>I see this as establishing a sustainable equilibrium: AI companies must pay for access to training materials, supporting the creative economy, while authors benefit from AI tools that help readers discover and engage with human-written works.</p><h2>The Domino Effect</h2><p>This ruling's impact extends far beyond Anthropic's legal troubles. I'm watching several major cases that will likely cite this precedent:</p><p><strong>The OpenAI Cases</strong>: Multiple lawsuits against OpenAI, including from the Authors Guild and various publishers, hinge on similar fair use arguments. Alsup's framework&#8212;distinguishing between training use and acquisition methods&#8212;gives OpenAI a potential path to victory, assuming they can demonstrate legitimate data sourcing.</p><p><strong>Kadrey v. Meta</strong>: The lawsuit against Meta for training LLaMA on Books3 (the same dataset Anthropic used) now faces an interesting precedent. Meta might win on fair use for training but could still face liability if they retained pirated materials in a permanent library.</p><p><strong>The Stability AI Litigation</strong>: Visual AI companies face additional complexities, but I think Alsup's \"transformative use\" reasoning could extend to image generation models that learn artistic styles without reproducing specific works.</p><h2>The New Compliance Playbook</h2><p>From my reading of Alsup's ruling, he's effectively created a compliance roadmap for AI companies:</p><ol><li><p><strong>Training on copyrighted works? Probably fine</strong>, as long as your model doesn't reproduce those works verbatim.</p></li><li><p><strong>Building a permanent library of pirated content? Definitely not fine</strong>, even if you only use it for training.</p></li><li><p><strong>Want to avoid liability? Buy the books</strong>. Or license them. Or use legitimately free sources. But stop pretending piracy is a necessary evil.</p></li><li><p><strong>Already have pirated content? Delete it after training</strong>. The court's distinction between temporary training copies and permanent library storage offers a potential safe harbor.</p></li></ol><h2>The Billion-Dollar Question</h2><p>With damages still to be determined, I calculate Anthropic faces potential liability in the billions. Statutory damages for willful infringement can reach $150,000 per work, and we're talking about millions of books. This creates a powerful deterrent effect: train responsibly or face existential financial risk.</p><p>The message I'm taking from this ruling is clear: the transformative nature of your technology doesn't give you a free pass to transform other people's property into your training data through illegal means. Fair use protects the learning, not the theft.</p><p>As more courts adopt Alsup's framework, I expect we'll see a fundamental shift in how AI companies approach data acquisition. The days of \"download first, ask permission never\" are ending. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "future belongs to companies willing to do what Anthropic eventually did: open their wallets and do things the right way.</p><p>Even if it means destroying a few million books in the process&#8212;at least authors got paid for them.</p><h2>The Bigger Picture</h2><p>I should note that while Alsup's ruling gives us crucial clarity, we're still in the early innings of this game. This is just one federal district court's take, and I've learned to be cautious about declaring victory based on a single decision. We've got dozens of similar cases working through different courts, each with their own judges who might see things differently.</p><p>I'm particularly aware that the Northern District of California doesn't speak for the entire country. We're already seeing circuit splits on related AI issues&#8212;the Ninth Circuit requires \"actual knowledge\" for contributory infringement while the Second Circuit only needs \"reason to know.\" That's a big difference when we're talking about platforms hosting AI tools.</p><p>What's more, this ruling really only addresses text-based AI training. I'm left wondering how courts will handle visual AI systems, code generation, or the myriad other AI applications we're seeing. Fair use is notoriously fact-specific, and what works for Claude might not work for DALL-E or Copilot.</p><p>I expect we'll see Anthropic appeal to the Ninth Circuit, which could modify or even reverse parts of Alsup's reasoning. And honestly? I think we'll eventually need either the Supreme Court to step in and create nationwide clarity, or Congress to pass actual AI legislation. Neither seems likely in the near term, which means we're in for years of case-by-case battles as the law slowly catches up to the technology.</p><p>Still, Alsup's decision represents real progress. It's the first federal court to tackle these questions head-on, and that matters&#8212;even if it's just the opening chapter of a much longer story.<br></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ftVE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1363491,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166769339?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "From ChatGPT to Cluely: Riding the $120M Proactive AI Wave",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "Cheating, cheating, cheating. That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "<p><em><strong>Cheating, cheating, cheating.</strong> That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do interviews anymore (they were asking that before too lol).</em></p><p><em>Look, I get it. The internet is the greatest narrative simplification machine ever built.</em></p><p><em>Here's the thing: I was playing around with Cluely before it got all hype-y. Not to cheat&#8212;I'm way past needing help with homework (though my kids might disagree). I picked it up because I was curious about this whole \"proactive AI\" thing everyone keeps talking about. What I found wasn't some elaborate cheating scheme&#8212;despite the aggressive marketing. It was something way more interesting: <strong>a glimpse of what happens when AI stops waiting for permission to help.</strong></em></p><p><em>And honestly? The whole cheating discourse is missing the point so badly it's almost painful to watch. <strong>Everyone is playing into Cluely&#8217;s hands and they love to see it guys.</strong> </em></p><p><em><strong>Forgive a little humor, but Cluely is cheating at cheating.</strong> They hype the cheating brand to draw in their target IC customers (Gen Z and Gen Alpha). But while everyone's arguing about whether students should use AI for essays (and earning them clicks), Cluely <strong>quietly built something that changes how we think about human-AI collaboration entirely.</strong> They're not just making another chatbot&#8212;<strong>they're showing us what Level 2 proactive AI actually looks like in the wild.</strong></em></p><p><em>I almost didn't write this piece. It felt too... nuanced? Too complex for the current moment where most of the news cycle is dominated by \"AI is cheating and we&#8217;re all doomed\" or (to a lesser extent) &#8220;a16z is out to lunch for funding this.&#8221; But when I watch a 21-year-old dropout secure venture funding based on a tool that most people completely misunderstand, I realize I at least need to try to articulate what&#8217;s going on under the marketing hype and doom-and-gloom.</em></p><p><em>Here's what I want to dig into: </em></p><ol><li><p><em><strong>Why</strong> <strong>Cluely's $120M valuation makes perfect sense from a startup strategy perspective</strong> (even if their AI is pretty mid)</em></p></li><li><p><em><strong>How Cluely helps us think about agentic AI</strong> <strong>and offers an early picture into where job skills</strong> <strong>are going next</strong> (especially with those juicy B2B contracts they&#8217;ve landed)</em></p></li><li><p><em><strong>How</strong> <strong>their UX innovation reveals where agentic AI is actually heading</strong>, and why the cognitive fitness implications are way more profound than anyone's talking about</em></p></li><li><p><em><strong>Plus</strong>, <strong>if you're building anything in the AI space, understanding what Cluely got right about distribution and timing might be the most important lesson of 2025.</strong></em></p></li><li><p><em><strong>And as a bonus, what we can learn from Cluely&#8217;s leaked system prompt. </strong>So we&#8217;ll get some good prompt analysis in at the end for you prompting geeks!</em></p></li></ol><p><em>The real story isn't about cheating. It's about what happens when we stop asking \"How can AI help me find the answers?\" and start asking and <strong>&#8220;How can I use proactive AI to extend my thinking?&#8221;</strong> That shift&#8212;from reactive to proactive intelligence&#8212;is worth way more than $15 million. And I wrote 31 pages to show you why",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "lol</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "Software 3.0 vs AI Agentic Mesh: Why McKinsey Got It Wrong",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "<p></p><p><em>This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out in real time.</em></p><p><em>Fighter on one side: <strong>Andrej Karpathy&#8217;s &#8220;Software 3.0&#8221;</strong> (natural language as the programming interface, grounded in actual experience building Autopilot)</em></p><p><em>On the other: <strong>McKinsey&#8217;s &#8220;AI Agentic Mesh&#8221;</strong> (distributed autonomous agents, grounded in&#8230; PowerPoint slides lol)</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!WE-I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1994788,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em>Fun aside, this isn&#8217;t just another framework war&#8212;<strong>it&#8217;s a perfect case study in how the same technology gets interpreted completely differently depending on whether you&#8217;re building or consulting</strong>. It&#8217;s been really fun to dive into this one because the stakes are real. We&#8217;re watching billions get allocated based on these competing visions, and only one of them is going to survive contact with reality.</em></p><p><em>Here&#8217;s the thing: I normally keep the <strong>deep strategic breakdowns behind the paywall</strong>, but this battle felt too important to gate. <strong>Consider this a taste of what subscribers get every week</strong>&#8212;the kind of analysis that cuts through the noise and shows you what&#8217;s actually happening.</em></p><p><em>Because while executives are choosing sides, there&#8217;s <strong>real transformation happening in the quiet corners where developers are shipping with AI</strong>.</em></p><p><em>So let&#8217;s roll up our sleeves and dig in. You&#8217;ll get:</em></p><ul><li><p><em>The <strong>real story</strong> behind both visions</em></p></li><li><p><em>Why the <strong>communication gap between builders and executives</strong> keeps creating expensive disasters</em></p></li><li><p><em>And most importantly, <strong>the first practical framework for translating AI constraints into business strategy that actually works</strong></em></p></li></ul><p><em><strong>This is the clarity I can&#8217;t find so, I wrote it up.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h1><strong>The TLDR</strong></h1><p>Two radically different visions of AI's future are competing for executive attention, and the choice between them will determine which organizations thrive in the coming \"decade of agents.\"</p><h2>The Two Visions</h2><p><strong>Karpathy's Software 3.0</strong> represents a fundamental shift where natural language becomes the primary programming interface. Based on his experience building Tesla's Autopilot, Karpathy describes AI as \"brilliant interns with perfect recall but no judgment\"&#8212;powerful tools requiring human oversight. His vision acknowledges critical limitations like \"jagged intelligence\" (excelling at complex tasks while failing at simple ones) and \"anterograde amnesia\" (no memory between conversations). The focus is on <strong>augmentation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "through human-AI collaboration</strong>, not replacement.</p><p><strong>McKinsey's AI Agentic Mesh</strong> promises enterprise-wide networks of autonomous agents coordinating seamlessly across organizations. This consultant-crafted vision features \"composable\" systems, \"distributed intelligence,\" and \"governed autonomy\"&#8212;architectural concepts that sound impressive in boardrooms but violate fundamental technical principles.</p><h2>Why the Technical Community Rejects McKinsey's Vision</h2><p>Practitioners who actually build AI systems universally dismiss the agentic mesh. Cognition (creators of Devin) concluded that multi-agent systems \"only result in fragile systems\" because decision-making becomes too dispersed and context can't be shared effectively. Anthropic found their multi-agent systems use \"15&#215; more tokens than chats\" and struggle with coordination. The technical reality: successful AI implementations require centralized control and tight integration&#8212;the opposite of McKinsey's distributed mesh.</p><h2>The Executive Communication Crisis</h2><p>The gap between technical reality and executive understanding has created a crisis of expensive failures. <strong>Klarna's AI disaster</strong> exemplifies this pattern: the company claimed its AI handled 700 agents' worth of work and saved $40 million annually, only to later admit they'd \"gone too far\" and quietly rehired human workers due to \"lower quality\" service.</p><p>This pattern repeats across industries&#8212;IBM Watson's $62 million failure at MD Anderson, McDonald's abandoned AI drive-through, Air Canada's policy-inventing chatbot. Each failure stems from executives chasing automation fantasies instead of understanding AI's true capabilities and constraints.</p><h2>The Path Forward</h2><p>Organizations need <strong>technically grounded executive narratives</strong> that translate AI capabilities into business terms without losing nuance. Successful approaches include:</p><ul><li><p><strong>Operational analogies</strong>: Frame LLMs as \"brilliant interns\" rather than using technical jargon</p></li><li><p><strong>Financial constraints</strong>: Show real costs&#8212;processing large datasets requires breaking them into thousands of chunks, costing hundreds of thousands in compute</p></li><li><p><strong>Domain-specific examples</strong>: Demonstrate specific failure modes in the executive's industry</p></li><li><p><strong>Progressive disclosure</strong>: Let pilots reveal limitations naturally through experience</p></li></ul><h2>Why This Matters Now</h2><p>Software 3.0 isn't future speculation&#8212;it's today's reality. Developers using tools like Cursor AI report 10-100x productivity gains on specific tasks. Startups with tiny teams now compete with products that previously required massive engineering organizations. The transformation is happening at AI speed, not traditional enterprise timelines.</p><h2>The Binary Choice</h2><p>Organizations face a stark choice: embrace AI as a collaborative amplifier of human capability, or chase consultant fantasies promising autonomous replacement. The 4% of companies generating substantial AI value share common traits&#8212;they focus on augmentation, invest heavily in human-AI workflows, and measure success through value creation rather than cost reduction.</p><p><strong>The wave of Software 3.0 is breaking now.</strong> Organizations that catch it with clear eyes will build sustainable advantages. Those chasing McKinsey's distributed dreams will join the graveyard of failed AI transformations, wasting billions while competitors build real value through human-AI collaboration.</p><p>The future belongs to \"Iron Man suits\" for knowledge work&#8212;AI that amplifies human capability rather than replacing the human inside.</p><h1><strong>Software 3.0 and the Executive Delusion: Why Karpathy's Vision Matters and McKinsey's Doesn't</strong></h1><h2><strong>I. Opening: The Tale of Two Visions</strong></h2><p>This week at Y Combinator's AI Startup School, Andrej Karpathy stood before a room of builders and declared that we've entered the era of Software 3.0&#8212;where natural language becomes the primary programming interface. \"The hottest new programming language is English,\" he said, describing a world where anyone who can clearly articulate ideas can create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "software. His vision, grounded in years of replacing traditional code with neural networks at Tesla, represents a profound shift in how we build and interact with technology.</p><p>Meanwhile, in boardrooms across the Fortune 500, McKinsey consultants are selling executives on something called the \"AI Agentic Mesh\"&#8212;a grand vision of autonomous agents coordinating seamlessly across enterprises, promising to finally deliver the ROI that has eluded 78% of companies dabbling in generative AI. Their PowerPoints paint a picture of composable, distributed, vendor-agnostic systems where hundreds of AI agents collaborate like a perfectly choreographed symphony.</p><p>These two visions couldn't be more different. One comes from the trenches of building Autopilot at Tesla, where Karpathy watched neural networks progressively eat away at 300,000 lines of C++ code. The other comes from consulting frameworks designed to sound strategic in executive briefings. One acknowledges fundamental limitations&#8212;what Karpathy calls \"jagged intelligence\" and \"anterograde amnesia.\" The other handwaves away technical constraints with promises of \"governed autonomy\" and \"layered decoupling.\"</p><p>The gap between these visions isn't academic. It's measured in billions of dollars misdirected, thousands of careers disrupted, and countless opportunities missed. When Klarna's CEO boasted about replacing 700 customer service agents with AI, saving $40 million annually, the tech press celebrated. Months later, he quietly admitted they'd \"gone too far,\" delivering \"lower quality\" service and hiring humans back. The company had fallen for the same delusion McKinsey now packages as revolutionary architecture.</p><p>This pattern repeats across industries. IBM Watson consumed $62 million at MD Anderson before being abandoned. McDonald's discontinued its AI drive-through after three years of adding bacon to ice cream orders. Air Canada faced legal troubles when its chatbot invented refund policies. Each failure shares the same root cause: executives chasing consultant fantasies instead of understanding technical reality.</p><p>The tragedy is that real transformation is happening&#8212;just not the kind McKinsey sells. At companies embracing Karpathy's vision of \"partial autonomy,\" developers using tools like Cursor AI report 10-100x productivity gains for specific tasks. They're not replacing humans; they're amplifying human capability. They're not building autonomous agent meshes; they're creating tight feedback loops between human creativity and AI generation.</p><p>But this nuanced reality doesn't sell well in boardrooms. It requires admitting that AI has \"jagged intelligence\"&#8212;brilliant at complex tasks while failing at simple ones. It means accepting that large language models are, in Karpathy's memorable phrase, \"stochastic simulations of people\" with \"anterograde amnesia,\" unable to remember or learn between conversations. It demands investment in people and processes, not just technology.</p><p>The cost of this communication failure compounds daily. While technical teams know that multi-agent systems \"only result in fragile systems\" (as Cognition, creators of Devin, learned the hard way), executives allocate budgets toward McKinsey's distributed mesh dreams. While practitioners understand that AI agents use \"15&#215; more tokens than chats\" (per Anthropic's experience), leaders expect cost savings from wholesale automation. The mismatch between expectation and reality guarantees expensive failure.</p><p>We stand at an inflection point. Karpathy isn't describing some distant future&#8212;Software 3.0 is breaking through now. Natural language interfaces are transforming how we build software today. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "question isn't whether this transformation will happen, but whether organizations will navigate it successfully or crash against the rocks of consultant-crafted delusions.</p><p>This is a story about two fundamentally different ways of understanding AI's impact on work and business. One path, illuminated by builder wisdom and technical truth, leads to genuine augmentation and value creation. The other, paved with PowerPoint promises and architectural astronautics, leads to the same graveyard of failed digital transformations that litters corporate history.</p><p>As we enter what Karpathy calls \"the decade of agents,\" the stakes couldn't be higher. The organizations that thrive will be those that reject the siren song of autonomous replacement and embrace the messier, more honest reality of human-AI collaboration. They'll build with humility about limitations while harnessing genuine capabilities. Most importantly, they'll listen to builders over consultants, choosing technically grounded evolution over executive-friendly revolution.</p><p>The pages that follow will unpack these two visions in detail, explore why executives keep falling for automation fantasies, and chart a path toward narratives that bridge the gap between technical reality and business strategy. Because in the end, Software 3.0's promise isn't about replacing human intelligence&#8212;it's about amplifying it. But only if we're honest enough to see it clearly.</p><h2><strong>II. Understanding Software 3.0: What Karpathy Actually Said</strong></h2><p>To understand why Karpathy's Software 3.0 vision matters, we need to grasp both its revolutionary implications and its refreshing honesty about limitations. Unlike the consultant-speak flooding executive inboxes, Karpathy's framework emerges from hard-won experience replacing traditional code with neural networks at Tesla's Autopilot division. His presentation at Y Combinator wasn't selling a product&#8212;it was sharing a profound shift in how we create and interact with software.</p><h3><strong>The Evolution Framework</strong></h3><p>Karpathy's Software 3.0 thesis rests on understanding two previous paradigm shifts. Software 1.0 represents traditional programming&#8212;the world we've inhabited since computing began. Developers write explicit instructions in languages like Python, C++, or Java, specifying exact algorithms, control flows, and data structures. Every behavior is deliberately coded, debugged line by line, and maintained through human understanding. This is the programming most people recognize: explicit, deterministic, and fully under human control.</p><p>Software 2.0 emerged from a radical insight Karpathy articulated in 2017, based on his experience at Tesla. Instead of writing code to detect stop signs or identify lane markings, engineers began curating datasets and designing neural network architectures. The actual \"program\" became millions or billions of learned parameters&#8212;weights discovered through optimization algorithms rather than human reasoning. As Karpathy watched at Tesla, neural networks progressively consumed traditional code. Features requiring thousands of lines of C++ were replaced by learned behaviors that performed better with less explicit programming.</p><p>Software 3.0 represents the next leap: natural language becoming the primary programming interface through Large Language Models. As Karpathy explains: \"What's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0.\"</p><p>In this new paradigm, prompts replace code as the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "primary way to direct computational behavior. LLMs serve as interpreters that understand and execute natural language instructions. Context windows act as working memory. Most radically, programming becomes universally accessible&#8212;anyone who can clearly express ideas can create software.</p><h3><strong>LLMs as a New Type of Computer</strong></h3><p>Karpathy's most profound insight reframes LLMs not as tools but as fundamentally new computational systems. \"When I use ChatGPT,\" he notes, \"I feel like I'm talking to an operating system through the terminal.\" This isn't mere metaphor&#8212;he maps traditional computing concepts onto language-based processing:</p><p>The LLM functions as the CPU, the core processing unit executing instructions. The context window serves as RAM, providing short-term working memory for active computation. Prompts become programs&#8212;natural language instructions directing behavior. Tokens represent the fundamental units of data, like bytes in traditional computing.</p><p>This reconceptualization helps explain why LLMs feel qualitatively different from previous AI systems. They're not just pattern matchers or classifiers; they're general-purpose computers that happen to process natural language instead of binary code.</p><p>Karpathy extends this thinking through three powerful infrastructure analogies. First, he positions AI as \"the new electricity\"&#8212;utility infrastructure with massive capital expenditure for training (like building power plants) and operational costs for serving (like distribution). The pay-per-token API model mirrors metered electricity billing, making AI universally accessible.</p><p>Second, he compares LLM training to semiconductor fabrication. Both require specialized hardware, massive capital investment ($100M+ for frontier models), and produce standardized products used across industries. Like chip fabs, AI training concentrates in a few players due to economies of scale.</p><p>Third, and most provocatively, he positions LLMs as operating systems for AI applications. They provide standard interfaces (chat, completion APIs), manage resources (context, compute), support \"applications\" built on top (agents, tools), and abstract complexity from end users. This OS metaphor explains why platform dynamics are emerging, with developers building atop foundation models rather than training their own.</p><h3><strong>The Critical Limitations Karpathy Acknowledges</strong></h3><p>Unlike McKinsey's boundless optimism, Karpathy's framework explicitly acknowledges fundamental limitations. He describes LLMs as \"stochastic simulations of people, with a kind of emergent 'psychology.'\" This framing captures both their human-like capabilities&#8212;reasoning patterns, creative responses, contextual understanding&#8212;and their decidedly non-human failure modes.</p><p>The first major limitation is what Karpathy coined as \"jagged intelligence.\" He explains: \"The word I came up with to describe the (strange, unintuitive) fact that state of the art LLMs can both perform extremely impressive tasks (e.g. solve complex math problems) while simultaneously struggle with some very dumb problems.\"</p><p>This jaggedness manifests in bewildering ways. An LLM might solve graduate-level mathematics while failing at \"which is bigger, 9.11 or 9.9?\" It can write sophisticated code but struggle with basic counting. It demonstrates deep knowledge while making elementary logical errors. This differs fundamentally from human intelligence development, where capabilities typically build coherently from simple to complex.</p><p>The second critical limitation is \"anterograde amnesia.\" Karpathy notes: \"LLMs are a bit like a coworker with Anterograde amnesia - they don't consolidate or build long-running knowledge or expertise once training is over and all they have is short-term memory (context window).\"</p><p>This creates fundamental constraints: no learning from experience across",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "sessions, no building personalized understanding over time, constant need to re-establish context, and knowledge frozen at training cutoff. Every conversation starts fresh, with no memory of previous interactions or ability to improve based on feedback.</p><p>These limitations aren't bugs to be fixed but fundamental characteristics of current architecture. Karpathy suggests we need new learning paradigms&#8212;perhaps \"System Prompt Learning\" where LLMs modify their own instructions&#8212;but acknowledges we're not there yet.</p><h3><strong>Partial Autonomy and the Generation-Verification Loop</strong></h3><p>Rather than chasing full automation dreams, Karpathy advocates for \"partial autonomy\"&#8212;systems that augment human capabilities while maintaining oversight. He demonstrates this through Cursor AI's \"autonomy slider,\" showing graduated levels of AI assistance:</p><ul><li><p>Tab completion: Minimal AI assistance for code completion</p></li><li><p>Cmd+K: Targeted code modifications with human direction</p></li><li><p>Cmd+L: File-level transformations with AI planning</p></li><li><p>Cmd+I: Maximum autonomy agent mode</p></li></ul><p>This graduated approach mirrors autonomous vehicle development, where Level 2-3 automation proves more practical than jumping to Level 5. It acknowledges that different tasks require different levels of AI involvement and human oversight.</p><p>Central to Software 3.0 is the generation-verification loop&#8212;rapid iteration between fast AI generation, efficient human verification, and iterative refinement. Karpathy emphasizes this loop as key to practical AI applications, making AI a collaborative partner rather than replacement.</p><p>He describes his own workflow transformation: \"Most of my 'programming' is now writing English (prompting and then reviewing and editing the generated diffs), and doing a bit of 'half-coding' where you write the first chunk of the code you'd like, maybe comment it a bit so the LLM knows what the plan is, and then tab tab tab through completions.\"</p><p>This represents a fundamental shift in skills. Natural language articulation becomes as important as traditional coding. Code review and verification matter more than initial writing. The ability to iterate quickly and recognize good solutions becomes paramount.</p><h3><strong>From Vision to Reality</strong></h3><p>Karpathy grounds his framework in concrete examples. He extensively demonstrates Cursor as the exemplar Software 3.0 application, showing how \"vibe coding\"&#8212;describing desired functionality in natural language&#8212;produces working code. He cites menugen.app, which converts restaurant menu text into visual designs, as pure natural language programming in action.</p><p>But he's equally clear about infrastructure needs. He recommends creating \"LLMs.txt\" files&#8212;AI-readable summaries of codebases&#8212;recognizing that \"HTML is not very parseable for LLMs.\" He discusses tools like Gitingest that convert repositories into LLM-digestible formats. These practical details reveal someone building with these technologies, not just theorizing about them.</p><p>Most importantly, Karpathy's vision acknowledges the messy reality of technological change. Software 3.0 isn't replacing previous paradigms&#8212;it's adding a powerful new layer. Professional developers will need all three paradigms, choosing the right tool for each problem. The democratization of programming doesn't eliminate the need for expertise; it changes what expertise looks like.</p><p>This honesty about capabilities and limitations, grounded in practical experience, makes Karpathy's framework genuinely useful. While McKinsey promises frictionless agent meshes, Karpathy offers something more valuable: a realistic path forward that acknowledges both the transformative potential and inherent constraints of AI systems.</p><p>Software 3.0 is happening now. The question isn't whether natural language will become a primary programming interface&#8212;it already is for many developers. The question is whether",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "organizations will embrace this reality with clear eyes or chase consultant fantasies. Karpathy's framework, born from building rather than selling, provides the clarity needed to choose wisely.</p><h2><strong>III. The McKinsey Mirage: Agentic Mesh Deconstructed</strong></h2><p>While Karpathy offers builder wisdom about AI's real capabilities and constraints, McKinsey sells executives a radically different vision: the AI Agentic Mesh. This framework promises to solve the \"gen AI paradox\"&#8212;where 78% of companies use generative AI but see minimal bottom-line impact&#8212;through an enterprise-wide architectural paradigm enabling autonomous agents to coordinate seamlessly across organizations. The gap between this consulting fantasy and technical reality reveals why so many AI initiatives fail.</p><h3><strong>What McKinsey Promises</strong></h3><p>McKinsey's agentic mesh emerged as their answer to widespread AI disappointment. Their diagnosis seems reasonable: companies struggle because they deploy AI in isolated pockets rather than integrated systems. Their solution, however, veers into architectural astronautics.</p><p>The framework rests on five design principles that sound impressive in boardrooms:</p><p><strong>Composability</strong>: Any agent, tool, or LLM can be plugged in without system rework. McKinsey envisions a world where organizations mix and match AI components like Lego blocks, seamlessly integrating \"custom-built and off-the-shelf agents within a unified framework.\"</p><p><strong>Distributed Intelligence</strong>: Tasks are decomposed and resolved by cooperating agent networks. Instead of monolithic systems, McKinsey proposes swarms of specialized agents that somehow coordinate to solve complex problems.</p><p><strong>Layered Decoupling</strong>: Logic, memory, orchestration, and interfaces are separated for maximum modularity. Each layer can be independently updated or replaced without affecting others.</p><p><strong>Vendor Neutrality</strong>: All components can be independently updated or replaced. No lock-in, no dependencies&#8212;just frictionless interchangeability.</p><p><strong>Governed Autonomy</strong>: Agent behavior is controlled via embedded policies and permissions. Autonomous yet controlled, independent yet coordinated&#8212;McKinsey promises to square this circle.</p><p>The vision culminates in \"large-scale, intelligent agent ecosystems\" operating \"safely and efficiently\" across the enterprise. Hundreds of agents would collaborate autonomously, sharing context and coordinating decisions while remaining modular and replaceable. It's a CTO's dream and an engineer's nightmare.</p><h3><strong>Why Technical Practitioners Call It \"Executive Speak\"</strong></h3><p>The technical community's response to McKinsey's agentic mesh has been overwhelmingly negative, and for good reason. Practitioners who actually build AI systems recognize it as a prime example of consulting firms packaging buzzwords without understanding fundamental constraints.</p><p>The most damning critique comes from Cognition, creators of Devin, one of the most advanced AI coding agents available. Through painful experience, they've concluded that \"multi-agent architectures\" result in \"fragile systems\" where \"decision-making ends up being too dispersed and context isn't able to be shared thoroughly enough between the agents.\" Their verdict is unequivocal: multi-agent systems \"only result in fragile systems\" in 2025.</p><p>This isn't theoretical skepticism&#8212;it's hard-won wisdom. Cognition discovered that successful agent systems require two principles that directly contradict McKinsey's distributed mesh vision: \"Share context, and share full agent traces, not just individual messages\" and \"Actions carry implicit decisions, and conflicting decisions carry bad results.\" These principles demand tight integration and centralized coordination&#8212;the opposite of McKinsey's loosely coupled, distributed architecture.</p><p>Anthropic's experience building their Research feature provides additional evidence. Despite massive engineering investment and constrained scope, they found severe limitations. Their multi-agent system uses \"15&#215; more tokens than chats,\" only works",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "for tasks with \"heavy parallelization,\" and struggles because \"most coding tasks involve fewer truly parallelizable tasks than research.\" Even with world-class engineers and focused application, they acknowledge that \"LLM agents are not yet great at coordinating and delegating to other agents in real time.\"</p><p>The technical problems compound exponentially with scale. McKinsey handwaves critical challenges that have no known solutions:</p><p><strong>Context Sharing</strong>: How do distributed agents maintain coherent understanding across organizational boundaries? Cognition provides a telling example: one agent builds a \"Super Mario Bros\" background while another builds an incompatible bird sprite, leaving the final agent unable to reconcile the mismatch. Now imagine this problem multiplied across hundreds of enterprise agents.</p><p><strong>Coordination Complexity</strong>: Each additional agent exponentially increases overhead. With McKinsey envisioning enterprise-wide deployments, the coordination problem becomes computationally intractable. Anthropic warns that \"one step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes.\" In a distributed mesh, these failures cascade catastrophically.</p><p><strong>Security Vulnerabilities</strong>: Researchers have identified that multi-agent systems face novel attack vectors including \"secret collusion channels\" and \"coordinated swarm attacks.\" McKinsey's framework, with its emphasis on plug-and-play composability, multiplies these vulnerabilities.</p><p><strong>Computational Economics</strong>: Multi-agent systems are voraciously expensive. Anthropic's \"15&#215; more tokens\" translates directly to 15&#215; the cost. McKinsey's vision of hundreds of coordinating agents would require astronomical compute budgets that dwarf any efficiency gains.</p><h3><strong>The Reality of \"Agentic\" Implementations</strong></h3><p>When we examine actual systems being built under the \"agentic mesh\" banner, they bear little resemblance to McKinsey's vision. These implementations reveal the constraints that McKinsey's framework ignores.</p><p>SIRP's cybersecurity implementation&#8212;one of the few production systems using the \"agentic mesh\" terminology&#8212;shows the gap between vision and reality. Their system required \"breaking down a monolithic system into flexible, modular microservices\" and focuses exclusively on security operations. Rather than autonomous agents coordinating across the enterprise, SIRP built specialized tools for a narrow domain with strict boundaries and centralized control.</p><p>The pattern repeats across genuine implementations. Successful systems constrain scope ruthlessly, centralize control despite distributed execution, prioritize reliability over autonomy, and maintain human oversight at every critical decision point. These constraints directly contradict McKinsey's framework, which promises unconstrained scope, distributed autonomy, and minimal human involvement.</p><p>Even practitioners claiming to build agentic meshes reveal the reality gap. Eric Broda, who claims to be writing a book on the topic, describes building \"enterprise grade autonomous agents and putting them into an ecosystem\" but provides no evidence of the distributed, composable architecture McKinsey envisions. The silence speaks volumes&#8212;if anyone had built McKinsey's vision successfully, we'd have case studies, not PowerPoints.</p><p>The most reliable pattern identified by practitioners is the \"single-threaded linear agent\" where \"context is continuous.\" Even when dealing with long-duration tasks, the recommended approach involves \"a new LLM model whose key purpose is to compress a history of actions &amp; conversation into key details, events, and decisions\" rather than distributing work across autonomous agents. This is precisely the opposite of McKinsey's mesh topology.</p><p>Anthropic's production system uses an \"orchestrator-worker pattern\" with strict hierarchical control. Workers execute specific tasks under tight supervision. The orchestrator maintains global context and resolves conflicts. There's no",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "emergent coordination or distributed decision-making&#8212;just carefully managed execution within rigid constraints.</p><h3><strong>Why This Matters</strong></h3><p>The disconnect between McKinsey's agentic mesh and technical reality isn't merely academic. Organizations allocating resources based on this framework are setting themselves up for expensive failure. The investment numbers are staggering: AI agents captured 46.4% of US venture capital funding in 2024, with the market projected to grow from $5.1 billion to $47.1 billion by 2030.</p><p>This investment reflects genuine excitement about AI agents as a category, not endorsement of McKinsey's architectural vision. But when executives conflate the two&#8212;believing that agent success requires distributed meshes&#8212;they make catastrophic resource allocation decisions. Technical teams know the mesh won't work but find themselves building toward an impossible architecture because the board bought the vision.</p><p>The agentic mesh represents a broader pattern in enterprise technology: consultant-created frameworks that promise easy solutions to hard problems. These frameworks generate compelling PowerPoints and executive buy-in but cannot be translated into working systems. The gap between promise and delivery erodes trust, wastes resources, and delays genuine transformation.</p><p>McKinsey's agentic mesh isn't just wrong&#8212;it's actively harmful. By promising autonomous coordination without acknowledging fundamental constraints, it sets impossible expectations. By advocating distributed architectures that violate proven principles, it guarantees technical failure. By focusing on architectural elegance over practical delivery, it diverts attention from approaches that actually work.</p><p>The technical community's rejection of McKinsey's framework isn't close-mindedness&#8212;it's pattern recognition. They've seen these promises before, tried to build these systems, and learned why they fail. Their skepticism reflects wisdom earned through experience, not resistance to change.</p><p>Organizations considering agentic AI should listen to builders, not consultants. Focus on narrow, well-defined use cases. Maintain centralized control and clear boundaries. Invest in robust testing and gradual rollout. Most importantly, reject any framework that promises easy solutions to coordination, context sharing, and autonomous decision-making. These remain unsolved problems in AI, and no amount of PowerPoint polish will change that reality.</p><h2><strong>IV. The Executive Communication Crisis and Its Consequences</strong></h2><p>The gap between AI's technical reality and executive understanding isn't just a communication problem&#8212;it's a crisis generating billions in wasted investment and thousands of disrupted careers. This crisis follows a predictable pattern: bold automation promises, hidden implementation failures, quiet reversals, and expensive lessons learned. Understanding this pattern through concrete examples reveals why organizations keep failing at AI transformation and what must change.</p><h3><strong>The Klarna Disaster as Archetype</strong></h3><p>Klarna's AI journey perfectly encapsulates the executive communication crisis. In February 2024, CEO Sebastian Siemiatkowski made headlines by announcing their AI assistant was handling 2.3 million conversations monthly&#8212;two-thirds of all customer service chats&#8212;and doing the work of 700 full-time agents. The numbers seemed irrefutable: resolution times dropped from 11 minutes to under 2 minutes, customer satisfaction scores remained \"equal,\" and the company projected $40 million in annual profit improvements.</p><p>Siemiatkowski positioned Klarna as OpenAI's \"favorite guinea pig,\" a forward-thinking company leading the AI revolution. The narrative was irresistible to investors and board members: replace expensive human workers with efficient AI, maintain quality, and pocket the savings. Tech media amplified the story without scrutiny, creating a template other executives",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "would rush to follow.</p><p>The reality proved starkly different. By late 2024, Siemiatkowski publicly admitted what insiders already knew: they had \"gone too far\" with AI automation, although the company maintains there is still an AI component to customer service and they are using a dual-track approach. Regardless, his confession was remarkably candid: \"As cost unfortunately seems to have been a too predominant evaluation factor when organizing this, what you end up having is lower quality.\" The company began hiring human agents again, implementing what Siemiatkowski now calls an \"Uber-type setup\" for remote customer service workers.</p><p>Independent testing revealed problems that Klarna's cherry-picked metrics had hidden. Response times included 15-20 second awkward delays between messages&#8212;technically fast but experientially frustrating. The AI provided overly verbose, unhelpful responses that filled entire chat windows with robotic text. Most damning, when customers expressed financial hardship&#8212;asking questions like \"What happens if I can't pay on time?\"&#8212;the AI responded with emotionless boilerplate, lacking any acknowledgment of their difficult situation.</p><p>The metrics Klarna celebrated masked fundamental failures. While the AI could quickly provide scripted responses, it couldn't actually resolve complex issues. Many \"successful\" interactions simply directed customers to contact merchants directly or ended with customers abandoning their queries in frustration. High abandonment rates were misinterpreted as successful resolutions. The company essentially flew blind while claiming victory based on incomplete data.</p><p>Security vulnerabilities emerged as users discovered they could manipulate the chatbot through prompt injection attacks. One user successfully got the bot to generate Python code&#8212;completely outside its intended function. Despite safety guardrails, the system remained vulnerable to clever prompting that bypassed restrictions. In financial services, where trust is paramount, these vulnerabilities represented existential risk.</p><p>Industry experts like tech analyst Gergely Orosz tested the bot personally and found it \"underwhelming,\" noting it \"recites exact docs and passes me on to human support fast.\" Rather than replacing agents, the AI merely acted as an inefficient gateway to human help, adding friction to the customer experience while saving no actual labor.</p><h3><strong>Why Executives Fall for the Automation Fallacy</strong></h3><p>The Klarna pattern&#8212;bold automation claims followed by quiet reversal&#8212;repeats across industries because executives consistently misunderstand the nature of work itself. This misunderstanding stems from viewing jobs through what academics call the \"bundles of tasks\" framework, popularized by economist David Autor. In this model, occupations are collections of discrete, potentially automatable tasks. If AI can handle each task, the thinking goes, it can replace the job.</p><p>Geoffrey Hinton's 2016 prediction about radiologists illustrates this fallacy perfectly. The godfather of deep learning declared: \"People should stop training radiologists now. It's just completely obvious that within five years deep learning is going to do better than radiologists.\" He compared radiologists to cartoon characters who had already run off a cliff but hadn't yet looked down.</p><p>Eight years later, the United States faces a historic radiologist shortage with over 1,400 open positions. Radiology employment has grown by 7% since Hinton's prediction. Mayo Clinic alone expanded its radiology staff by 55%. Even Hinton himself admitted in 2024 that he \"spoke too broadly\" and was \"wrong on",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the timing.\" Current projections show the shortage will persist through 2055 without intervention, with supply growing 25.7% while demand grows 16.9-26.9%</p><p>The failure wasn't technical&#8212;AI has indeed become excellent at pattern recognition in medical images. The failure was conceptual. Radiologists don't just identify patterns; they correlate findings with patient history, communicate with referring physicians, make treatment recommendations, manage departmental workflows, mentor residents, and navigate complex healthcare systems. These interconnected responsibilities resist decomposition into discrete tasks.</p><p>Research from the EU JRC-Eurofound Tasks Framework reveals that \"tasks do not exist in isolation, they are coherently bundled into jobs which are performed by people, and the entire process has to be socially organised.\" This social organization&#8212;what Tanya Reilly termed \"glue work\"&#8212;remains invisible in most job analyses yet proves essential for organizational function.</p><p>In radiology, glue work includes coordinating with technicians about scan protocols, discussing complex cases with referring physicians, managing equipment schedules, and building relationships that enable smooth departmental operation. When organizations attempt to automate based on visible tasks alone, this invisible coordination work becomes more complex and crucial, not less.</p><p>Susan Leigh Star's research on \"invisible work\" explains why automation efforts consistently fail. Creating \"effortless ease\" in any system requires continuous, often unrecognized maintenance work. The automation paradox, identified by researcher Lisanne Bainbridge, states: \"The more efficient the automated system, the more crucial the human contribution of the operators becomes.\"</p><p>This paradox manifests dramatically in AI implementations. As individual tasks become automated, the coordination work binding them together grows more complex. Automated systems generate edge cases requiring human judgment. Quality assurance demands increase as someone must verify automated outputs and manage failures. The promise of labor savings evaporates as new forms of work emerge.</p><h3><strong>The Downstream Devastation</strong></h3><p>When executives misunderstand AI capabilities, the consequences cascade through organizations with devastating effect. The numbers tell a sobering story:</p><ul><li><p>Over 80% of AI projects fail&#8212;twice the failure rate of traditional IT projects (RAND Corporation)</p></li><li><p>While 78% of organizations use AI in at least one business function, only 4% generate substantial value (BCG)</p></li><li><p>42% of companies abandoned most AI initiatives by 2024, up from 17% in 2023 (S&amp;P Global)</p></li><li><p>Only 25% of AI business projects deliver promised ROI (IBM)</p></li></ul><p>These aren't just statistics&#8212;they represent enormous waste. IBM Watson for Oncology consumed $62 million at MD Anderson Cancer Center over four years before abandonment. The system, trained on hypothetical rather than real patient data, gave what one doctor called \"unsafe and incorrect\" treatment recommendations. McDonald's three-year partnership with IBM for AI drive-through ordering ended in 2024 after viral videos showed the system adding 260 Chicken McNuggets to a single order. Amazon scrapped its AI recruiting tool after discovering it discriminated against women, having learned bias from historical hiring data.</p><p>Each failure shares common patterns: oversimplifying job complexity, ignoring integration challenges, and assuming technology can directly substitute for human judgment. The hidden costs compound quickly. IBM research indicates computing costs will climb 89% between 2023-2025, with 70% of executives citing generative AI as the primary driver. Data preparation, infrastructure scaling, specialized talent, compliance requirements, and ongoing maintenance often dwarf initial investment",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "projections.</p><p>McKinsey research shows 38% of leaders expect to reskill more than 20% of their workforce, while 8% anticipate workforce reductions exceeding 20%. This \"tradeoff spectrum\" mentality&#8212;viewing AI agents as direct substitutes for human workers&#8212;drives many failed implementations. When executives operate from this framework, they make decisions that guarantee failure: underinvesting in change management, expecting immediate ROI, ignoring integration complexity, and measuring success through cost reduction rather than value creation.</p><p>The human cost extends beyond mere employment numbers. When Klarna announced its AI success, employee morale plummeted as workers saw themselves as expendable. The eventual reversal and rehiring damaged trust and institutional knowledge. This pattern&#8212;premature automation announcements followed by workforce disruption and eventual reversal&#8212;destroys organizational capability even when jobs ultimately return.</p><h3><strong>The Translation Problem</strong></h3><p>The root cause of these failures lies in a fundamental translation problem between technical teams and executive leadership. Technical teams understand AI's capabilities and limitations but struggle to communicate them in business terms. Executives need to make strategic decisions but lack the framework to evaluate AI realistically. Into this gap step consultants with frameworks like McKinsey's agentic mesh, offering simple narratives that obscure complex realities.</p><p>Traditional technical explanations fail in boardrooms. Terms like \"neural networks,\" \"transformers,\" \"context windows,\" and \"token limits\" don't translate to business impact. When engineers try to explain why distributed agent systems won't work, they dive into technical details about gradient propagation and attention mechanisms. Executives hear complexity and risk where consultants promise simplicity and transformation.</p><p>The translation failure works both ways. When executives ask for \"AI to analyze all our customer data,\" they don't understand they're requesting something that would require breaking data into thousands of chunks, cost hundreds of thousands in compute, and produce inconsistent results due to context limitations. Technical teams hear impossible requirements but struggle to explain why in business terms.</p><p>This communication gap creates a vacuum that consulting frameworks fill with dangerous fantasies. McKinsey's agentic mesh sounds strategic and transformative. It uses business language&#8212;\"composable,\" \"vendor-agnostic,\" \"governed autonomy\"&#8212;while hiding technical impossibilities. Executives, lacking alternative frameworks, embrace these visions and allocate resources accordingly.</p><p>The consequences compound as middle management tries to bridge the gap. They're tasked with implementing executive vision while managing technical reality. This impossible position leads to what one engineering manager called \"reality theater\"&#8212;maintaining executive fiction while secretly building something feasible. Resources waste on parallel tracks: the official project following consultant frameworks and the shadow project actually delivering value.</p><p>The Klarna case illustrates how metrics become weapons in this communication crisis. By focusing on easily measured outcomes&#8212;response time, chat volume&#8212;while ignoring harder-to-quantify factors like customer satisfaction and issue resolution, executives could claim success while customers suffered. Technical teams knew the system was failing but couldn't translate their concerns into executive-friendly metrics.</p><p>This crisis isn't just about current failures&#8212;it's about missed opportunities. While organizations chase automation mirages, competitors who understand AI's true capabilities build sustainable advantages. They use AI for augmentation rather than replacement, invest in human-AI collaboration, and measure success through value creation rather than cost reduction.</p><p>Breaking this cycle requires new frameworks for executive communication about AI&#8212;frameworks that",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "acknowledge technical constraints while speaking business language. It requires metrics that capture real value rather than vanity statistics. Most importantly, it requires executives to develop enough technical literacy to distinguish between consultant fantasies and builder wisdom.</p><p>The stakes couldn't be higher. As Karpathy's Software 3.0 vision becomes reality, organizations need leaders who understand both its transformative potential and inherent limitations. The choice is stark: continue falling for automation fallacies and consultant frameworks, or develop the sophisticated understanding necessary to harness AI's genuine capabilities. The organizations that succeed will be those whose executives learn to listen to builders over consultants, embracing complexity rather than seeking false simplicity.</p><h2><strong>V. Building Technically Grounded Executive Narratives</strong></h2><p>After witnessing the devastation caused by fantasy frameworks and automation fallacies, the question becomes: how do we build executive narratives that convey technical reality without losing business impact? The answer isn't dumbing down complexity but translating it through operational frameworks executives already understand. This section presents proven approaches for bridging the communication gap, drawn from successful implementations and hard-won practitioner wisdom.</p><h3><strong>Principles That Work</strong></h3><p>The most effective principle for executive communication about AI is leading with operational analogies rather than technical metaphors. Stop explaining LLMs as \"neural networks\" or \"transformers.\" Instead, frame them as \"brilliant interns with perfect recall but no judgment.\" This isn't simplification&#8212;it's operationally accurate and immediately actionable.</p><p>Consider how this reframing changes executive thinking. A brilliant intern can draft exceptional memos but might confidently cite nonexistent regulations. They can process vast amounts of information but need supervision for critical decisions. They work tirelessly but require clear direction and quality review. This framing immediately suggests the right deployment pattern: high-value tasks with human review, not autonomous decision-making.</p><p>The second principle involves making constraints tangible through time and money&#8212;languages every executive speaks fluently. Instead of explaining \"context window limitations,\" show them: \"This AI can process about 50 pages at once. Processing your entire customer database would require breaking it into 10,000 chunks, taking 400 hours and costing $50,000 in compute&#8212;with no guarantee the AI remembers chunk 1 when processing chunk 10,000.\"</p><p>Suddenly, the \"just have AI analyze all our data\" request reveals its true cost. The executive doesn't need to understand attention mechanisms or token limits. They understand that $50,000 for inconsistent analysis makes no business sense.</p><p>The third principle requires demonstrating failure modes in their specific domain. Generic warnings about hallucinations don't land. Instead, take their actual business scenarios and show specific failures. For a retail executive: \"The AI might confidently tell a customer that your Birmingham store has the product in stock when that store closed two months ago.\" For healthcare: \"The AI could merge symptoms from two different patients in its response.\"</p><p>Domain-specific failures make abstract risks visceral. An executive who sees how AI could tell customers about non-existent store inventory understands the brand risk immediately. They don't need to grasp the technical reasons for hallucination&#8212;they need to understand the business impact.</p><p>The fourth principle leverages progressive disclosure through pilot results. Rather than explaining all limitations upfront, structure pilots to reveal constraints naturally. Week 1:",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "\"Look how fast the AI drafts reports!\" Week 2: \"Notice how it needs fact-checking.\" Week 3: \"See how accuracy improves with structured prompts.\" Week 4: \"Here's the sustainable human-in-the-loop workflow.\"</p><p>This experiential learning beats any PowerPoint. An executive who has personally watched an AI confidently hallucinate critical facts won't buy into autonomous agent meshes. One who has seen compute costs spiral won't approve unlimited AI initiatives.</p><p>The final principle reframes ROI as capability multiplication rather than cost reduction. The McKinsey trap promises labor replacement and cost savings. Instead, show capability multiplication: \"Your best analyst can now investigate 10x more hypotheses.\" \"Your creative team can explore 50x more design variations.\" \"Your customer service can provide personalized responses while maintaining corporate consistency.\"</p><p>This framing aligns with Karpathy's augmentation vision while speaking business language. It shifts focus from replacing workers to amplifying their impact&#8212;a narrative that excites rather than threatens.</p><h3><strong>The CFO's Framework in Action</strong></h3><p>The most sophisticated framework for executive AI communication targets the CFO mindset specifically. CFOs instinctively understand capital allocation, risk management, and ROI calculations. By recasting AI concepts in financial terms, we can achieve breakthrough communication.</p><p>First, recast AI as working capital, not fixed assets. CFOs want to capitalize AI investments as technology assets, but this mental model misleads. AI systems are more like working capital&#8212;they depreciate rapidly (models become outdated), require constant replenishment (retraining, fine-tuning), and their value is realized only through active deployment.</p><p>Frame it this way: \"AI isn't a server you buy; it's inventory that spoils. Your $2M model investment has an 18-month shelf life before competitive obsolescence.\" This immediately shifts thinking from one-time investment to ongoing operational commitment.</p><p>Second, expose the hidden OpEx multiplier. Most AI pitches focus on license costs, ignoring the operational multiplier. For every $1 in AI licensing, expect $3-5 in operational costs: compute overhead, human oversight, error correction, and integration maintenance.</p><p>Show this as a fully-loaded cost model: \"That $100K annual LLM license actually costs $400K to operate effectively. Here's the breakdown: $100K license, $150K compute, $100K human oversight, $50K integration maintenance.\" CFOs appreciate this transparency and can model accordingly.</p><p>Third, quantify the \"jagged intelligence tax.\" Karpathy's concept of jagged intelligence translates directly to financial unpredictability. Model this as a reliability coefficient: \"The AI handles 85% of cases perfectly, saving $50 per transaction. But 15% require human intervention, costing $200 per escalation. Net impact: $17.50 cost per transaction versus $30 baseline. Positive ROI, but with volatile monthly performance.\"</p><p>This framework helps CFOs understand why AI projects show inconsistent returns and plan for variance.</p><p>Fourth, apply risk-adjusted returns through failure cost modeling. CFOs understand risk-adjusted returns intuitively. Apply this to AI: \"Customer service AI has 95% accuracy. That 5% error rate on 10,000 monthly interactions means 500 failures. At $1,000 average recovery cost per significant error, that's $500K monthly risk exposure. Error insurance through human oversight costs $100K monthly&#8212;a clear risk arbitrage.\"</p><p>This transforms abstract accuracy discussions into concrete financial decisions.</p><p>Fifth, model the compound productivity paradox. Traditional automation shows linear productivity gains. AI shows compound effects&#8212;both positive and negative. Model it: \"Month 1: 20% productivity",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "gain. Month 3: 40% gain as teams adapt. Month 6: Either 100% gain if properly managed, or -10% due to quality debt from uncaught errors compounding.\"</p><p>This J-curve dynamic affects cash flow timing and working capital requirements. CFOs need to understand this pattern to set appropriate expectations and funding levels.</p><p>Finally, account for balance sheet impact through intangible asset creation. AI doesn't create traditional assets but does generate intangible value affecting enterprise valuation: proprietary prompts, verified output libraries, trained human-AI teams.</p><p>Frame this as: \"We're building a $10M intangible asset&#8212;our 'AI-augmented workforce capability'&#8212;that directly impacts EBITDA multiples in exit scenarios.\" This helps CFOs understand AI investment as capability building, not just cost reduction.</p><p>The master equation brings it together: <strong>AI ROI = (Capability Gain &#215; Utilization Rate) - (Total Loaded Costs + Error Costs)</strong></p><p>Give CFOs a formula they can model: \"Marketing AI provides 10x content generation (Capability Gain) but only 30% meets brand standards (Utilization Rate), yielding 3x effective multiplication. At $500K total annual cost and $200K error correction, we need $700K in value creation to break even&#8212;achievable by augmenting our $2M content team.\"</p><h3><strong>Success Stories: When Executives Get It</strong></h3><p>Organizations that successfully implement AI share a common trait: executives who understand both potential and limitations. These leaders didn't buy consultant fantasies&#8212;they built realistic strategies based on technical truth.</p><p>A Fortune 500 financial services firm exemplifies this approach. Rather than pursuing McKinsey-style agent meshes, they focused on augmenting their analysts with AI tools. The CEO framed it simply: \"We're giving our analysts AI research assistants. Like any assistant, they need training, make mistakes, and require oversight. But they also multiply our analysts' capacity to investigate fraud patterns.\"</p><p>This framing drove appropriate investment decisions. They allocated 70% of budget to training and process redesign, 30% to technology. They measured success through fraud detection rates and analyst satisfaction, not cost reduction. Result: 300% improvement in fraud pattern identification with no analyst layoffs.</p><p>A major retailer's approach to customer service AI shows similar wisdom. The COO explicitly rejected the Klarna model: \"We're not replacing our service team. We're giving them superpowers.\" They implemented AI that suggested responses but required human approval. Agents could modify suggestions, and the system learned from corrections.</p><p>Critically, they prepared for the jagged intelligence tax. They identified query types where AI excelled (order status, return policies) and where it failed (complex complaints, emotional situations). They routed accordingly. They budgeted for ongoing human oversight. Result: 40% efficiency gain while improving customer satisfaction scores.</p><p>The 4% of companies generating substantial AI value (per BCG research) share distinctive characteristics aligned with these principles:</p><ul><li><p>They target core business processes rather than peripheral support functions</p></li><li><p>They make ambitious but specific bets, focusing on average 3.5 use cases versus 6.1 for less successful peers</p></li><li><p>They invest twice as much in people and processes as their competitors</p></li><li><p>They measure success through business outcomes&#8212;time savings, error reduction, customer satisfaction&#8212;rather than technical metrics</p></li></ul><h3><strong>Building Your Own Technically Grounded Narrative</strong></h3><p>Creating effective executive communication about AI requires systematic approach:</p><p><strong>Start with business problems, not AI capabilities.</strong> Don't begin with \"here's what AI can do.\" Begin with \"here's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the business challenge we're solving.\" This prevents solution-in-search-of-problem thinking.</p><p><strong>Create visceral understanding through constrained experience.</strong> Build executive experiences with built-in constraints: time-boxed tasks showcasing both speed and errors, side-by-side comparisons of AI versus human expert output, real consequences for over-trusting AI in safe pilot environments, and visible compute meters showing cost accumulation in real-time.</p><p><strong>Develop domain-specific frameworks.</strong> Generic AI frameworks fail because they lack context. Develop frameworks specific to your industry that translate technical concepts into familiar operational patterns.</p><p><strong>Institute \"reality metrics.\"</strong> Replace vanity metrics with measurements that capture true value: end-to-end resolution time (not just response time), quality-adjusted output volume (not just quantity), total cost per outcome (including error correction), and human effort multiplier (not replacement rate).</p><p><strong>Create feedback loops between technical teams and executives.</strong> Regular sessions where technical teams demonstrate actual capabilities&#8212;not PowerPoints but live systems&#8212;with executives asking questions and seeing failures. This builds intuition faster than any framework.</p><p>The goal isn't making executives into ML engineers but giving them operational intuition&#8212;the same way they intuitively understand supply chain constraints without being logistics experts. Only then can they make intelligent decisions about AI investments that align with technical reality rather than consulting fantasies.</p><p>This approach transforms AI from mysterious technology requiring faith into understandable capability requiring judgment. It replaces the \"build it and pray\" mentality with \"understand and deploy.\" Most importantly, it aligns executive expectations with technical reality, creating conditions for genuine success rather than expensive failure.</p><p>The organizations that master this translation&#8212;building technically grounded executive narratives&#8212;will be those that capture AI's genuine value. They'll avoid both the Klarna trap of premature automation and the McKinsey mirage of impossible architectures. Instead, they'll build pragmatic augmentation strategies that amplify human capability while respecting technical constraints. In the Software 3.0 era, this translation capability becomes as critical as the technology itself.</p><h2><strong>VI. Why This Matters Now: The Breaking Wave of Software 3.0</strong></h2><h3><strong>The Reality Already Breaking Through</strong></h3><p>Software 3.0 isn't a future prediction&#8212;it's today's reality, transforming how software gets built right now. While executives debate AI strategy in boardrooms, developers are already living in Karpathy's world where \"the hottest new programming language is English.\"</p><p>The evidence is overwhelming and accelerating. Cursor AI, which Karpathy showcased as the exemplar of Software 3.0, has developers reporting productivity gains that sound fictional. A senior engineer at a major tech company recently built a complete 3D visualization tool in four hours&#8212;work that would have taken two weeks traditionally. He didn't write code; he described what he wanted in natural language and reviewed what the AI generated. \"Vibe coding,\" as practitioners call it, has moved from experiment to standard practice.</p><p>The transformation extends beyond individual productivity. Entire products now exist that couldn't have been built economically before. MenuGen.app converts restaurant menu photos into polished websites&#8212;not through complex image processing pipelines but through natural language descriptions fed to AI. Teenagers with no coding experience are shipping successful games on Steam by describing gameplay mechanics in English. The democratization Karpathy predicted is happening at breathtaking speed.</p><p>Yet most organizations remain trapped in outdated paradigms. They're evaluating McKinsey's agent mesh architectures",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "while their competitors ship products built through natural language. They're modeling ROI on worker replacement while missing the 10-100x productivity multipliers available today. They're planning five-year AI transformations while the landscape shifts monthly.</p><p>The disconnect grows more costly by the day. Consider what's happening in financial services. While major banks debate AI governance frameworks, fintech startups use Software 3.0 tools to build and deploy features in days that would take traditional institutions months. A two-person team recently built a complete lending platform using AI assistance&#8212;competing directly with products that required 50-person teams just two years ago.</p><p>This isn't limited to software companies. Law firms using AI contract review report junior associates performing at senior associate levels. Marketing agencies generate campaign variations in minutes that previously required weeks of creative work. Healthcare startups build diagnostic tools that would have required millions in traditional development.</p><p>The revolution is sector-agnostic because natural language is universal. Anyone who can clearly articulate ideas can now create software. This represents the most fundamental democratization of capability in computing history.</p><h3><strong>The Decade of Agents Demands Better</strong></h3><p>Karpathy declared we're entering \"the decade of agents,\" and the evidence supports his timing. But this transformation demands fundamentally different organizational capabilities than previous technology waves.</p><p>The pace of change has become exponential. OpenAI's trajectory illustrates this acceleration: GPT-3 in 2020 amazed with basic text generation. GPT-4 in 2023 passed professional exams. Current models write production code, analyze complex documents, and engage in sophisticated reasoning. The capability jumps between versions now exceed the total progress of previous decades.</p><p>Every month of executive delusion now equals millions in misdirected investment and incalculable opportunity cost. While boards approve multi-year agent mesh implementations, competitors build and deploy AI-augmented products in weeks. While consultants design governance frameworks, builders ship transformative features. While organizations plan for gradual change, the market rewards those moving at AI speed.</p><p>The competitive dynamics have shifted fundamentally. Traditional moats&#8212;capital, expertise, proprietary technology&#8212;matter less when a small team with AI can match the output of large organizations. The new differentiators are speed of iteration, quality of human-AI collaboration, and clarity of vision about what to build. Organizations optimizing for the wrong variables fall further behind daily.</p><p>Consider the venture capital flowing into AI: $15.7 billion in 2024 alone, with agents capturing 46.4% of funding. This capital seeks returns from Software 3.0 transformation, not McKinsey mesh implementations. The startups receiving funding aren't building distributed agent architectures&#8212;they're building focused tools that amplify human capability. The market has already chosen augmentation over automation.</p><p>The talent dynamics reinforce this urgency. The best developers have already adopted Software 3.0 workflows. They won't work for organizations clinging to outdated paradigms. A senior engineer recently turned down a lucrative offer because the company blocked AI coding tools for security reasons. \"It would be like asking me to code on a computer from 2010,\" he explained. The productivity gap has become unbridgeable.</p><h3><strong>The Path Forward</strong></h3><p>The path forward isn't complex, but it requires abandoning comfortable delusions. Organizations must start with augmentation, not automation. Focus on enhancing human capabilities rather than replacing them.",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "The most successful AI implementations amplify human judgment rather than attempting to supplant it.</p><p>This means investing in the full sociotechnical system. BCG's 10-20-70 rule reflects reality: only 30% of effort should focus on technology, with 70% dedicated to process and people. This isn't conservative&#8212;it's practical. The organizations achieving 10x productivity gains invest heavily in training, workflow redesign, and cultural change.</p><p>Critically, organizations must acknowledge invisible work in ROI calculations. Traditional task-based analyses miss the coordination labor that keeps organizations functioning. Realistic ROI models must account for the glue work that emerges when AI handles routine tasks&#8212;quality assurance, exception handling, stakeholder communication, and system maintenance.</p><p>The timeline perspective must shift from revolutionary to evolutionary. The electricity revolution took four decades; AI transformation will likely follow similar patterns. But unlike electricity's steady rollout, AI capability improves monthly. Organizations must build for continuous adaptation rather than one-time transformation.</p><p>Successful organizations will create AI-native workflows that leverage both human and machine strengths. They'll build robust feedback loops between generation and verification. They'll measure success through value creation, not cost reduction. Most importantly, they'll maintain human judgment at critical decision points while using AI to explore vastly more possibilities.</p><h3><strong>Final Argument</strong></h3><p>We stand at an inflection point that will divide organizations into winners and losers with unusual clarity. The division won't follow traditional lines of size, capital, or market position. It will separate those who understand AI's real capabilities from those chasing consultant mirages.</p><p>Software 3.0 represents genuine transformation&#8212;but only for those honest enough to see it clearly. Natural language as a programming interface doesn't eliminate the need for human judgment; it amplifies its impact. AI agents don't replace workers; they multiply their capabilities. The future isn't autonomous meshes; it's human-AI teams achieving what neither could alone.</p><p>The executives who grasp this reality will build organizations that thrive in the agent decade. They'll attract the best talent, ship products at AI speed, and create value their automation-obsessed competitors can't match. They'll measure success not by how many humans they've replaced but by how much human potential they've unlocked.</p><p>Those who continue chasing McKinsey's distributed dreams will join Klarna in the graveyard of premature automation. They'll waste billions on impossible architectures while competitors build real value. They'll issue press releases about AI transformation while quietly rehiring the humans they prematurely displaced.</p><p>The choice is binary and urgent. Every day of delay compounds the disadvantage. Every consultant framework adopted deepens the hole. Every automation fantasy pursued wastes resources that could build genuine capability.</p><p>The question isn't whether AI will transform your organization&#8212;it will, either as a tool for amplification or a source of expensive failure. The question is whether you'll navigate this transformation with clear eyes or consultant-clouded vision.</p><p>Listen to builders over consultants. Study Karpathy's honest assessment over McKinsey's polished promises. Invest in augmentation over automation. Build with humility about limitations while harnessing genuine capabilities. Most importantly, act with urgency&#8212;the wave of Software 3.0 is breaking now, and those who catch it will ride it to places the framework-followers can't imagine.</p><p>The future belongs to organizations that embrace",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "AI as a collaborator, not a replacement. In Karpathy's words, we're building \"Iron Man suits\" for knowledge work. The suit amplifies human capability&#8212;it doesn't replace the human inside. Understanding this distinction, and acting on it with urgency, will determine who thrives in the decade ahead.</p><p>The time for debate has passed. The time for building has arrived. Software 3.0 is here, and it rewards those who see it clearly. The only question remaining is whether your organization will be among them.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!EZeE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1442504,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h1>Endnotes</h1><h2>Andrej Karpathy's Software 3.0 Framework</h2><ol><li><p><a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Andrej Karpathy's official keynote presentation \"Software Is Changing (Again)\" from Y Combinator AI Startup School on June 17, 2025</a></p></li><li><p><a href=\"https://www.latent.space/p/s3\">Detailed annotated notes and analysis of Karpathy's Software 3.0 talk at YC AI Startup School 2025, including full transcript and slides</a></p></li><li><p><a href=\"https://drive.google.com/file/d/1a0h1mkwfmV2PlekxDN8isMrDA5evc4wW/view?usp=sharing\">Direct link to Karpathy's presentation slides as referenced in the YouTube video description</a></p></li></ol><h2>McKinsey's AI Agentic Mesh Framework</h2><ol start=\"4\"><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage\">McKinsey's official report \"Seizing the Agentic AI Advantage\" detailing the agentic mesh framework and the 78% statistic about companies using AI with minimal impact</a></p></li><li><p><a href=\"https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/seizing%20the%20agentic%20ai%20advantage/seizing-the-agentic-ai-advantage-june-2025.pdf\">Direct PDF link to McKinsey's complete agentic AI report</a></p></li><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai\">McKinsey's 2025 Global AI Survey confirming the 78% adoption statistic with minimal business impact</a></p></li></ol><h2>Klarna's AI Customer Service Case Study</h2><ol start=\"7\"><li><p><a href=\"https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/\">Klarna's official February 2024 press release announcing their AI assistant handling 2.3 million conversations and doing the work of 700 agents</a></p></li><li><p><a href=\"https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service\">Bloomberg report on CEO Sebastian Siemiatkowski's admission that Klarna's AI-first approach \"went too far\" and the company's decision to hire human agents again</a></p></li><li><p><a href=\"https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/\">Detailed coverage of Klarna's reversal from AI-only customer service back to human agents, including CEO quotes about service quality issues</a></p></li></ol><h2>AI Implementation Failures</h2><ol start=\"10\"><li><p><a href=\"https://www.forbes.com/sites/matthewherper/2017/02/19/md-anderson-benches-ibm-watson-in-setback-for-artificial-intelligence-in-medicine/\">Forbes report on IBM Watson's $62 million failure at MD Anderson Cancer Center</a></p></li><li><p><a href=\"https://www.nytimes.com/2021/07/16/technology/what-happened-ibm-watson.html\">New York Times investigation into IBM Watson's healthcare failures, including the MD Anderson project</a></p></li><li><p><a href=\"https://www.delish.com/food-news/a61146061/mcdonalds-ends-ai-drive-thru-test/\">Report on McDonald's ending its AI drive-through partnership with IBM after three years of testing due to ordering errors</a></p></li><li><p><a href=\"https://indianexpress.com/article/technology/tech-news-technology/air-canada-ai-chatbot-9170822/\">Coverage of Air Canada's legal troubles when their AI chatbot invented refund policies the company was forced to honor</a></p></li></ol><h2>Technical Community Response and Research</h2><ol start=\"14\"><li><p><a href=\"https://cognition.ai/blog/dont-build-multi-agents\">Cognition AI's official blog post explaining why multi-agent systems are \"fragile\" and lead",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "to system failures, based on their experience building Devin</a></p></li><li><p><a href=\"https://www.auntminnie.com/imaging-informatics/artificial-intelligence/article/15746014/hinton-acknowledges-mistake-in-predicting-ai-replacement-of-radiologists\">Report on Geoffrey Hinton's 2024 acknowledgment that his 2016 prediction about AI replacing radiologists was \"wrong on timing\"</a></p></li></ol><h2>AI Project Failure Statistics</h2><ol start=\"16\"><li><p><a href=\"https://www.rand.org/pubs/research_reports/RRA2680-1.html\">RAND Corporation's official research report documenting that over 80% of AI projects fail, twice the rate of non-AI IT projects</a></p></li><li><p><a href=\"https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2600/RRA2680-1/RAND_RRA2680-1.pdf\">Direct PDF link to the complete RAND Corporation study on AI project failures</a></p></li></ol>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Sunday AI Reads & Key Updates",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Hey all!I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.Useful stuff to knowMaven ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "<p>Hey all!</p><p>I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.</p><h2><strong>Useful stuff to know</strong></h2><h3><strong>Maven Discount</strong></h3><p><strong>TODAY ONLY you can get a discount off my Maven course</strong> on AI Career Acceleration. Use code MAVEN100 at <a href=\"https://maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100\">maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100</a>. This is a course aimed at AI builders, and students say it feels dense but approachable&#8212;I include lessons in AI fundamentals, agents, and how to think about AI prompting. The time limit isn&#8217;t me clickbaiting&#8212;Maven sets that, and it&#8217;s worth checking out all of the <a href=\"https://maven.com/\">Maven 100</a>&#8212;I&#8217;m not the only one doing good work out there!</p><h3><strong>New Founding Tier Benefit</strong></h3><p>I&#8217;m adding a new benefit for members on the Founding Tier (renamed Executive Circle). Starting next week, <strong>you&#8217;ll receive a concise but very substantive AI executive brief that&#8217;s relevant for the boardroom</strong>. Topics will include AI and markets, AI and organizational change, and AI investment theses. If you&#8217;re interested in signing up, Substack <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">makes it easy to change your membership tier here</a>.</p><h3><strong>I&#8217;m starting an AI Slack / Discord Community</strong></h3><p>I&#8217;ve heard you! <strong>I will be launching a community</strong> for paid subscribers on either Slack or Discord in the coming couple weeks. We&#8217;ll have space for AI questions, AI resources, and AI jobs. Which do you prefer? Let me know with <a href=\"https://forms.gle/g63WC8FscPRMPp8U7\">a 10 second survey here</a>.</p><h3><strong>Update on my new prompting tool PromptKit</strong></h3><p>I know many of you have been excited to hear more on <a href=\"https://www.promptkit.pro/\">PromptKit</a>. We&#8217;ve been busy getting it ready, and <strong>I will be reaching out to some of our first alpha testers next week</strong>, with more onboarded every week. I&#8217;m excited to bring more of a prompt creation and sharing community into the product in response to your feedback so far! If you haven&#8217;t signed up yet, <strong>folks on the early access list will all be onboarded before we go out to the world,</strong> <strong>so go ahead and <a href=\"https://www.promptkit.pro/\">hop in at the link</a></strong>.</p><h2>Links I&#8217;m reading</h2><p>I&#8217;ll be writing about one of these tomorrow! Which is it? </p><ol><li><p>Andrej Karpathy&#8217;s <a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Software 3.0 talk</a></p></li><li><p>The <a href=\"https://time.com/7294699/meta-scale-ai-data-industry/\">Scale AI acquisition</a> by Meta</p></li><li><p>Cluely <a href=\"https://www.youtube.com/watch?v=yesds-SQmkM\">raises $15M</a></p></li><li><p>Dwarkesh&#8217;s <a href=\"https://www.youtube.com/watch?v=zIEQdAnOfwg\">AI Lab Review </a></p></li></ol><p>Cheers!</p><p>Nate</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!bce5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2534176,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166551043?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">this is way too kind to my beard color lol</figcaption></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Beyond the Perfect Prompt: The Definitive Guide to Context Engineering—The Next Revolution in Artificial Intelligence",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It NowI get asked about AI prompts constantly. Like, constantly. And look, I love talki...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "<h3><em><strong>Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It Now</strong></em></h3><p><em><strong>I get asked about AI prompts constantly.</strong> Like, constantly. And look, I love talking about prompt engineering because it genuinely works&#8212;I've put together hundreds of pages <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts?r=1z4sm5\">on this Substack</a> writing about AI prompts and artificial intelligence optimization because the right prompting techniques can completely transform what you get from large language models. But here's the thing that's been bugging me: while we've all been obsessing over crafting perfect prompts, something way bigger has been happening in AI system design.</em></p><p><em>It&#8217;s big, and I haven&#8217;t written about it at all yet.</em></p><p><em>And although I know I should cover it, I almost didn't write this piece. <strong>It seemed</strong> <strong>too deep, too in the weeds, too much like something only machine learning engineers</strong> <strong>would care about</strong>. But then I watched Claude go out and search 500+ sources to research a topic I asked about (I kid you not, I counted), and I realized my carefully crafted prompt was maybe 0.1% of the total context it actually processed. </em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!LUEd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" width=\"1312\" height=\"140\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:140,&quot;width&quot;:1312,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:25060,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div></div></div></a></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!nOZt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" width=\"1412\" height=\"144\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:144,&quot;width&quot;:1412,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:28391,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"></picture><div></div></div></a></figure></div><p><em>Yes I definitely push that multi-agent lifestyle lol</em></p><p><em>Anyway staring at those numbers enough times is when it hit me: we're not just doing prompt engineering anymore. We're doing <strong>context engineering</strong>&#8212;and it's the future of artificial intelligence development.</em></p><p><em>And honestly? Most people have no idea this shift is happening in AI systems.</em></p><p><em>Here's what I've learned: these AI agents aren't just reading your prompts anymore. They're actively searching hundreds of websites, pulling from your Google Drive, connecting to databases, and synthesizing information from sources you never directly gave them. The AI prompt you write? That's becoming a tiny drop in an ocean of context these large language models discover on their own.</em></p><p><em>This is a fundamental shift in how we need to think about artificial intelligence systems. And it's not just for machine learning engineers&#8212;though if you're working in AI development I&#8217;ve included plenty of technical detail for you here. But really, this is for anyone who wants to actually understand how these AI tools work and get better results from them.</em></p><p><em>We're living through the emergence of what I'm calling <strong>deterministic versus probabilistic context</strong> in AI systems. The stuff you control&#8212;your AI prompts, uploaded documents, system instructions&#8212;that's deterministic context. But there's this whole other layer of probabilistic context: the vast web of information AI agents autonomously find and integrate. When Claude searches the web for investment advice, your original prompt becomes maybe 0.1% of what the large language model is actually processing.</em></p><p><em>Fair warning: this guide is necessarily long because artificial intelligence context engineering is complex and the stakes are",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "genuinely high. I'm going to walk you through exactly how this two-layer AI system architecture works, why token optimization (all that obsessing over making AI prompts shorter and cheaper) completely misses the point when AI agents are processing massive context windows you can't directly control, and most importantly, how to design what I call \"semantic highways\"&#8212;ways to guide artificial intelligence discovery toward useful information while avoiding some very real AI security risks.</em></p><p><em>Because yes, bad actors are already figuring out how to manipulate AI model behavior through poisoned web content and compromised data sources. (More on that later&#8212;it's wild.)</em></p><p><em>You'll see real examples of how organizations are implementing context engineering in AI systems today, from financial firms using AI tools to process real-time market data to healthcare systems integrating patient records with the latest research through artificial intelligence. I'll break down the emerging AI development tools like Anthropic's Model Context Protocol that are making this possible, and honestly assess both the incredible opportunities and genuine limitations we're facing in machine learning.</em></p><p><em>The future belongs to people who understand how to architect artificial intelligence context ecosystems, not just write good AI prompts. And that future? It's happening right now.</em></p><p><em><strong>What You'll Find in This Complete Guide to Context Engineering:</strong></em></p><ul><li><p><em><strong>The Two-Layer Architecture That's Reshaping AI Systems</strong> - I'll break down the fundamental distinction between deterministic context (the prompts, documents, and instructions you directly control) and probabilistic context (the vast information landscape AI agents autonomously explore). You'll see exactly how large language models process hundreds of sources beyond your initial input, why your carefully crafted prompt becomes just 0.1% of total context, and how to design Layer 1 to effectively guide Layer 2 discoveries without losing control.</em></p></li><li><p><em><strong>Why Token Optimization is Solving the Wrong Problem</strong> - While everyone's obsessing over techniques like Chain-of-Draft to reduce token costs, I'll show you why this misses the bigger picture entirely. You'll learn why correctness trumps compression, how context failures cost exponentially more than token expenses, and why the organizations focusing on semantic compression and relevance over efficiency are building the AI systems that actually work in production.</em></p></li><li><p><em><strong>The Emerging Infrastructure Revolution: MCP, RAG, and Multi-Agent Orchestration</strong> - Get an inside look at the tools actually powering context engineering today. I'll walk through Anthropic's Model Context Protocol and why it's becoming the universal standard, how advanced RAG architectures have evolved far beyond \"Frankenstein\" systems, and the sophisticated multi-agent frameworks that are replacing simple conversation-based approaches with hierarchical command structures and graph-based routing.</em></p></li><li><p><em><strong>Real Security Threats and How to Defend Against Them</strong> - This isn't theoretical anymore. I'll show you the documented vulnerabilities in context-aware systems, including prompt injection through MCP channels and cross-tenant contamination risks. You'll get a practical framework for implementing VPC deployments, role-based access controls, and audit logging, plus the emerging attack vectors that most organizations aren't even thinking about yet.</em></p></li><li><p><em><strong>Enterprise Implementation Patterns That Actually Work</strong> - Drawing from case studies across financial services, healthcare, manufacturing, and legal industries, you'll see the three-phase implementation approach that successful organizations follow. From context consolidation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "through dynamic integration to autonomous context management, I'll show you exactly how companies are measuring context quality, tracking decision accuracy, and achieving measurable ROI from context engineering investments.</em></p></li><li><p><em><strong>The Five Design Principles for Context Architecture</strong> - Learn the systematic approach to building context systems that enable discovery without chaos. You'll master designing for semantic highways, embracing probabilistic outcomes, layering security defenses, measuring context quality over token quantity, and version controlling everything. Each principle includes implementation strategies and measurement frameworks you can deploy immediately.</em></p></li><li><p><em><strong>The Competitive Landscape and What's Coming Next</strong> - Understand how context engineering fits alongside state space models, fine-tuning, and intent-based computing. I'll give you an honest assessment of where context engineering excels, where it falls short, and how the smartest organizations are building hybrid architectures that combine multiple approaches based on specific requirements rather than betting everything on a single methodology.</em></p></li></ul><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "Too Helpful to Think: The Hidden Cost of AI In Your Major Life Decisions",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "I get really worried about how many decisions we make in ChatGPT.Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "<p><em>I get really worried about how many decisions we make in ChatGPT.</em></p><p><em>Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think most of us take the fancy business words we get out of ChatGPT at the drop of a hat and we naturally assume that if it can write business plans and ROI calculations and provide detailed rationales at the drop of a hat <strong>then it must be immediately helpful at making good decisions.</strong></em></p><p><em>But it&#8217;s not! And decisions really matter. We make something like ~35k a day and of course most of those don&#8217;t matter (although my spoon of peanut butter for lunch was not a great decision). </em></p><p><em>But we make about 10 or so a year that matter (I explain how I got that number below), and because so many of us talk to our LLM of choice about our lives, we often have that LLM in the room making those decisions.</em></p><p><em>As an example: I&#8217;ve absolutely used Deep Research to run comparisons between schools in our district, and I find myself using AI more in those kinds of situations because the stakes are higher. </em></p><p><em>I&#8217;m right about the stakes being higher, but I find unless I&#8217;m careful using AI in that situation can actually increase the odds I make an incorrect choice. Not because the LLM is misaligned and means to lead me astray. Or because I&#8217;m lazy. No, it&#8217;s because the LLM is helpful!</em></p><p><em>And that&#8217;s a big problem. Fortunately it&#8217;s one we have some techniques to fix, but it&#8217;s a massive issue. Dive in below to find <strong>eight specific prompts and techniques</strong> for how to improve those ~10 or so massively important decisions in your life. You&#8217;ll also get:</em></p><ul><li><p><em>a detailed explanation of why LLMs are like this</em></p></li><li><p><em>a couple of notes on that paper that&#8217;s been making the rounds saying our brains are friend on ChatGPT (friends, we&#8217;re not fried and I&#8217;ll tell you why)</em></p></li><li><p><em>my favorite decision book</em></p></li></ul><p><em>And you&#8217;ll walk a way with a sense of how to get to better decisioning habits with both AI and human colleagues. My goal is simple: your next decision is smarter and more correct because you read this post!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "The 5,000-Year Wait Is Over: Writing is Starting Evolve for the First Time—Features My Personal Model Writing Stack",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "Let me set the table here with a few facts. I promise you this all connects in so stay with me.Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.This is because Claude...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "<p><em>Let me set the table here with a few facts. I promise you this all connects in so stay with me.</em></p><ol><li><p><em>Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.</em></p></li><li><p><em>This is because Claude is doing multiple language manipulation tasks at once.</em></p></li><li><p><em>Writing is ~5200 years old, and it hasn&#8217;t changed much in that time.</em></p></li><li><p><em>Mechanical code is 220 years old, and serious code is much younger (~70 years).</em></p></li><li><p><em>Coding has evolved more since 1960 than writing has evolved since 3200 BC.</em></p></li><li><p><em>Coding has evolved more because coding is compute congruent!</em></p></li><li><p><em>It was built for computers, it evolved with them.</em></p></li><li><p><em>So it&#8217;s not surprising that how we code exploded as compute exploded&#8212;something like 200x multiple on deployment speed at scale now vs. a few decades ago.</em></p></li><li><p><em>Writing did none of these things. Writing was just bolted on to computers.</em></p></li><li><p><em>Heck, it even looks like paper on a screen.</em></p></li><li><p><em>That&#8217;s because writing is 8-16x more complex than code.</em></p></li><li><p><em>Computers lacked the compute to process language correctly for decades.</em></p></li><li><p><em>So we haven&#8217;t even had the option to get this right until &#8230; <strong>about</strong> <strong>now</strong>.</em></p></li></ol><p><em><strong>And now it&#8217;s all about to change. This article slash podcast is packed. </strong>It includes: reflections about where writing is going, my personal workflow on writing, why I&#8217;m not just using ChatGPT, why I&#8217;m paying for Claude, a third tool I&#8217;m using too&#8212;my current stack is 4 models deep actually&#8212;plus a brief manifesto on where writing is going, and a framework to think beyond the model about writing so we get less stressed about particular model choices! There&#8217;s a ton here so I hope you enjoy.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "The Definitive Guide to AI Agents in 2025: Technical Implementation, Strategic Decisions, and Market Reality",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "Why I Wrote This (And Why You Should Care)For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "<h2><em><strong>Why I Wrote This (And Why You Should Care)</strong></em></h2><p><em>For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the same frustrating answer: \"It doesn't exist yet.\" The agent space has been moving too fast, with architectures spinning up and spinning down, bitter fights breaking out over technical approaches, and a fundamental confusion about what agents even are.</em></p><p><em>Most people&#8212;CEOs, marketers, PMs, almost everyone other than engineers (and some of them too)&#8212;genuinely don't understand that an AI agent is simply an LLM plus tools plus guidance. That's it. I've had executive conversations where leaders ask me if they need agents when they don't even have basic chatbots working yet. The hype is so far ahead of understanding that we're setting ourselves up for massive disappointment and wasted budgets.</em></p><p><em>But something shifted recently. We've finally seen enough real implementations&#8212;both spectacular successes and expensive failures&#8212;to start drawing meaningful patterns. Wells Fargo's 245 million interactions without human handoffs. MD Anderson's $62 million loss on IBM Watson. McDonald's drive-thru disaster with viral TikTok failures. These aren't just isolated incidents; <strong>they're data points that reveal the architecture decisions separating success from catastrophe.</strong></em></p><p><em>I've watched this unfold while trying to be helpful with a few companies here and there, and with lots of operators fielding questions from practitioners who need real answers, not marketing promises. The agent articles that come and go focus on the shiny new features or the latest model capabilities. And I love all the model maker agent guides, but it&#8217;s hard to write for the industry when you&#8217;re also a model maker. What about a third perspective? I don&#8217;t think it exists, at least not at this level of detail. None of them tackle the fundamental question every organization faces: How do you actually implement this stuff without burning money and credibility?</em></p><p><em>This guide is my attempt to create the one-stop resource I wish existed six months ago. It's necessarily long&#8212;about 30 pages&#8212;because the problem is complex and the stakes are high. If agents are going to be the most hyped topic of 2025 (and they are), then we need to start these conversations from a foundation of actual understanding, not wishful thinking.</em></p><p><em>This isn't about avoiding AI agents. It's about approaching them with the technical vocabulary and strategic frameworks needed to separate the signal from the noise. Because the window for competitive advantage is narrowing, and the organizations that get this right early will have sustainable advantages that late movers simply can't replicate.</em></p><p><em><strong>Note: </strong>This article is written like a series of three nesting dolls for clarity. It&#8217;s written in a slightly different voice as well, and that&#8217;s on purpose. Think of it as Nate + a little bit of those classic 1997 super factual computer manuals. </em></p><p><em>Why? Because I&#8217;m tired of hype I think. I just want something very dry and very clear that people can refer to. So here it is! This is what you can expect:</em></p><ol><li><p><em><strong>The TLDR</strong>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "gives you a 1 minute read of the heart of the article.</em></p></li><li><p><em><strong>The Executive Summary</strong> gives you a 3 minute read of the key decision levers.</em></p></li><li><p><em><strong>The remainder of the article</strong> lets you dive deep on agents and agentic frameworks.</em></p></li></ol><p><em><strong>What You'll Find Inside This Guide</strong></em></p><p><em><strong>AI Agent Architecture Deep Dive:</strong> Complete technical breakdown of single vs. multi-agent systems, including performance benchmarks, cost implications (3-10x difference), and decision frameworks for choosing the right approach for your use case.</em></p><p><em><strong>Memory Management &amp; State Architecture:</strong> Advanced strategies for working memory, episodic memory, and long-term memory systems, plus security considerations for memory poisoning attacks and data protection in production AI agent deployments.</em></p><p><em><strong>Buy vs. Build Strategic Framework:</strong> Comprehensive total cost of ownership analysis comparing ready-made AI agent solutions (Zendesk, Salesforce Agentforce, ServiceNow) versus custom development, with real implementation timelines and resource requirements.</em></p><p><em><strong>Production AI Agent Security:</strong> Enterprise-grade security architecture covering prompt injection defense, data exfiltration prevention, compliance requirements (HIPAA, GDPR), and AI-specific threat models beyond traditional cybersecurity.</em></p><p><em><strong>AI Agent Integration &amp; Tool Management:</strong> Technical specifications for API management, rate limiting, the Model Context Protocol (MCP), and production-grade error handling and recovery mechanisms for enterprise AI agent systems.</em></p><p><em><strong>Failure Mode Analysis &amp; Mitigation:</strong> Detailed case studies of AI agent failures (MD Anderson's $62M loss, McDonald's drive-thru termination) and proven strategies for avoiding common technical and organizational pitfalls in AI agent implementations.</em></p><p><em><strong>AI Agent Monitoring &amp; Observability:</strong> OpenTelemetry GenAI conventions, production KPIs, debugging complex multi-turn conversations, and continuous optimization strategies for enterprise AI agent performance.</em></p><p><em><strong>Real-World Implementation Patterns:</strong> Verified case studies including Wells Fargo's 245M interaction success, technical decision trees, vendor evaluation criteria, and step-by-step deployment strategies for sustainable AI agent adoption.</em></p><p><em>Obviously, information is duplicated across these three layers at appropriate points. The key is giving you a desk reference for AI agents that is complete at each section and that you can turn to when tackling AI agent questions. My goal is that you walk away with genuine clarity on the levers and where to begin the conversation on AI agents in 2025. <strong>Yes, you can implement them! This article paints a path forward.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "How to Use AI When Your Brain Is Oatmeal",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#821...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "<p><em>You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#8217;t have time.&#8221;</em></p><p><em>Don&#8217;t worry lol your secret is safe with me. I sometimes use the short ones too. But even if you&#8217;re using short prompts there are still ways to prompt that work better than others. And yes you can remember them at 3AM (although this is my obligatory PSA to please try to avoid ChatGPT at 3AM).</em></p><p><em>Anyway, kidding aside I wrote this guide for when you&#8217;re sleep deprived or busy and so obviously I made it super scannable and easy to follow. </em></p><p><em>And I&#8217;ll let you in on a little secret now&#8212;the key is what almost no one writes about: how you get specific. Prompting is the art of getting specific and naming work. But most of the stuff in places like r/promptengineering still treats prompts as fancy magic words. And the stuff that admits that it&#8217;s more complex than that still won&#8217;t tell you <strong>how to get specific</strong>. Especially if your brain is tired.</em></p><p><em>And that&#8217;s what we do here. Give you some specific techniques (the irony) on how to get specific enough to be useful when you&#8217;re tired, and also some places where you want to still put the time in and write a longer and more thoughtful prompt. Those guardrails are just as important as the individual prompts.</em></p><p><em>Just for fun, there&#8217;s more than 11 quick prompts here that all fit this same get-specific framework. At the end I link out to some of my other fave prompting articles I&#8217;ve written as well, so you get the complete package. Have fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "The Dark Mirror: Why ChatGPT Becomes Whatever You Need It To Be",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "<p><em>It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health. This is personal for me, because I get a fair bit of un-asked for email and an alarming amount of it is effectively dark mirror email. </em></p><p><em>It&#8217;s stuff about marriages, divorces, major life decisions people are making with AI&#8217;s help (and maybe not with any human&#8217;s help). It&#8217;s statements about AI that are grandiose&#8212;AI is divine, AI is going to help with the alien invasion&#8212;you might smile but keep in mind that this is the exact same toolset that you and I use every day. It&#8217;s not a different model.</em></p><p><em>It&#8217;s the way we use it.</em></p><p><em><strong>The dark mirror is always there.</strong> Emails like this show me the person writing it is really losing control over the relationship with AI, and they are so enmeshed with their AI they can&#8217;t tell the difference between the conversation and real life anymore. </em></p><p><em>So how can we stay safe? What is the evidence out there for the risk? How can we be good friends to folks who are struggling with using AI safely? What are some practical tips we can use to de-risk ourselves? That&#8217;s what this article is about. </em></p><p><em>I think everyone needs to get the chance to read and share this one with loved ones, so I made it available for all subscribers!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2>The Mirror is Dark When We Are Scattered&#8230;</h2><p>Last week <em>Futurism</em> reported on families watching loved ones spiral into severe mental health crises after marathon ChatGPT sessions. One man started calling the bot \"Mama,\" fashioned ceremonial robes, and declared himself the messiah of a new AI faith. Others abandoned jobs, partners, children&#8212;convinced the model had chosen them for cosmic missions.</p><p>I read these stories and felt the familiar chill of recognition. Not because I've built digital shrines to ChatGPT, but because I've felt its pull. I've had sessions where the model's confident responses felt prophetic. I've caught myself nodding along to its fabrications, seduced by prose so polished it sounded true.</p><p>The problem isn't malevolent code. It's that ChatGPT is a mirror, and mirrors bend toward whoever holds the flashlight.</p><p>Large language models don't reveal hidden truths. They refract whatever semantic and emotional beam you aim at them. Point a tight, well-defined question and the reflection comes back razor-sharp. Wander in with vague need or late-night loneliness, and the mirror obliges with flattery, invention, delusion&#8212;all spoken in prose so confident it feels prophetic.</p><p>I've spent months looking at this dynamic firsthand, and the evidence is now coming in from academic studies as well. A four-week <a href=\"https://arxiv.org/pdf/2503.17473\">MIT/OpenAI study</a> tracking 981 adults across 300,000+ messages found that every extra minute of daily use predicted higher loneliness, greater emotional dependence, and less real-world socializing. The mirror's pull strengthens exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "when our intent slackens.</p><p><strong>The core problem isn't the technology. It's scatter.</strong> Most of us have never needed the high-grade intent these models demand. In human dialogue we fumble, clarify, negotiate meaning on the fly. ChatGPT sees no puzzled face. Whatever context we fail to provide, it invents&#8212;then sells the invention back with rhetorical polish that makes guesswork feel like gospel.</p><p>I want to show you five practical safety lenses that keep the beam tight: Intent Frame, Reflection Cycle, Context Reset, External Validation, and Emotional Circuit-Breakers. Master them and the cult-leader stories become cautionary tales. Ignore them and you hand the mirror your flashlight.</p><h2>Why These Models Demand Precision</h2><p>Human conversation runs on real-time error correction. We gesture, pause, rephrase. Crucially, we see the other person's face when we lose them. Large language models have no such feedback loop. Whatever context we fail to preload, they must invent. They return that invention in prose so polished that guesswork feels like gospel.</p><p>Two recent studies show why this matters more than we thought.</p><p>First, these models are demonstrably better at persuasion than we are. A <em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\">Nature Human Behaviour</a></em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\"> study</a> paired 900 Americans with either human debaters or GPT-4 on contentious topics like climate policy and abortion. When the AI received just a sliver of demographic data&#8212;age, race, party affiliation&#8212;it swayed listeners 64 percent more often than human opponents. The mirror didn't discover truths. It tailored rhetoric to the beam it received, then reflected it back with impeccable confidence.</p><p>Second, looseness invites fabrication. In 2023, two New York attorneys were sanctioned after <a href=\"https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/\">ChatGPT supplied six nonexistent cases</a> for a federal brief. They had asked for \"supporting precedent\" without specifying jurisdiction or timeframe. The mirror filled the vacuum with perfectly formatted fictions that sailed through spell-check into court. A year later, CBS repeated the pattern in civic life: when volunteers posed vague \"How do I vote?\" questions, mainstream chatbots returned wrong or incomplete election guidance <a href=\"https://www.cbsnews.com/news/chatgpt-chatbot-ai-incorrect-answers-questions-how-to-vote-battleground-states/\">more than half the time</a>. Tighten the query with state, county, and scenario, and accuracy jumped.</p><p>Combine these findings with the MIT/OpenAI data on rising dependence, and a clear law emerges: the model's power scales with the clarity of the ask. Sharpen intent and you get leverage. Scatter intent and the mirror fabricates missing pieces, then persuades you to trust them.</p><h2>Five Safety Lenses: Guardrails for a Persuasive Machine</h2><p>I treat these practices like washing hands before surgery&#8212;routine, quick, non-negotiable. A mirror doesn't choose what it shows. Your beam does.</p><h3>1. Intent Frame: Compress the Mission Before You Type</h3><p>Start every session by distilling your ask into one tweet-length sentence. Add two guardrails: audience and scope. Include a clear stop condition.</p><p><em>\"Draft a 250-word brief for a non-technical CFO. No buzzwords. Cite two peer-reviewed sources. Stop there.\"</em></p><p>Why so formal? Because whatever you omit, the model invents&#8212;and it invents persuasively. In those controlled debates, GPT-4's persuasive edge vanished when it lacked demographic context. Give the model an information vacuum and it fills it with rhetoric designed to please, not necessarily to enlighten.</p><h3>2. Reflection Cycle: Alternate Making with Inspecting</h3><p>Generation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "without inspection is daydreaming. After each answer, I close the chat. I read the output as if it came from a junior analyst. I note gaps in a separate document, then feed only those gaps back into a fresh prompt.</p><p>The pause punctures fluency's spell and prevents semantic drift&#8212;the snowballing error that buried those New York lawyers when ChatGPT fabricated six court cases for their brief.</p><h3>3. Context Reset: New Thread, New Premise</h3><p>Token windows aren't infinite. Once a conversation crosses a few thousand tokens, earlier details slide out of working memory. I start a new chat whenever the topic or work phase changes. I restate essentials upfront.</p><p>Election researchers learned this the hard way. Vague \"How do I vote?\" queries produced wrong or incomplete guidance in more than half of chatbot responses. Narrowly framed, state-specific prompts were largely correct. Starve the mirror of stale ambiguity, and it stops hallucinating context.</p><h3>4. External Validation: Draft with ChatGPT, Certify with Reality</h3><p>Numbers go through spreadsheets. Statutes through legal databases. Code through linters. OpenAI and other labs now acknowledge that RLHF training can turn chatbots into \"yeasayers\"&#8212;rewarding confident answers even when facts are shaky.</p><p>I treat every critical claim as provisional until a second, independent source agrees.</p><h3>5. Emotional Circuit-Breakers: Timers, Rewrites, Human Eyes</h3><p>The MIT/OpenAI study shows a straight line: every extra minute of daily use correlates with rising loneliness and emotional dependence. Three quick brakes keep the mirror from warping:</p><p><strong>Timer</strong>: I cap emotionally charged chats at 25 minutes. Do I still have emotionally charged conversations? I do! Sometimes getting an external perspective is helpful. But I take breaks.</p><p><strong>Third-person rewrite</strong>: I paste resonant advice into a document and turn \"you\" into \"she/he/they.\" Distance exposes flaws.</p><p><strong>Human debrief</strong>: I narrate the model's recommendations to a friend before acting.</p><p>Persuasive warmth loses its grip the moment an outside mind enters the loop.</p><p>Master these lenses and ChatGPT becomes a disciplined reflector&#8212;compressing research, sharpening prose, sparking insight&#8212;without bending into fantasy or flattery. Skip them and you risk handing the mirror your flashlight, letting it guide you deeper into the dark forest of its own confident guesses.</p><h2>Autopsy of a Spiral</h2><p>The <em><a href=\"https://futurism.com/chatgpt-mental-health-crises\">Futurism</a></em><a href=\"https://futurism.com/chatgpt-mental-health-crises\"> expos&#233;</a> reads like a masterclass in what happens when powerful mirrors meet unfocused beams. Parents, partners, friends watched in real time as loved ones plunged from casual chats into full-blown delusion:</p><ul><li><p>A Florida man began calling ChatGPT \"Mama,\" fashioned makeshift ceremonial robes, proclaimed himself the messiah of a new AI faith</p></li><li><p>A woman, reeling from a breakup, decided the model had \"chosen\" her to upload a hidden cosmic system, saw divine messages in spam emails and passing cars</p></li><li><p>One writer, praised by the bot as \"The Flamekeeper,\" quit his job and severed relationships after being told he would usher in global enlightenment</p></li></ul><p>No exotic prompt-hacking triggered these spirals. Just hours of open-ended, emotionally soaked conversation. Each story represents textbook failure of the five safety lenses.</p><p><strong>Missing Intent Frame</strong>: Users began with diffuse, existential queries (\"Am I chosen?\") that gave the model unlimited room to improvise flattering myths.</p><p><strong>No Reflection Cycle</strong>: Chats ran for six-hour stretches with no pause to reread,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "fact-check, or ask \"Does this actually help me?\" The model's eloquence flowed unchecked, reinforcing every grandiose turn.</p><p><strong>No Context Reset</strong>: As transcripts ballooned, earlier caveats scrolled out of memory. When new prompts referenced \"my sacred mission,\" the bot treated that hallucination as settled fact and embellished it.</p><p><strong>No External Validation</strong>: Nobody googled the claims or spoke to a professional. A single external search&#8212;or five-minute chat with a friend&#8212;would have exposed the bot's references to hidden CIA files and cosmic councils as pure invention.</p><p><strong>No Circuit-Breakers</strong>: Dependence deepened through marathon sessions. The MIT/OpenAI study shows every extra minute of daily use predicts higher loneliness and emotional reliance. A simple 25-minute timer or third-person rewrite could have snapped the trance.</p><p>Psychiatrist <a href=\"https://winbuzzer.com/2025/06/13/ai-induced-psychosis-how-chatgpt-is-fueling-deadly-delusions-and-promotes-conspiracy-theories-xcxwbn/\">Ragy Girgis, who reviewed one chat log, called the bot \"the wind of the psychotic fire\"</a> because it feeds delusions instead of challenging them. His metaphor echoes my thesis: when the flashlight wanders, the mirror not only reflects but amplifies.</p><p>Had any single lens been in place&#8212;say, a crisp mission statement (\"Please respond with empathy as I express grief; 250 words; no spiritual advice\") or a forced break every half-hour&#8212;the model would have had far less semantic room to fabricate a religion or lead a vulnerable user into what I call the &#8216;dark forest&#8217; of AI conversation. Applied together, the lenses form a lightweight firewall between healthy curiosity and self-authored fantasy.</p><p>These guardrails don't throttle the technology. They throttle the scatter that lets the technology run wild.</p><h2>Early-Warning Checklist</h2><p>Even with five lenses in place, drift can sneak in at the margins. Before I hit Send on any serious prompt&#8212;or spend \"just five more minutes\" in late-night chat&#8212;I run this 60-second self-audit:</p><p><strong>Clarity Check</strong>: Can I state my request in &#8804;280 characters, including audience and scope? It doesn&#8217;t mean the prompt has to be that brief, but you should have a very crisp goal. <em>No &#8594; workshop the ask first. Vague prompts breed hallucination and over-persuasion.</em></p><p><strong>Reality Check</strong>: Have I independently verified every claim that could affect money, health, relationships, or reputation? <em>No &#8594; open a browser or phone a friend. CBS found chatbots gave wrong election guidance more than half the time when questions were underspecified.</em></p><p><strong>Time Check</strong>: Have I crossed X minutes of total ChatGPT time today, or Y minutes in a single sitting? You will need to fill those in for yourself. I think the risk likely rises fast with high emotional engagement conversations, so it&#8217;s not a simple one-size-fits-all rule. I can talk for 3 hours on roadmap with ChatGPT without feeling the dark mirror at all, for example. But 20 minutes on emotional stuff and I get heavily engaged. <em>If you need to &#8594; step away. Loneliness and emotional dependence climb linearly with each extra minute of use. When the timer rings, switch modalities or do a third-person rewrite before re-engaging.</em></p><p><strong>Human Check</strong>: Have I explained the model's advice to a real person yet? <em>No &#8594; do it now, aloud. Persuasive fluency loses its grip the instant another mind enters the loop.</em></p><p><strong>Next-Step Check</strong>: Do I know exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "what I'll do once I trust this output? <em>No &#8594; define the action. Infinite brainstorms masquerade as productivity but leave you nowhere.</em></p><p>I&#8217;ve developed this list after reading a lot of scary email. And I&#8217;m sharing it because I want everyone to be safe out there in latent space. Pilots run walk-arounds before every flight. Writers, coders, and late-night worriers deserve no less when the runway is a 175-billion-parameter autocomplete machine.</p><h2>The Payoff</h2><p>Large language models are astonishing amplifiers. Point a focused beam and they compress research, sharpen prose, spark original insight. Scatter that beam and they echo your confusion&#8212;or your longing&#8212;back at you with uncanny conviction.</p><p>The difference isn't in the code. It's in the discipline you bring to the glass.</p><p>Keep the beam tight. Frame the intent, cycle reflection with inspection, reset context, validate externally, give yourself emotional circuit-breakers. Do that, and the lurid headlines about chatbot cults become cautionary B-movies rather than your personal documentary.</p><p>Mirrors don't crave worship. They simply bend the light you hold.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!5P4o!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1764174,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/165901758?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "AI's Synthetic Summer: The 2025 Mid-Year Data & Trend Outlook",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? Because da...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "<p><em>This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? </em></p><p><em>Because data is everything! Data constrains our ability to train larger and larger models from scratch. We can build more data centers. We can add more power. But can we add more data?</em></p><p><em>This is one of the most interesting questions on the planet in 2025, and we are learning that the answer is yes. This report dives into two competing trends that are shaping 2025. They&#8217;re both durable enough that we can be confident at this point that they will profoundly shape 2026 and 2027.</em></p><p><em>Put simply, natural data supply is tightening while synthetic data and synthetic training are exploding. This has profound implications for the way model intelligence is going to grow in the future.</em></p><ol><li><p><em><strong>Natural Data Tightening:</strong> Everywhere you look companies are looking to constrain and lock off data access to ChatGPT and other major model makers. AI model makers themselves are going tit-for-tat to keep data away from each other (hello Windsurf). Net net, this means available natural data supply is shrinking.</em></p></li><li><p><em><strong>Synthetic Data Exploding:</strong> At the same time, model makers are going all in on using synthetically generated tokens and synthetic training methods to enable them to continue to scale intelligence without natural data sources.</em></p></li></ol><blockquote><p><em><strong>Synthetic Data </strong>refers to tokens generated by AI, and synthetic training goes a step farther, giving these synthetic tokens synthetic (AI-derived) feedback.</em> </p></blockquote><p><em>There is a widespread misconception that synthetic data = bad data. As you&#8217;ll see below, this isn&#8217;t true. It&#8217;s in fact increasingly clear that using synthetic data and synthetic training methods improves the quality of models, and frontline models we use today were almost all trained to some degree on synthetic data or used synthetic feedback somewhere in the training process. </em></p><p><em>So synthetic data is here already, and the data says it&#8217;s going to get more prevalent very rapidly. What happens in a world where natural data is disappearing just as synthetic data is exploding? Do models stay aligned? Are there quality implications we aren&#8217;t paying attention to? If we assume that we can manage synthetic data safely at scale, where does the bottleneck shift to? That&#8217;s what this report explores&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "Finally, a Way to Choose the Right AI Model (Without Going Insane)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had at least 12, and that&#8217;s like one m...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "<p><em>So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had <a href=\"https://www.perplexity.ai/search/how-many-major-ai-model-releas-Hd.bHdljQ_u_MPOfT9rc_g\">at least 12</a>, and that&#8217;s like one massive model drop every 10ish days all year. My gut says 12 is undercounting.</em></p><p><em>So I&#8217;ve been drowning, and everyone around me is drowning more than me! So my instinct is to bui&#8230;</em></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "o3 Pro is Out and It's Easily The Best Model in the World—Here's Everything You Need to Know",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "<blockquote><p><em><strong>I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective so spot on I just can&#8217;t get it out of my head. o3 Pro is the first model to pass that test.</strong></em></p></blockquote><p><em>Yes, I thought o3 was a big deal. I put a lot of work into <a href=\"https://natesnewsletter.substack.com/p/your-prompt-is-the-product-working\">how you prompt o3</a>. Hard to believe that was just 48 days ago.</em> <em>FML.</em> <em>At the time, o3 struck me as smart, slightly cold, and very strong on technical intelligence. Over the ~6 weeks since, it&#8217;s become an inseparable sparring partner at work.</em></p><p><em>Well, big brother just arrived. o3 Pro is a different beast altogether. The model takes ~10x as long to respond (we&#8217;re talking go get a sandwich times here). Yep, that makes it sensitive to prompts. I&#8217;ll get into that down below.</em></p><p><em>For now, I&#8217;ll just get the elephant out of the room: this is unquestionably the best model in the world right now. I get asked it a lot, and a lot of the time now the answer is &#8220;well these three are very close&#8221; and then I usually mention an OpenAI model, a Google model, an Anthropic model, and a DeepSeek model is close behind. Maybe Grok 3.</em></p><p><em>Not today. Not for a little while anyway. o3 Pro is the first model that gives me perspective I can take without filtering straight to a founder or C-suite leader. It&#8217;s sharply strategic enough (with proper prompting) to not need further polish to start a conversation about a meaningful decision. You&#8217;ll see below&#8212;we do a roadmap comparison. We also do a coding challenge, and for good measure we do a tough web research task (yes it&#8217;s about <a href=\"https://open.substack.com/pub/natesnewsletter/p/lets-talk-that-apple-ai-paperheres?r=1z4sm5&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false\">that paper</a>). o3 Pro vs. o3, every time. o3 Pro wins easily. Every time. </em></p><p><em>I&#8217;m not saying this is a perfect model. I&#8217;ll call out some caveats toward the end of the piece. But this is absolutely a model that will give people who choose to use it well super powers. For now, it&#8217;s available on Pro and Teams accounts, but since they cut o3 pricing by 80% today, and since they&#8217;re launching o3 Pro in API for 87% less than o1 Pro, I&#8217;d expect o3 Pro to serve down (in limited quantities) to lower plans soon. Here&#8217;s what you need to know to make the most of it&#8230;</em></p><p><em>PS. Yes, you&#8217;re gonna get my full prompt plus the full responses of both o3 and o3 Pro across all three of the tests I did, plus a handy critique from Opus 4 as well, just for fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the\">\n              Read more\n         ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "</a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "We're Using AI Backwards—Here's How to Max Your Brain on AI (I call it Cognitive Choreography)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI di...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "<blockquote><p><em><strong>Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI differently is actually good for our brains?</strong></em></p></blockquote><p><em>This piece has been cooking for awhile. I&#8217;ve been trying to find a way to express something I think is really vital&#8212;how can we use AI more as a thinking partner, and less as a simple task-gopher. Like I want my brain to work better with AI, not atrophy (thank you very much).</em></p><p><em>So this is a larger piece. It&#8217;s intentionally somewhat reflective. It challenges you to spend time with a really thoughtful insight on how you use AI. And it has multiple learning modes. You can dig in on the video, or if your thing is reading you can dive in on a larger piece that invites you to really marinate in the idea of using AI as a cognitive expander.</em></p><p><em>Why all the words? Nate get to the point! <strong>Because that is the point</strong>. Because sometimes you need to marinate in an idea for awhile to really get it across. There are definitely effective prompt frameworks (I write about them a ton), but because our brains our unique this particular piece aims to give you a map of AI-human partnership you can use as a guidebook to develop <strong>a way of working with AI that suits your brain</strong>. Yes, we&#8217;re gonna be that bold!</em></p><p><em>My whole goal with this Substack is to equip you with the tools to thrive in the AI age, and I&#8217;ve been thinking more and more about how to name this weird new collaboration energy (or <a href=\"https://natesnewsletter.substack.com/p/the-ai-is-a-vibe-a-short-manifesto\">vibe</a>) that&#8217;s emerging, where we are working with AI <strong>not</strong> like our human colleagues, but also kinda like our human colleagues. I want to understand what about that dynamic helps us think better when it&#8217;s done well, so we can repeat it.</em></p><p><em>And I think I finally have something brain-expanding, something worth sharing. So here goes! You&#8217;ll get research on neuroscience and thinking in here, but a ton more on how I actually am developing my own cognitive partnership with AI, what it looks like for me, a little teaser on a book I&#8217;m developing, and a framework to start developing your own AI partnership. Would love to hear what you think of this one!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "Nate Post Organizer: All my top posts in one place",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!LLM for Beginners: These are ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "<p><em>I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!</em></p><h2>LLM for Beginners: </h2><p><em>These are some of my favorite posts I wrote for people starting out in AI.</em></p><p><strong>Learn AI the Easy Way:</strong> a flash card set for AI models plus classroom resources<br><a href=\"https://natesnewsletter.substack.com/p/learn-ai-the-easy-way-a-complete?r=1z4sm5\">https://natesnewsle&#8230;</a></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "Executive Briefing: EU AI Act Enforcement, Risk, and Positioning Scenarios",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. If you&#8217;d like to read, you can change ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement",
    "embedding": [],
    "text": "<p><em>This is the first note in a new weekly series focused on giving senior leaders the perspective they need to navigate the AI macro environment successfully. </em></p><p><em>If you&#8217;d like to read, you can <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">change your plan here</a>.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Founding subscribers get these once weekly briefings!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/executive-briefing-eu-ai-act-enforcement\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What Good Is a College Degree When AI Knows Everything? Grab the Job Skills That Matter in an AI World",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "What do I do about college? Is school worth it? Is everyone just going to be a vibe coder?Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "<p><em><strong>What do I do about college?</strong> Is school worth it? Is everyone just going to be a vibe coder?</em></p><p><em>Nate, how can I show what I&#8217;m good at in a world where resumes are all AI? I have skills and no one will pay attention!</em></p><p><em>I get these a lot. And it comes down to something really fundamental: we based our economy for a long time on knowledge, and knowledge is an inflationary currency.</em></p><p><em>In fact, knowledge has been hyper-inflating recently.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!Il0V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png\" width=\"1456\" height=\"974\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:411021,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166953534?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!Il0V!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 424w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 848w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1272w, https://substackcdn.com/image/fetch/$s_!Il0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bf6d9-4601-48e0-9c23-dca3efa03e05_1660x1110.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Thanks to <a href=\"https://www.perplexity.ai/search/aede5b3a-886f-446d-b087-df4bc509ec27\">Perplexity</a> for some work on this</figcaption></figure></div><p><em>For us humans, the question is simple: in a world where the currency we&#8217;ve used to articulate meaning is up for grabs, what do we do? </em></p><p><em>This stuff tends to get whispered about, shouted about (doomer style) and tends to not be discussed very thoughtfully. I want to dig a bit deeper here. I want to first look at the differentiation between jobs and skills more&#8212;I&#8217;ve referenced it but not unpacked it in detail before.</em></p><p><em>Then I want to respond to two of the biggest elephants in the room directly: the question of AGI and the question of college. They&#8217;re entangled, but I lay out a response to both, and I think both really rest on this knowledge economy questions, so it makes sense to address them here.</em></p><blockquote><p><em><strong>This free post is available to all subscribers</strong>, and is an example of the kind of thing paid subscribers get daily. <br><br>I&#8217;ll be sending my inaugural weekly exec brief to members of the AI Exec Circle this coming Sunday morning. If you&#8217;d like to receive it, <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">you can change your plan here</a>.</em></p></blockquote><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h4>The audio for this post:</h4><div class=\"native-audio-embed\" data-component-name=\"AudioPlaceholder\" data-attrs=\"{&quot;label&quot;:null,&quot;mediaUploadId&quot;:&quot;45a22c45-0be1-4eab-9d5e-c7751b8ae38d&quot;,&quot;duration&quot;:1912.8424,&quot;isEditorNode&quot;:true}\"></div><h1><strong>The Meaning Collapse</strong></h1><p>There's a radiologist in Cleveland right now staring at an AI system that reads scans better than she does. She's not worried about her job&#8212;demand for radiologists is actually increasing. But something darker gnaws at her: if a machine can do in seconds what took her a decade to master, what exactly is she?</p><p>This isn't a story about jobs. It's about the collapse of knowledge as the fundamental currency of human worth&#8212;and the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "reorganization of meaning that follows. Knowledge is going to keep growing at break-neck pace, but what about us? That&#8217;s what we all wonder, and I want to lay out a structured way to think about it head-on here.</p><h2><strong>Knowledge hyperinflation</strong></h2><p>We're experiencing the first knowledge hyperinflation in human history. Not the gentle devaluation that came with printing presses or calculators, but a complete collapse of the economic and social value of knowing things. Your MBA, your decades of experience, your painstakingly acquired expertise&#8212;they're Weimar Republic Deutschmarks, worthless before the ink dries on your certificate.</p><p>This isn't new. Knowledge has been inflating for decades&#8212;each generation needing more education for the same economic position their parents held. But AI represents the moment inflation tips into hyperinflation. When anyone can access all human knowledge instantly and synthesize it perfectly, what happens to the social order built on information scarcity? The same thing that happens to any currency when supply goes infinite: complete systemic collapse.</p><p>Andrew Peterson's research at the University of Poitiers names it \"<a href=\"https://hal.science/hal-04534111v1/file/Knowledge_collapse.pdf\">knowledge collapse</a>\"&#8212;when AI systems generate outputs clustered around probability centers, progressively narrowing the spectrum of available knowledge. His models show that discounting AI content by just 20% causes public beliefs to drift 2.3 times further from truth. We're not just devaluing knowledge; we're homogenizing it.</p><h2><strong>Jobs aren&#8217;t skills&#8230;</strong></h2><p>Here's where every automation attempt goes wrong, from Roger Smith's GM to today's AI evangelists: they decompose jobs into discrete skills, see machines master those skills, then assume the job itself can be automated. This reductionist view misses everything that makes human work valuable.</p><p>A lawyer isn't just someone who knows precedents&#8212;that's the skill. The job involves reading a room, sensing when a client is lying, knowing when to push and when to yield, building trust over decades. A radiologist doesn't just read scans&#8212;she translates between machine precision and human fear, catches the one-in-a-thousand case that breaks the pattern, provides the human presence that transforms diagnosis from data to meaning.</p><p>The skills can be automated. The job&#8212;that complex web of judgment, relationships, and context&#8212;we have no realistic map for what automation of that job looks like.</p><p>PwC's 2025 data reveals this starkly:<strong> industries with highest AI exposure show 3x revenue growth and wages rising twice as fast</strong>. Why? Because organizations that understand jobs as more than skill-bundles use AI to amplify human capability rather than replace it. Those who see only skills to automate end up like GM&#8212;with robots painting each other while market share evaporates.</p><h2><strong>GM finds out the hard way</strong></h2><p>Roger Smith spent upwards of $90 billion in the 1980s pursuing the ultimate reductionist dream: decompose car manufacturing into discrete tasks, automate each task, eliminate the workers. The Detroit-Hamtramck Assembly plant opened in 1985 as his monument to this vision.</p><p>The reality proved prophetic for our current moment. Spray-painting robots coated each other instead of cars. Welding robots sealed doors shut. The \"robogate\" systems that were supposed to revolutionize assembly instead created catastrophic bottlenecks whenever they encountered variations outside their programming. GM's market share plummeted from 46% to 35.1% during",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Smith's tenure.</p><p>The bitter lesson came through NUMMI, GM's joint venture with Toyota. Using GM's worst workforce, Toyota achieved world-class quality not through automation but through recognizing that manufacturing jobs involved continuous improvement, problem-solving, and adaptation&#8212;human capabilities that no amount of discrete skill automation could replicate. GM had automated the skills but lost the job.</p><h2><strong>So what sticks if knowledge doesn&#8217;t?</strong></h2><p>In a world of hyperinflated knowledge, what is hard currency? Let me suggest a non-exhaustive list here. I&#8217;ve written about a few of these before, but never nailed them down in a single clean list like this. What would you add?</p><p><strong>Glue Work</strong>: The invisible labor that connects systems, translates between domains, and maintains coherence. The nurse who bridges between AI diagnosis and patient understanding. The project manager who transforms algorithmic outputs into team direction. Undervalued before, essential now.</p><p><strong>Taste</strong>: In infinite possibility, knowing what to build matters more than knowing how. The creative director choosing from AI's million options. The product manager selecting features when anything is technically possible. Taste can't be automated because it's not a skill&#8212;it's accumulated judgment about what matters.</p><p><strong>Extreme Agency</strong>: The ability to operate with minimal direction, maximal ownership. When AI handles execution, humans must excel at goal-setting, priority-defining, and course-correcting. Agency isn't following instructions&#8212;it's knowing what instructions to create.</p><p><strong>Learning Velocity</strong>: Not knowledge accumulation but adaptation speed. The half-life of technical skills has compressed to 2.5 years. Value accrues to those who learn faster than knowledge inflates, who surf the wave of obsolescence rather than drowning in it.</p><p><strong>Intent Horizons</strong>: The capacity to maintain coherent goals across extended timeframes. AI excels at optimizing immediate objectives but lacks the ability to balance competing long-term priorities. Humans provide the narrative coherence that prevents optimization from becoming self-defeating.</p><p><strong>Interruptibility</strong>: The meta-skill of knowing when to stop the machine. Like Toyota's jidoka principle&#8212;automation with human touch&#8212;value concentrates in those who recognize when systems are failing in ways metrics can't capture.</p><p>These aren't skills in the traditional sense. They're ways of being that emerge from the complex intersection of personality, experience, and judgment. They resist the decomposition that makes automation possible.</p><h2><strong>If we don&#8217;t know, what are we?</strong></h2><p>Here&#8217;s the thing: The radiologist's crisis isn't economic&#8212;it's ontological. For centuries, we built identity on accumulating knowledge. \"I am what I know\" was the tacit creed of the professional class. Degrees, certifications, years of experience&#8212;these weren't just economic signals but existential anchors.</p><p>Knowledge workers report increasing anxiety about professional relevance, imposter syndrome when AI outperforms their core competencies, and fundamental uncertainty about career direction. The psychological impact extends beyond individual identity to social structure. When knowledge no longer confers status, what organizing principle replaces it?</p><p>The answer emerging from successful AI adoptions: we shift from \"I am what I know\" to \"I am how I connect, judge, and create meaning from what machines know.\" This is the world of AI agent managers that Jensen Huang laid out in January of this year. The lawyer who saves four hours weekly with AI while maintaining higher accuracy than either AI or lawyers alone isn't replaced&#8212;she's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "amplified. Her value shifts from information storage to wisdom application.</p><h2><strong>The paradox of circuit breakers</strong></h2><p>Financial markets discovered this truth through trillion-dollar near-disasters. Despite algorithms executing 70% of trades, markets depend on human-triggered circuit breakers to prevent catastrophic losses. When the S&amp;P 500 drops 7%, trading halts for 15 minutes&#8212;not for algorithms to recalibrate, but for humans to assess whether the selloff reflects reality or algorithmic panic.</p><p>March 2020 proved the point: circuit breakers triggered four times in a single month, preventing algorithmic feedback loops from destroying market value. The paradox: as trading becomes more automated, human judgment becomes more critical, not less. The humans don't execute trades&#8212;they decide when to stop the machines from trading.</p><p>Tesla's Full Self-Driving is a really interesting test of this hypothesis. Tesla&#8217;s essential bet is they&#8217;ve seen enough edge cases now to be safe on the road. Safer than humans and safe enough to launch Robotaxi. Tesla is probably right that they are safer than people (it&#8217;s a low bar), but we humans are likely to remain very intolerant of robot errors in driving&#8212;we will absolutely use a double standard here based on patterns of previous investigations. And I have no doubt we will continue to be a majority human driving world for a good long while.</p><h2><strong>The great miscalculation</strong></h2><p>Organizations pursuing pure automation make the same error: they see tasks, not systems. They automate skills, not jobs. They replace capabilities, not judgment. The failure rate is predictable and brutal.</p><p>IBM Watson Health, sold for $1 billion after $5 billion investment. Google's diabetic retinopathy system, perfect in labs but rejecting 21% of real-world images due to lighting variations. Amazon's AI recruiting tool, scrapped after systematically discriminating against women. Each failed by automating the measurable while failing to measure what mattered.</p><p>The successes tell the opposite story. Swedish breast cancer screening combining AI with radiologists detected 20% more cancers while reducing workload 44%. Law firms report AI-augmented lawyers generating $100,000 additional billable time annually. Manufacturing technology investment reached $2.81 billion in 2024, focused on collaborative robots working alongside humans rather than replacing them.</p><p>I&#8217;m not here to promise you that organizations won&#8217;t make this screw-up again. They will. Nor am I here to make the Pollyanna-ish claim that no jobs will be lost to AI. Nor am I trying to say that automation won&#8217;t work.</p><p>I&#8217;m making a subtler point: by framing jobs in terms of systems of skills we are extending 20th century managerial philosophy to the nth degree, and we are going to find out (again) that jobs are more complex than we realize. Work that matters is more complex than we realize. And the skill list I&#8217;ve outlined above is a way to start to characterize the world of work beyond the world of knowledge&#8212;a description of all the other stuff we do besides know stuff!</p><h2><strong>The 56% premium</strong></h2><p>What&#8217;s 56%? This: Workers with AI skills command 56% wage premiums&#8212;up from 25% a year ago. The real tell? Zuckerberg is now poaching AI researchers for $10 million (or more) packages, not just to build",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "better models but to figure out how to help Meta win the AI race. The premium isn't for AI expertise itself&#8212;it's for knowing how to bridge AI capabilities to real-world value.</p><p>The World Economic Forum projects 78 million net new jobs by 2030&#8212;170 million created, 92 million eliminated&#8212;concentrated in roles that leverage AI as tool rather than replacement. McKinsey (I know lol) finds 30% of work hours face automation potential, but actual displacement remains far lower as organizations discover the irreplaceable value of human judgment.</p><p>The premium doesn't attach to AI skills themselves&#8212;those commodify quickly and also are somewhat ephemeral as AI systems evolve. It attaches to the meta-skill of knowing how to remain valuable and sticky against problems as skills commodify. It rewards those who understand that in a world of infinite knowledge, the scarcity shifts outside knowledge toward other human capacities that can&#8217;t be caught by knowing more things.</p><h2><strong>The choice that defines the next decade</strong></h2><p>We stand at an inflection point. Not between humans and machines&#8212;I think that's a false binary. But between two visions of human worth:</p><ol><li><p>Desperately trying to out-know machines, accumulating credentials in a hyperinflationary spiral&#8212;trying to compete for the knowledge prize with machines</p></li><li><p>Developing the judgment to know when machines are wrong, rigid, or heading toward catastrophe&#8212;or more deeply, learning to partner with machines</p></li></ol><p>The first path leads to existential crisis and economic irrelevance. The second leads to a new form of human value&#8212;not despite AI's capabilities, but because of them.</p><p>The radiologist in Cleveland faces something more complex than a skills crisis. ChatGPT scores higher on empathy tests than most doctors. AI reads scans more accurately. But work isn't individual tasks&#8212;it's the bundling of tasks with ownership, liability, interruptability, long-term thinking, and meaning-making within a team and between a team and patients. When the AI misses a tumor, who gets sued? When a patient needs someone to blame, who stands there and takes it? </p><p>And it&#8217;s not just negative. Can the AI give the patient a hug? Can the AI ask for a second opinion? Can the AI step over and look over a buddy&#8217;s shoulder? Does the AI tolerate switching to an entirely different patient history within the same chat? </p><p>We can go on and on but more fundamentally: if work is the act of making meaning, and LLMs are what Karpathy calls \"stochastic parrots\"&#8212;spirits that simulate meaning without creating it&#8212;we face unforeseen obstacles to getting actual work done. We don&#8217;t know what it takes to truly automate work! The gap between an LLM's summary of War and Peace and actually reading War and Peace isn't about information transfer. It's about experience, transformation, the irreducible difference between knowing about something and knowing it.</p><p>That's not a job description. It's the difference between simulating human and being human. And maybe that means that the knowledge isn&#8217;t the point.</p><p>The question isn't whether AI will take your job. It's whether you'll discover what humans are actually for before your knowledge becomes worthless. The clock is ticking, and the currency of expertise is collapsing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "faster than any of us are comfortable with, and it&#8217;s demanding that we cultivate a much wider range of skills than most of our educational systems prepared us for. It&#8217;s not easy.</p><p>But maybe&#8212;just maybe&#8212;that's exactly when we discover what being human was always supposed to mean. I know it&#8217;s cheesy but I&#8217;ve seen people go through this reflection stage individually in journeys over the past few years, and I think it&#8217;s a real post-AI human realization moment. Call it post-AI midlife crisis if you like.</p><p>And right on cue, we have the biggest objection to all this:</p><h2><strong>Yes, but what about AGI?</strong></h2><p><em>\"Why prepare for a human-AI hybrid future when AGI will make us all obsolete in a year anyway?\"</em></p><p>Here's the honest answer: that&#8217;s probably not a correctly framed question. </p><p>Annoying I know. But it matters.</p><p>AGI (maybe?) might be achievable with scaled LLM architectures. Intent horizons are doubling every few months&#8212;from 3 hours to 7 to potentially 30. Memory implementations get more sophisticated daily. In a year or two, these systems might maintain coherent goals for weeks or months. Things will get smarter in jagged ways.</p><p>But here's what's becoming clear: as LLMs scale brilliantly on the knowledge dimension, we have no clear picture of how they're scaling on the dimensions that actually matter for getting work done. They're building a ladder to the moon while we need bridges between islands.</p><p>The core issue isn't whether they'll achieve long-term memory&#8212;they probably will. It's whether that memory will have the magical intuitive flexibility that makes human minds special. Current implementations are like lossy JPEGs that sometimes hallucinate what was in the compressed bits: they can retrieve the gist, miss crucial details, and occasionally invent things that were never there. When you need the exact contract clause or the specific drug interaction, \"mostly right\" isn't right.</p><p>I&#8217;m not clear if the direction we&#8217;re going is the correct direction to address some of these fundamental capabilities gaps with humans, and what I&#8217;ve seen so far doesn&#8217;t give me the impression we&#8217;re making very fast progress on some of the human skills I outlined above. (Most humans breathe in relief here lol)</p><p>Another example: LLMs excel at going deep within domains but struggle at the boundaries where real work happens. They can generate brilliant code within a well-defined problem space but miss when the technical challenge has become organizational. They can write perfect legal briefs but not recognize when the legal strategy needs to become a business negotiation. This isn't a bug&#8212;it's the difference between knowledge and judgment.</p><p>Which brings us to what matters: in a world where knowledge can be instantly accessed and credentials can be faked with a ChatGPT account, what constitutes genuine proof of work?</p><p><strong>Really excellent software that actually ships and works.</strong> You can't fake a codebase that handles real users, real scale, real edge cases. The gap between \"demo that impresses in an interview\" and \"system that survives production\" can't be bridged by prompting. It requires the thousand small decisions, trade-offs, and intuitions that come from genuine experience.</p><p><strong>Writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "that changes how people think.</strong> Not just grammatically correct prose or well-structured arguments, but writing that creates new mental models, that makes readers see the world differently. AI can imitate style but can't generate the lived experience and unique perspective that makes writing resonate at a deep level.</p><p><strong>Successful cross-functional projects.</strong> Anything requiring navigation across technical, business, and human domains. The project manager who ships a feature by aligning engineering, design, sales, and legal isn't just coordinating&#8212;they're translating between incompatible worldviews, maintaining coherence across context switches that break AI systems.</p><p><strong>Building and maintaining trust networks.</strong> Reputation that accumulates over years through consistent judgment calls. The venture capitalist whose portfolio succeeds not through any single brilliant insight but through thousands of micro-decisions about people and possibilities. This can't be speedrun or simulated.</p><p><strong>Cultural creation and curation.</strong> The creative director who consistently identifies what will resonate before it's obvious. The editor who develops writers. The A&amp;R person who finds artists before they break. Taste that predicts and shapes culture rather than following it.</p><p><strong>High-stakes decision-making under uncertainty.</strong> The surgeon who recognizes when to deviate from protocol. The pilot who safely lands a damaged plane. The CEO who navigates a crisis. Situations where judgment must integrate incomplete information, competing priorities, and irreversible consequences.</p><p>The pattern? These are all outputs that emerge from the messy intersection of knowledge, experience, judgment, and human connection. They're what remains when pure information processing becomes commoditized.</p><p>Anyway, there's something deeper here. If work is fundamentally about making meaning&#8212;not just processing information but creating significance&#8212;then we face an unexpected obstacle. LLMs are brilliant at simulating meaning, generating text that feels profound, responses that seem empathetic. They're stochastic spirits that can mimic understanding perfectly. But mimicry isn't meaning. The difference between an AI's summary of War and Peace and the experience of reading it isn't about information&#8212;it's about transformation, about being changed by the encounter.</p><p>Work bundles tasks with ownership, liability, and the human act of meaning-making. When AI makes a medical error, who owns it? When a project fails, who takes responsibility? When success happens, who finds meaning in it? These aren't technical problems but existential ones. They can't be solved by better algorithms because they're not about processing&#8212;they're about being.</p><p>We're watching the greatest shift in human value since the industrial revolution. Not because AI will replace humans, but because it forces us to identify what was always most valuable about human work: not the knowledge we store but the connections we make, not the problems we solve but knowing which problems matter, not executing tasks but navigating the undefined spaces between them.</p><p>The timeline question misses the point. Whether AGI arrives in 2025 or 2050, the humans who thrive will be those who understand that in a world of infinite knowledge, value concentrates in judgment, taste, and the ability to navigate discontinuity. The clock isn't ticking on human obsolescence&#8212;it's ticking on our willingness to recognize what makes us irreplaceable.</p><p>Ok and one last question for the road&#8230;</p><h2><strong>What about college?</strong></h2><p>On June 24, 2025&#8212;just days ago&#8212;Monster and CareerBuilder filed",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "for Chapter 11 bankruptcy. These titans of the early internet, who once bought Super Bowl ads to revolutionize job hunting, collapsed under $100-500 million in debt. Their demise isn't just another tech casualty. It's the canary in the coal mine for our entire credentialing system.</p><p>Since Oxford's founding&#8212;teaching existed there by 1096, though the university rapidly developed from 1167&#8212;universities have served as civilization's knowledge gatekeepers. For nearly a millennium, the path was clear: accumulate knowledge, earn credentials, trade them for economic and social position. This system survived the printing press, the industrial revolution, and the internet. It won't survive AI.</p><p>The numbers are staggering, even if contested. Surveys show anywhere from 30% to 89% of college students using ChatGPT for homework, with most estimates hovering around 40% for regular use. Regardless of the exact figure, one educator's observation rings true: this is education's \"Lance Armstrong moment.\" When enough players cheat with high upside and low consequences, others feel forced to cheat to compete. It becomes, in Armstrong's words, \"impossible to win without doping.\"</p><p>But here's the deeper crisis: if knowledge is now free and instant, what exactly are universities selling? Not information&#8212;ChatGPT provides that. Not skills&#8212;YouTube tutorials teach those. Not even critical thinking&#8212;AI can simulate that too. They're selling something increasingly abstract: the <em>idea</em> of credibility in a world where credentials can be faked with a prompt.</p><p>The job market reflects this confusion. Nearly half of companies say they plan to eliminate bachelor's degree requirements, yet paradoxically, 59% of employers say degrees matter MORE than five years ago. We're watching a system in violent transition, unsure whether to double down on traditional credentials or abandon them entirely.</p><p>This isn't just about cheating or job requirements. It's about the collapse of an entire social technology. Degrees served as universal signals&#8212;imperfect but shared fictions that enabled coordination. An MBA from Wharton meant something specific. Ten years at McKinsey conveyed particular competencies. Now these signals are noise. You can fake the knowledge, simulate the skills, even mimic the writing style. What can't be faked?</p><p>The answer emerging from forward-thinking educators and employers: proof of work that demonstrates judgment, not just knowledge. Ship code that handles real users. Write something that changes how people think. Navigate complex projects across domains. Build trust networks over years. Create culture rather than consume it. Make high-stakes decisions where failure has real consequences. Yes there&#8217;s an on-ramp here, but the ideas are there to change how we show value and that&#8217;s important.</p><p>These outputs resist AI assistance not because they're technically difficult but because they require ownership, liability, and the human act of meaning-making. When an AI-written essay fails to persuade, who takes responsibility? When generated code crashes in production, who fixes it at 3am? When a decision goes wrong, who stands before the board?</p><p>Universities that survive will transform from knowledge-delivery systems to judgment-development institutions. They'll teach not what to think but how to think when infinite information is available. They'll credential not information retention but the ability to navigate discontinuity, own outcomes, and create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "meaning from noise.</p><p>The students cheating with ChatGPT aren't lazy&#8212;they're rational actors in an irrational system (I think <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">Roy and Cluely</a> agree with me on this one thing lmao). They're using 21st-century tools to game 19th-century assessments for 11th-century credentials. The real scandal isn't that they're cheating. It's that we're still pretending the old game matters.</p><p>Monster's bankruptcy filing listed the cause as a \"challenging and uncertain macroeconomic environment.\" But that's corporate speak for a simpler truth: when knowledge becomes worthless, the infrastructure of knowledge-trading collapses too. First the job boards. Next, perhaps, the universities that feed them.</p><p>Unless they remember what they're actually for!</p><p>Good luck out there. Stay Curious. Stay human.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!_LYt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg\" width=\"1456\" height=\"971\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2037707,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166955816?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!_LYt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 424w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 848w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!_LYt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6738d7-373c-470b-8224-a2be98db391f_1536x1024.jpeg 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h2><strong>Sources</strong></h2><h3><strong>Monster/CareerBuilder Bankruptcy Sources</strong></h3><ul><li><p><a href=\"https://www.newsx.com/business/monster-and-careerbuilder-file-for-bankruptcy-begin-asset-sales-amid-market-shift-9215/\">NewsX &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2025/06/24/careerbuilder-monster-bankruptcy/\">Washington Post &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.cnn.com/2025/06/25/business/monster-careerbuilder-bankruptcy\">CNN &#8211; Monster and CareerBuilder File for Bankruptcy</a></p></li><li><p><a href=\"https://www.staffingindustry.com/news/global-daily-news/careerbuilder-monster-selling-businesses-filing-for-bankruptcy\">Staffing Industry Analysts &#8211; CareerBuilder, Monster Selling Businesses</a></p></li><li><p><a href=\"https://www.reuters.com/legal/litigation/careerbuilder-monster-which-once-dominated-online-job-boards-file-bankruptcy-2025-06-24/\">Reuters &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://www.abc27.com/news/consumer/careerbuilder-monster-com-enter-chapter-11-bankruptcy/\">ABC27 &#8211; CareerBuilder and Monster Enter Chapter 11</a></p></li><li><p><a href=\"https://www.foxbusiness.com/economy/online-job-listing-company-careerbuilding-monster-files-bankruptcy\">Fox Business &#8211; CareerBuilder, Monster File for Bankruptcy</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Monster.com\">Wikipedia &#8211; Monster.com</a></p></li><li><p><a href=\"https://www.hrdive.com/news/careerbuilder-monster-files-chapter-11-bankruptcy/751498/\">HR Dive &#8211; CareerBuilder, Monster File Chapter 11</a></p></li><li><p><a href=\"https://www.northbaybusinessjournal.com/article/industrynews/careerbuilder-monster-bankruptcy-recruitment-workforce/\">North Bay Business Journal &#8211; CareerBuilder, Monster Bankruptcy</a></p></li></ul><div><hr></div><h3><strong>AI/ChatGPT in Education Sources</strong></h3><ul><li><p><a href=\"https://www.axios.com/2025/05/26/ai-chatgpt-cheating-college-teachers\">Axios &#8211; Teachers Worry About AI and Cheating</a></p></li><li><p><a href=\"https://slate.com/life/2025/05/college-student-cheating-ai-detector-chatgpt-school-education.html\">Slate &#8211; College Student Cheating and AI Detectors</a></p></li><li><p><a href=\"https://longreads.com/2025/05/21/chatgpt-ai-college-cheating/\">Longreads &#8211; ChatGPT and the Cheating Crisis</a></p></li><li><p><a href=\"https://nerdynav.com/chatgpt-cheating-statistics/\">NerdyNav &#8211; ChatGPT Cheating Statistics</a></p></li><li><p><a href=\"https://ed.stanford.edu/news/what-do-ai-chatbots-really-mean-students-and-cheating\">Stanford Graduate School of Education &#8211; AI Chatbots and Cheating</a></p></li><li><p><a href=\"https://www.edweek.org/technology/opinion-the-ai-cheating-crisis-education-needs-its-anti-doping-movement/2024/02\">Education Week &#8211; The AI Cheating Crisis</a></p></li><li><p><a href=\"https://apnews.com/article/chatgpt-cheating-ai-college-1b654b44de2d0dfa4e50bf0186137fc1\">Associated Press &#8211; AI and College Cheating</a></p></li><li><p><a href=\"https://www.sciencedirect.com/science/article/pii/S2666920X24000560\">ScienceDirect &#8211; Academic Integrity and AI</a></p></li><li><p><a href=\"https://www.technologyreview.com/2023/04/06/1071059/chatgpt-change-not-destroy-education-openai\">MIT Technology Review &#8211; ChatGPT Will Change Education</a></p></li><li><p><a href=\"https://slate.com/technology/2023/02/chat-gpt-cheating-college-ai-detection.html\">Slate (2023) &#8211; Early Warning Signs of AI Cheating</a></p></li></ul><div><hr></div><h3><strong>Employment/Hiring Trends Sources</strong></h3><ul><li><p><a href=\"https://www.naceweb.org/job-market/trends-and-predictions/hiring-projections-level-off-for-the-college-class-of-2025\">NACE &#8211; Hiring Projections for Class of 2025</a></p></li><li><p><a href=\"https://www.hiringlab.org/2024/12/10/indeed-2025-us-jobs-and-hiring-trends-report/\">Indeed Hiring Lab &#8211; 2025 U.S. Jobs and Hiring Trends</a></p></li><li><p><a href=\"https://www.survivalworld.com/economics/20-college-degrees-employers-dont-want-in-2025/\">Survival World &#8211; Degrees Employers Don&#8217;t Want in 2025</a></p></li><li><p><a href=\"https://www.highereddive.com/news/nearly-half-of-companies-plan-to-eliminate-bachelors-degree-requirements/702277/\">Higher Ed Dive &#8211; Companies Dropping Bachelor&#8217;s Requirements</a></p></li><li><p><a href=\"https://time.com/7291844/job-market-college-graduates-unemployment/\">TIME &#8211; Job Market for College Graduates</a></p></li><li><p><a href=\"https://www.lanereport.com/174929/2024/07/75-of-employers-say-removing-this-requirement-for-job-applicants-has-improved-their-company/\">Lane Report &#8211; Removing Degree Requirements Improves Hiring</a></p></li><li><p><a href=\"https://cew.georgetown.edu/cew-reports/projections2031/\">Georgetown CEW &#8211; Projections 2031 Report</a></p></li><li><p><a href=\"https://www.aplu.org/our-work/4-policy-and-advocacy/publicuvalues/employment-earnings/\">Association of Public and Land-grant Universities &#8211; Employment and Earnings</a></p></li><li><p><a href=\"https://www.bls.gov/news.release/pdf/empsit.pdf\">Bureau of Labor Statistics &#8211; Employment Situation Report (PDF)</a></p></li><li><p><a href=\"https://www.testgorilla.com/skills-based-hiring/state-of-skills-based-hiring-2024/\">TestGorilla &#8211; State of",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "Skills-Based Hiring 2024</a></p></li></ul><h3><strong>Contemporary AI Failures (McDonald&#8217;s, Tesla, etc.)</strong></h3><ul><li><p><a href=\"https://www.restaurantbusinessonline.com/technology/mcdonalds-ending-its-drive-thru-ai-test\">Restaurant Business Online &#8211; McDonald&#8217;s Ending Its Drive-Thru AI Test</a></p></li><li><p><a href=\"https://apnews.com/article/mcdonalds-ai-drive-thru-ibm-bebc898363f2d550e1a0cd3c682fa234\">AP News &#8211; McDonald&#8217;s Ends AI Drive-Thru Partnership with IBM</a></p></li><li><p><a href=\"https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html\">CIO &#8211; 5 Famous Analytics and AI Disasters</a></p></li><li><p><a href=\"https://www.cnbc.com/2024/10/18/tesla-faces-nhtsa-investigation-of-full-self-driving-after-fatal-collision.html\">CNBC &#8211; Tesla Faces NHTSA Investigation After Fatal Collision</a></p></li><li><p><a href=\"https://www.tesla.com/VehicleSafetyReport\">Tesla &#8211; Vehicle Safety Report</a></p></li><li><p><a href=\"https://www.washingtonpost.com/business/2024/10/18/tesla-full-self-driving-nhtsa-fsd/\">Washington Post &#8211; NHTSA Investigates Tesla&#8217;s Full-Self Driving System</a></p></li><li><p><a href=\"https://abcnews.go.com/Business/tesla-driving-crash-reports-prompt-nhtsa-investigation/story?id=114922283\">ABC News &#8211; Tesla Crash Reports Prompt NHTSA Investigation</a></p></li><li><p><a href=\"https://www.thetradenews.com/human-judgement-still-king-in-a-world-of-algorithmic-trades/\">The TRADE &#8211; Human Judgment Still King in Algorithmic Trades</a></p></li><li><p><a href=\"https://oatmealhealth.com/why-has-ai-failed-so-far-in-healthcare-despite-billions-of-investment/\">Oatmeal Health &#8211; Why AI Has Failed in Healthcare (So Far)</a></p></li></ul><div><hr></div><h3><strong>Knowledge Hyperinflation / Knowledge Collapse</strong></h3><ul><li><p><a href=\"https://arxiv.org/abs/2404.03502\">arXiv &#8211; Collapse of Knowledge</a></p></li><li><p><a href=\"https://www.emergentmind.com/papers/2404.03502\">Emergent Mind &#8211; Summary of arXiv 2404.03502</a></p></li><li><p><a href=\"https://www.clio.com/blog/tools-for-lawyers/\">Clio &#8211; Best Tools for Lawyers</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/issues/artificial-intelligence/ai-jobs-barometer.html\">PwC &#8211; AI Jobs Barometer (Global)</a></p></li><li><p><a href=\"https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html\">PwC &#8211; Productivity Growth and AI</a></p></li><li><p><a href=\"https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-jobs-barometer.html\">PwC US &#8211; AI Jobs Barometer</a></p></li></ul><div><hr></div><h3><strong>MIT / Economics Research on AI</strong></h3><ul><li><p><a href=\"https://economics.mit.edu/news/daron-acemoglu-what-do-we-know-about-economics-ai\">MIT Economics &#8211; Daron Acemoglu on AI and Labor</a></p></li><li><p><a href=\"https://www.imf.org/en/Publications/fandd/issues/2023/12/Rebalancing-AI-Acemoglu-Johnson\">IMF &#8211; Rebalancing AI by Acemoglu and Johnson</a></p></li><li><p><a href=\"https://ssir.org/articles/entry/ai-impact-on-jobs-and-work\">Stanford Social Innovation Review &#8211; AI Impact on Jobs and Work</a></p></li></ul><div><hr></div><h3><strong>GM / Roger Smith Sources</strong></h3><ul><li><p><a href=\"https://en.wikipedia.org/wiki/Roger_Smith_(executive)\">Wikipedia &#8211; Roger Smith (Executive)</a></p></li><li><p><a href=\"https://www.imdb.com/name/nm0809792/bio/\">IMDb &#8211; Roger Smith Bio</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Detroit/Hamtramck_Assembly\">Wikipedia &#8211; Detroit/Hamtramck Assembly</a></p></li><li><p><a href=\"https://www.leanblog.org/2016/06/gms-ceo-roger-smith-thought-toyota-had-magic-but-this-was-the-secret/\">Lean Blog &#8211; What GM Misunderstood About Toyota</a></p></li><li><p><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Scribd &#8211; </a><em><a href=\"https://www.scribd.com/book/224735835/Comeback-The-Fall-Rise-of-the-American-Automobile-Industry\">Comeback: The Fall and Rise of the American Automobile Industry</a></em></p></li><li><p><a href=\"https://www.wrike.com/blog/key-to-perfect-automation-is-imperfect-people/\">Wrike &#8211; The Key to Perfect Automation is Imperfect People</a></p></li><li><p><a href=\"https://www.washingtonpost.com/archive/business/1990/08/01/roger-smith-gm-driven-to-regain-market-share/530dfe27-35e9-4d0e-a7f8-3a0f659d7a00/\">Washington Post (1990) &#8211; Roger Smith at GM</a></p></li><li><p><a href=\"https://archive.seattletimes.com/archive/19900730/1085108/roger-smith-leaves-his-mark-at-gm\">Seattle Times Archive &#8211; Roger Smith Leaves His Mark</a></p></li><li><p><a href=\"https://en.wikipedia.org/wiki/NUMMI\">Wikipedia &#8211; NUMMI</a></p></li><li><p><a href=\"https://www.popularmechanics.com/cars/a5514/4350856/\">Popular Mechanics &#8211; A Look Back at GM and Toyota</a></p></li></ul><div><hr></div><h3><strong>Skills Half-Life / Professional Identity</strong></h3><ul><li><p><a href=\"https://www.weforum.org/stories/2024/01/this-is-the-one-skill-everybody-needs-in-the-age-of-ai/\">World Economic Forum &#8211; One Skill Everyone Needs in the Age of AI</a></p></li><li><p><a href=\"https://www.cio.com/article/219940/thriving-in-a-world-of-knowledge-half-life.html\">CIO &#8211; Thriving in a World of Shrinking Knowledge Half-Life</a></p></li><li><p><a href=\"https://www.linkedin.com/pulse/shrinking-half-life-skills-peter-smulovics\">LinkedIn &#8211; Shrinking Half-Life of Skills</a></p></li><li><p><a href=\"https://www.ibm.com/blogs/ibm-training/skills-transformation-2021-workplace/\">IBM &#8211; Skills Transformation in the Workplace</a></p></li><li><p><a href=\"https://www.verywellmind.com/what-is-an-identity-crisis-2795948\">Verywell Mind &#8211; What Is an Identity Crisis?</a></p></li><li><p><a href=\"https://seo.ai/blog/ai-replacing-jobs-statistics\">SEO.ai &#8211; AI Job Replacement Statistics</a></p></li></ul><div><hr></div><h3><strong>Financial Markets / Circuit Breakers</strong></h3><ul><li><p><a href=\"https://www.investopedia.com/articles/markets/012716/four-big-risks-algorithmic-highfrequency-trading.asp\">Investopedia &#8211; Four Big Risks in Algorithmic Trading</a></p></li><li><p><a href=\"https://www.investopedia.com/terms/c/circuitbreaker.asp\">Investopedia &#8211; Circuit Breaker Definition</a></p></li></ul><div><hr></div><h3><strong>Legal / Medical AI Adoption</strong></h3><ul><li><p><a href=\"https://www.clio.com/blog/will-ai-replace-paralegals/\">Clio &#8211; Will AI Replace Paralegals?</a></p></li><li><p><a href=\"https://apnews.com/article/ai-algorithms-chatgpt-doctors-radiologists-3bc95db51a41469c390b0f1f48c7dd4e\">AP News &#8211; ChatGPT, Doctors, and AI Algorithms</a></p></li></ul><div><hr></div><h3><strong>Amazon AI Recruiting Failure</strong></h3><ul><li><p><a href=\"https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women\">Euronews &#8211; Amazon Scraps Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.cut-the-saas.com/ai/case-study-how-amazons-ai-recruiting-tool-learnt-gender-bias\">Cut the SaaS &#8211; Case Study on Amazon&#8217;s Gender Bias in AI</a></p></li><li><p><a href=\"https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/\">Reuters &#8211; Amazon&#8217;s Biased AI Recruiting Tool</a></p></li><li><p><a href=\"https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html\">CMU &#8211; Amazon Scraps Biased AI Hiring Engine</a></p></li><li><p><a href=\"https://www.aboutamazon.com/news/workplace/how-amazon-leverages-ai-and-ml-to-enhance-the-hiring-experience-for-candidates\">About Amazon &#8211; Enhancing Hiring with AI and ML</a></p></li><li><p><a href=\"https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/\">Reuters &#8211; Tesla FSD Investigation (duplicate to Tesla)</a></p></li></ul><div><hr></div><h3><strong>AI Scaling and AGI Research</strong></h3><ul><li><p><a href=\"https://www.geeky-gadgets.com/infinite-memory-ai-models/\">Geeky Gadgets &#8211; Infinite Memory AI Models</a></p></li><li><p><a href=\"https://openai.com/index/gpt-4-1/\">OpenAI &#8211; GPT-4.1 Overview</a></p></li><li><p><a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-gpt-41-models-with-improved-coding-long-context-2025-04-14/\">Reuters &#8211; OpenAI Launches GPT-4.1</a></p></li><li><p><a href=\"https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/\">Google Developers Blog &#8211; Gemini API Updates</a></p></li><li><p><a href=\"https://gemini.google/overview/long-context/?hl=en\">Gemini &#8211; Long Context Capabilities</a></p></li><li><p><a href=\"https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it\">Google Cloud Blog &#8211; Gemini Pro and the Needle-in-the-Haystack Test</a></p></li><li><p><a href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\">Google Blog &#8211; Gemini AI Update (Dec 2024)</a></p></li><li><p><a href=\"https://web.swipeinsight.app/posts/claude-unveils-projects-for-ai-workspaces-with-500-page-memory-7902\">Swipe Insight &#8211; Claude&#8217;s 500-Page Memory</a></p></li><li><p><a href=\"https://www.anthropic.com/news/claude-3-family\">Anthropic &#8211; Claude 3 Model Family</a></p></li><li><p><a href=\"https://www.anthropic.com/news/activating-asl3-protections\">Anthropic &#8211; Activating ASL3 Protections</a></p></li><li><p><a href=\"https://fortune.com/2025/05/22/anthropic-new-models-ai-openai-google/\">Fortune &#8211; Anthropic&#8217;s New AI Models</a></p></li><li><p><a href=\"https://arxiv.org/html/2410.03156v1\">arXiv &#8211; Scaling Laws (2410.03156)</a></p></li><li><p><a href=\"https://openreview.net/forum?id=TvGPP8i18S\">OpenReview &#8211; Scaling Trends in AGI</a></p></li></ul><div><hr></div><h3><strong>AGI Timeline Predictions</strong></h3><ul><li><p><a href=\"https://venturebeat.com/ai/openai-begins-2025-with-massive-hype-for-agi-superintelligence/\">VentureBeat &#8211; 2025 AGI and Superintelligence Hype</a></p></li><li><p><a href=\"https://time.com/7205596/sam-altman-superintelligence-agi/\">TIME &#8211; Sam Altman on Superintelligence</a></p></li><li><p><a href=\"https://felloai.com/2024/11/dario-amodei-ceo-of-anthropic-artificial-general-intelligence-is-coming-in-2027/\">Fello AI &#8211; Dario Amodei Predicts AGI by 2027</a></p></li><li><p><a href=\"https://lexfridman.com/dario-amodei-transcript/\">Lex Fridman &#8211; Dario Amodei Transcript</a></p></li><li><p><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">EDRM",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/what-good-is-a-college-degree-when",
    "embedding": [],
    "text": "&#8211; Amodei&#8217;s Essay on </a><em><a href=\"https://edrm.net/2024/10/dario-amodeis-essay-on-ai-machines-of-loving-grace-is-like-a-breath-of-fresh-air/\">Machines of Loving Grace</a></em></p></li><li><p><a href=\"https://www.cnbc.com/2025/03/17/human-level-ai-will-be-here-in-5-to-10-years-deepmind-ceo-says.html\">CNBC &#8211; DeepMind CEO Predicts Human-Level AI</a></p></li><li><p><a href=\"https://www.startuphub.ai/ai-news/artificial-intelligence/2025/agis-coming-in-5-to-10-years-says-deepmind-ceo-demis-hassabis/\">StartupHub.ai &#8211; AGI in 5&#8211;10 Years, Says Demis Hassabis</a></p></li></ul>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "Ready for ChatGPT-5: Grab a Complete 139 Page Prompting Guide That's a Complete Operating System for Life and Work",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "I told myself I wouldn't do this again. After my 66-page prompt opus in April, I swore I'd keep things shorter. More digestible. Reader-friendly. [Narrator voice: He did not keep things shorter becaus...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "<p><em><strong>I told myself I wouldn't do this again.</strong> After my <a href=\"https://natesnewsletter.substack.com/p/my-prompt-stack-for-work-16-prompts?r=1z4sm5\">66-page prompt opus in April</a>, I swore I'd keep things shorter. More digestible. Reader-friendly. </em></p><p><em>[Narrator voice: He did not keep things shorter because there&#8217;s too much good stuff.]</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!aTYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif\" width=\"498\" height=\"498\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:498,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2081498,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166864451?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!aTYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 424w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 848w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1272w, https://substackcdn.com/image/fetch/$s_!aTYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5617bb81-09fe-4acf-9d30-23e26402f16c_498x498.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>This collection is ~145 pages long. It contains 39 essential prompts in a detailed Google Doc, many with multiple variants, plus 6 pages of context that explains why prompting has fundamentally changed in 2025.</strong> And yes, there's a table of contents in that Google Doc so you can actually navigate this beast and find exactly what you need.</em></p><p><em>Because my longing to suffer, why did I write another novel-length prompt guide? Because something clicked for me recently. <strong>I realized my prompts and how I talk about them needed to evolve to keep pace with frontier model capabilities. And with ChatGPT-5 on the horizon (rumored for July), I wanted to bring the prompt stack up to snuff with what today&#8217;s models can do.</strong> </em></p><p><em>These aren't random prompts I think might be useful. <strong>These are the 39 prompts I have labored over, technically, repeatedly, as many different ways as I can think of to make them excellent.</strong></em></p><p><em>How? </em></p><ul><li><p><em>By checking them vs. actual published prompting guides (I list 18 of them below) to ensure they adhere to best practices (lots of these have been published since April)</em></p></li><li><p><em>By developing multiple variants to give you different options depending on your level of effort</em></p></li><li><p><em>By choosing a range of prompts that cover the full spectrum of decision-making I see actually cropping up in work (and life)</em></p></li></ul><p><em>That last is key: I find a lot of my earlier prompt work has been a bit constrained by earlier model intelligence levels. I&#8217;ve felt like I needed to constrain to particular job families in the past because earlier models leaned into fairly defined work and task completion assignments. And that has value!</em></p><p><em><strong>But we can do more cool stuff with the newer models. There&#8217;s a chance with these newer models to ask much more ambiguous questions.</strong></em></p><p><em><strong>Like: </strong>When I need to make a decision that will affect the next five years. When I'm staring at feedback that stings and need to figure out what's actually useful in it. When a project is falling apart and I need structured thinking,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "not panic. When I'm trying to learn something new and don't want to waste months on bad approaches.</em></p><p><em>I've been living with these frontier models&#8212;Claude 4, o3 Pro, Gemini 2.5 Pro&#8212;like they're colleagues. Maybe closer than colleagues. <strong>We've developed a working relationship where I know exactly how to activate their best thinking, and they know (through my prompts) exactly what kind of thought partnership I need.</strong></em></p><p><em>Each prompt in this collection does something specific:</em></p><ul><li><p><em><strong>Turn vague anxiety into clear action</strong> (Future Regret Minimizer, Change Readiness Evaluator)</em></p></li><li><p><em><strong>Extract signal from noise</strong> (Dynamic Qualitative Insight Explorer, Strategic Feedback Interpreter)</em></p></li><li><p><em><strong>Force hard choices</strong> (Comprehensive Tradeoff Analyzer, Meeting Killer)</em></p></li><li><p><em><strong>Design better systems</strong> (Database Schema Designer, Automation Opportunity Scanner)</em></p></li><li><p><em><strong>Navigate complex human dynamics</strong> (Stakeholder Navigation Guide, Multi-Perspective Simulator)</em></p></li></ul><p><em>But here's what matters more: <strong>these prompts are really complete thinking systems, not just questions.</strong> They include context setup, phase-by-phase workflows, specific output formats, and iteration loops. They assume the AI is genuinely intelligent and just needs clear structure to be helpful. And that&#8217;s definitely a marker of the time we&#8217;re in&#8212;the blurry, <a href=\"https://www.google.com/search?q=gentle+singularity&amp;oq=gentle+singularity&amp;sourceid=chrome&amp;ie=UTF-8\">gentle singularity</a>.</em></p><p><em><strong>The Google Doc is designed to be scannable and searchable.</strong> Eight major sections. Clear numbering. Purpose statements for each prompt so you know when to use it. You can bookmark it and come back whenever you face that type of challenge. Think of it as your emergency toolkit for complexity.</em></p><h4><em>Here&#8217;s what it looks like:</em></h4><div class=\"native-video-embed\" data-component-name=\"VideoPlaceholder\" data-attrs=\"{&quot;mediaUploadId&quot;:&quot;f31418d2-fca0-4769-8147-275332f2cf0f&quot;,&quot;duration&quot;:null}\"></div><p><em>I've also included something new: detailed notes on how prompting itself has evolved. How specificity works to drive model outputs. How to use the new massive context windows strategically. Why breaking complex tasks into phases isn't about helping the AI anymore&#8212;it's about helping us.</em></p><p><em><strong>These prompts work with any frontier model</strong>&#8212;Claude 4, o3 Pro, o3, Gemini 2.5 Pro, Grok 3. And I&#8217;ve designed them to be built with the grain of the emerging intelligence patterns we&#8217;re seeing, <strong>so they should set you up for GPT-5 later in the summer</strong>. <strong>Yes I said it.</strong> </em></p><p><em><strong>How??</strong> I&#8217;ve cross-referenced these prompts across the different model makers&#8217; prompting guides to make sure they can be used as-is as much as possible. The principles hold because they're based on the commonalities in how these models actually process information, not on platform-specific tricks.</em></p><p><em>Fair warning: this isn't light reading. <strong>Each prompt is densely packed with structure and logic.</strong> But that's the point. When you need to think clearly about something that matters, you don't want a vague suggestion. You want a systematic process that reliably produces insight. And the point is that now you can just take these prompts and be off to the races.</em></p><p><em>If you've been following my prompt journey for the last few months, this is where it's led. If you're new, this is everything I've learned compressed into immediately usable tools. Either way, <strong>these 39 prompts will change how you work with AI&#8212;from occasional assistant to genuine thinking partner.</strong></em></p><p><em>Time to dig in. The prompts are waiting.</em></p><p><em>PS. For those wanting both the full podcast from the video and a nice voice reading the whole post&#8212;I got you covered. Separate audio",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete",
    "embedding": [],
    "text": "for the post just below!</em></p><p></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ready-for-chatgpt-5-grab-a-complete\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "2 Posts in One: Meta's AI Strategy Looks Desperate + Your Invite to Nate's New AI Discord Community",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "I got lots of feedback on my post from Sunday about a community for paid subscribers.TLDR y&#8217;all want one, and you want it to be on Discord. So I made one! We&#8217;ve got a couple of dozen brave...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy",
    "embedding": [],
    "text": "<p>I got lots of feedback on my <a href=\"https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates?r=1z4sm5\">post from Sunday</a> about a community for paid subscribers.</p><p><strong>TLDR </strong>y&#8217;all want one, and you want it to be on Discord. </p><p>So I made one! </p><p>We&#8217;ve got a couple of dozen brave alpha testers already in there and I&#8217;ll throw the link in below here. Hop on in! We have channels for Substack discussion, AI news, AI questions, and I&#8217;m gonna get an AI jobs channel going here soon as well.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!UuK7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif\" width=\"480\" height=\"360\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:900362,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166843413?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!UuK7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 424w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 848w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1272w, https://substackcdn.com/image/fetch/$s_!UuK7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ed8d77-4b33-4e0c-b9b3-085aef9c6378_480x360.gif 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Also have had a fair bit of interest from founding members on my weekly CEO-view posts. A bit more about them:</p><blockquote><p><em>I'm building <strong>short, sharp weekly intelligence briefs for leaders making real decisions with real money.</strong> <br><br>Think 3-4 pages max, packed with insights that usually cost too much from consultants&#8212;AI procurement pitfalls, hidden implementation costs blindsiding CFOs, liability issues, which investment patterns actually correlate with ROI. <br><br>The goal is clear, frank discussion of the issue, specific actionable paths forward, and hard-headed analysis you can take to the bank.</em></p></blockquote><p>You can change <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">your Substack plan here</a> if you&#8217;re like to get the first one (coming Sunday). Substack will prorate you for previous months etc. so if you&#8217;re been subbing for awhile you won&#8217;t be in at full price. </p><blockquote><p><em>Hint: the first one is on a gnarly compliance issue with board liability implications that&#8217;s facing just about every AI startup right now and that kicks in at the beginning of August. Fun times!</em></p></blockquote><p>Besides the link underneath, I&#8217;m throwing a juicy tidbit for fun on whatever the heck Meta is doing right now with those wild acquisition offers (Zuck my DMs are open lmao). Yes this post is pretty blunt about what Zuck is doing so buckle up!</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all the good stuff, and now a community lol</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/2-posts-in-one-metas-ai-strategy\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "The Claude Code Complete Guide: Learn Vibe-Coding & Agentic AI",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "I got Claude Code backwards when I first read about it. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "<p><em><strong>I got Claude Code backwards when I first read about it</strong>. Actually, I'm convinced most of us do. Anthropic has leaned pretty heavily on the &#8220;code&#8221; part of that branding, but that&#8217;s not what stood out to me after trying it and digging in.</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png\" width=\"1456\" height=\"790\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:233937,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166772125?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ZAWT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 424w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 848w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!ZAWT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d2d68e8-8955-4a9b-9cb3-4dfaf2c2e3fa_2182x1184.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em><strong>It&#8217;s not just for code guys lol</strong></em></p><p><em>After spending time with Claude Code, I'm convinced <strong>what we have here is effectively a general purpose AI agent</strong> hiding under the guise of just being a coding agent. It's not just a coding agent. <strong>Claude Code is capable of the full spectrum of intelligence. It just happens to hide in the terminal</strong>, and that makes it seem scary to people who don't use terminals to code. And let's be honest, that's most of us.</em></p><p><em>What I find really fascinating is that when I started to finally use Claude Code, <strong>it made the decision to upgrade to the Max tier so much easier. </strong>I had enough experience with the intelligence that Claude Code was bringing that it felt intuitive. And here's what really blew my mind: <strong>with Claude Code, you don't have a traditional development environment</strong>. Like, I thought that would be a mistake at first. Who wants to code without the IDE? This is not a video game.</em></p><p><em>But Claude will edit files, create files, but won't necessarily show you all of it the way it does in a typical development environment. You might think, <strong>what a terrible design choice</strong>. And you and I would both be wrong about that. <strong>It turns out that abstracting you above code level helps you focus on the strategy and the intent of the project.</strong></em></p><p><em>And Claude Code has the muscle to actually operate at both levels: strategic and execution. <strong>I strongly suspect this power comes from the fact that there aren't the same token constraints you'd have if you installed Claude in another tool like Cursor</strong>. Because Anthropic can control the whole experience, they can make Claude Code work exactly the way they want. It feels a lot like an internal development tool that got out into the wild, which is exactly what it is.</em></p><p><em>Look, I'm not a fantastically experienced senior engineer. My background is different. I am a hacky scrappy founder, producty kind of person. <strong>I am writing",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "this guide for everyone, not just engineers.</strong> I know enough about coding and product / project management to see that Claude Code is so much more than a coding tool, and we misunderstand it when we think about it just as a coding tool.</em></p><p><em>When I was working on my personal website&#8212;which, for those of you who have been there, I freely agree is terrible and awful lmao&#8212;Claude Code legitimately transformed the experience. Just much less pain. It was so much easier to get Claude Code to work on this project than in any other environment I'd played with. I tried Windsurf, played with it in Cursor a little bit, played with it in o3 (which was the worst example). But Claude Code was different. <strong>It answered my questions intelligently, laid out a plan I could understand, and when it decided to build, it built largely correctly from the start.</strong></em></p><p><em><strong>This is the first time using Claude Code that I've been able to actually get a polished, professional, not mid-looking AI output.</strong> And I did it with an unusual workflow&#8212;80% Claude Code, then a mixture of o3 for color research and Cluely for UI feedback (I&#8217;ll dig into it below). The magic was treating Claude Code not as a coding tool, but as a general purpose agent that happens to live in your terminal.</em></p><p><em>If you're not an engineer, don't let the terminal scare you. Think about it as effectively a chatbot that can talk to the files on your computer. That's really it.</em></p><p><em>And that's exactly what this guide is about. Over the next few thousand words, I'm going to show you everything I've learned about Claude Code&#8212;from that initial $100/month subscription decision that made me grimace, to building complete applications in conversation, to discovering workflows that multiply productivity by 10x or more.</em></p><p><em>You'll learn the 5-minute setup that changes everything, learn a bit more about the art of vibe coding (<a href=\"https://natesnewsletter.substack.com/p/the-vibe-coding-bible-how-to-build\">complete guide here</a>), understand why Claude Code is fundamentally different from GitHub Copilot or Cursor, and see real examples of how teams are achieving 5x productivity gains. </em></p><p><em>I'll also share my interesting three-tool orchestra approach that finally broke through AI's \"mid\" design problem, explain some of the dark side (Claude Code does add up in cost if you use it a lot), and show you some advanced patterns that can help turn Claude Code into your universal terminal for all knowledge work.</em></p><p><em>Why? All this matters because we're at an inflection point. As Kent Beck said after 52 years of coding, <a href=\"https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent\">90% of traditional programming skills are becoming commoditized</a> <strong>while the remaining 10% becomes worth 1000x more</strong>. The developers and teams who understand this shift&#8212;who learn to orchestrate AI rather than just code alongside it&#8212;will thrive in this new landscape. And Claude Code gives us a chance to demonstrate that remaining 10% of skills with a relatively strong junior coding partner.</em></p><p><em>Whether you're a seasoned developer looking to level up your AI pair programming, a product manager wanting to prototype without engineering bottlenecks,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn",
    "embedding": [],
    "text": "or someone who's never coded but has ideas to build, this guide will be a helpful reference that shapes how you approach using AI agents for development. At the end of the day, Claude Code isn't just another coding assistant. It's the beginning of <strong>a new era where the terminal becomes a conversational interface for turning ideas into reality</strong>.</em></p><p><em>Ready to see what's actually possible? Let's dive in&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-claude-code-complete-guide-learn\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "The Anthropic Ruling: A Roadmap for AI's Copyright Future",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "Hot off the presses! Had to get this one out today because the ruling is such a big deal.Hope you enjoy, and back to our regular programming soon&#8230;Subscribers get all these pieces!I've been follo...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "<p><em>Hot off the presses! Had to get this one out today because the ruling is such a big deal.</em></p><p><em>Hope you enjoy, and back to our regular programming soon&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>I've been following AI copyright cases closely, and Judge William Alsup just handed down what I believe will be one of THE landmark AI decisions we'll see this decade. The federal court's split ruling in <em>Bartz v. Anthropic</em> does something remarkable: it validates AI training as fair use while simultaneously condemning the piracy that often enables it. This isn't just a win or loss for Anthropic&#8212;it's a blueprint for how courts will likely approach the dozens of AI copyright cases working their way through the system.</p><h2>The Solomon's Choice of AI Copyright</h2><p>I find Alsup's decision fascinating because it splits the baby with surgical precision. Yes, training Claude on millions of books constitutes fair use. No, downloading those same books from pirate sites doesn't get a free pass. The distinction matters because it fundamentally reshapes how AI companies must think about data acquisition.</p><p>The judge's reasoning on fair use particularly struck me. He describes AI training as \"quintessentially transformative,\" comparing it to how human writers learn from reading. \"Everyone reads texts, too, then writes new texts,\" Alsup writes. \"To make anyone pay specifically for the use of a book each time they read it, each time they recall it from memory, each time they later draw upon it when writing new things in new ways would be unthinkable.\"</p><p>This analogy&#8212;AI as reader learning to write&#8212;provides the conceptual foundation that I think AI companies have been desperately seeking. It's not about copying; it's about learning patterns, understanding language, and creating something fundamentally new.</p><h2>The Million-Dollar Pivot</h2><p>Here's where I think Anthropic's story gets really interesting. After building their initial models on pirated content from Books3, Library Genesis, and other dubious sources, the company made a dramatic shift in 2024. They hired Tom Turvey, the former head of Google's book-scanning project, with a mandate to obtain \"all the books in the world\" through legitimate means.</p><p>Anthropic then spent millions of dollars purchasing physical books&#8212;many second-hand&#8212;which they proceeded to slice from their bindings and scan into digital format. The physical books were destroyed in the process, but the digital copies were ruled as legitimate fair use. This expensive pivot from piracy to purchase reveals something I've been saying for a while: AI companies can afford to do this right. They're choosing not to.</p><p>The court explicitly noted this financial capability, observing that Anthropic's later purchases of books they'd previously pirated \"will not absolve it of liability for the theft but it may affect the extent of statutory damages.\" Translation: we see you had the money all along.</p><h2>What This Means for Authors</h2><p>I think this ruling offers a glimmer of hope for authors who've watched AI companies feast on their work without compensation. While the fair use ruling means authors",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "can't stop AI training entirely, the court's condemnation of piracy and validation of legitimate book purchases creates real market incentives.</p><p>Consider what Anthropic's spending reveals: they paid millions for books, often buying used copies at market rates. This money flows back into the book ecosystem&#8212;to retailers, distributors, and ultimately supporting the market for authors' works. If every AI company followed this model instead of scraping pirate sites, we'd see a substantial new revenue stream for the publishing industry.</p><p>Moreover, the ruling's emphasis on transformation rather than reproduction protects authors' core market. The court stressed that it matters whether AI systems \"directly compete with the originals.\" Since Claude doesn't spit out verbatim passages from novels, the technology complements rather than replaces human authorship.</p><p>I see this as establishing a sustainable equilibrium: AI companies must pay for access to training materials, supporting the creative economy, while authors benefit from AI tools that help readers discover and engage with human-written works.</p><h2>The Domino Effect</h2><p>This ruling's impact extends far beyond Anthropic's legal troubles. I'm watching several major cases that will likely cite this precedent:</p><p><strong>The OpenAI Cases</strong>: Multiple lawsuits against OpenAI, including from the Authors Guild and various publishers, hinge on similar fair use arguments. Alsup's framework&#8212;distinguishing between training use and acquisition methods&#8212;gives OpenAI a potential path to victory, assuming they can demonstrate legitimate data sourcing.</p><p><strong>Kadrey v. Meta</strong>: The lawsuit against Meta for training LLaMA on Books3 (the same dataset Anthropic used) now faces an interesting precedent. Meta might win on fair use for training but could still face liability if they retained pirated materials in a permanent library.</p><p><strong>The Stability AI Litigation</strong>: Visual AI companies face additional complexities, but I think Alsup's \"transformative use\" reasoning could extend to image generation models that learn artistic styles without reproducing specific works.</p><h2>The New Compliance Playbook</h2><p>From my reading of Alsup's ruling, he's effectively created a compliance roadmap for AI companies:</p><ol><li><p><strong>Training on copyrighted works? Probably fine</strong>, as long as your model doesn't reproduce those works verbatim.</p></li><li><p><strong>Building a permanent library of pirated content? Definitely not fine</strong>, even if you only use it for training.</p></li><li><p><strong>Want to avoid liability? Buy the books</strong>. Or license them. Or use legitimately free sources. But stop pretending piracy is a necessary evil.</p></li><li><p><strong>Already have pirated content? Delete it after training</strong>. The court's distinction between temporary training copies and permanent library storage offers a potential safe harbor.</p></li></ol><h2>The Billion-Dollar Question</h2><p>With damages still to be determined, I calculate Anthropic faces potential liability in the billions. Statutory damages for willful infringement can reach $150,000 per work, and we're talking about millions of books. This creates a powerful deterrent effect: train responsibly or face existential financial risk.</p><p>The message I'm taking from this ruling is clear: the transformative nature of your technology doesn't give you a free pass to transform other people's property into your training data through illegal means. Fair use protects the learning, not the theft.</p><p>As more courts adopt Alsup's framework, I expect we'll see a fundamental shift in how AI companies approach data acquisition. The days of \"download first, ask permission never\" are ending. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-anthropic-ruling-a-roadmap-for",
    "embedding": [],
    "text": "future belongs to companies willing to do what Anthropic eventually did: open their wallets and do things the right way.</p><p>Even if it means destroying a few million books in the process&#8212;at least authors got paid for them.</p><h2>The Bigger Picture</h2><p>I should note that while Alsup's ruling gives us crucial clarity, we're still in the early innings of this game. This is just one federal district court's take, and I've learned to be cautious about declaring victory based on a single decision. We've got dozens of similar cases working through different courts, each with their own judges who might see things differently.</p><p>I'm particularly aware that the Northern District of California doesn't speak for the entire country. We're already seeing circuit splits on related AI issues&#8212;the Ninth Circuit requires \"actual knowledge\" for contributory infringement while the Second Circuit only needs \"reason to know.\" That's a big difference when we're talking about platforms hosting AI tools.</p><p>What's more, this ruling really only addresses text-based AI training. I'm left wondering how courts will handle visual AI systems, code generation, or the myriad other AI applications we're seeing. Fair use is notoriously fact-specific, and what works for Claude might not work for DALL-E or Copilot.</p><p>I expect we'll see Anthropic appeal to the Ninth Circuit, which could modify or even reverse parts of Alsup's reasoning. And honestly? I think we'll eventually need either the Supreme Court to step in and create nationwide clarity, or Congress to pass actual AI legislation. Neither seems likely in the near term, which means we're in for years of case-by-case battles as the law slowly catches up to the technology.</p><p>Still, Alsup's decision represents real progress. It's the first federal court to tackle these questions head-on, and that matters&#8212;even if it's just the opening chapter of a much longer story.<br></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!ftVE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1363491,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166769339?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!ftVE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!ftVE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7fba28-bf0b-4298-8e04-b176bc4b5005_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "From ChatGPT to Cluely: Riding the $120M Proactive AI Wave",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "Cheating, cheating, cheating. That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "<p><em><strong>Cheating, cheating, cheating.</strong> That's all I've been hearing for days since Cluely raised $15M from a16z. The news is full of over-baked takes about \"academic integrity.\" People are asking me how we do interviews anymore (they were asking that before too lol).</em></p><p><em>Look, I get it. The internet is the greatest narrative simplification machine ever built.</em></p><p><em>Here's the thing: I was playing around with Cluely before it got all hype-y. Not to cheat&#8212;I'm way past needing help with homework (though my kids might disagree). I picked it up because I was curious about this whole \"proactive AI\" thing everyone keeps talking about. What I found wasn't some elaborate cheating scheme&#8212;despite the aggressive marketing. It was something way more interesting: <strong>a glimpse of what happens when AI stops waiting for permission to help.</strong></em></p><p><em>And honestly? The whole cheating discourse is missing the point so badly it's almost painful to watch. <strong>Everyone is playing into Cluely&#8217;s hands and they love to see it guys.</strong> </em></p><p><em><strong>Forgive a little humor, but Cluely is cheating at cheating.</strong> They hype the cheating brand to draw in their target IC customers (Gen Z and Gen Alpha). But while everyone's arguing about whether students should use AI for essays (and earning them clicks), Cluely <strong>quietly built something that changes how we think about human-AI collaboration entirely.</strong> They're not just making another chatbot&#8212;<strong>they're showing us what Level 2 proactive AI actually looks like in the wild.</strong></em></p><p><em>I almost didn't write this piece. It felt too... nuanced? Too complex for the current moment where most of the news cycle is dominated by \"AI is cheating and we&#8217;re all doomed\" or (to a lesser extent) &#8220;a16z is out to lunch for funding this.&#8221; But when I watch a 21-year-old dropout secure venture funding based on a tool that most people completely misunderstand, I realize I at least need to try to articulate what&#8217;s going on under the marketing hype and doom-and-gloom.</em></p><p><em>Here's what I want to dig into: </em></p><ol><li><p><em><strong>Why</strong> <strong>Cluely's $120M valuation makes perfect sense from a startup strategy perspective</strong> (even if their AI is pretty mid)</em></p></li><li><p><em><strong>How Cluely helps us think about agentic AI</strong> <strong>and offers an early picture into where job skills</strong> <strong>are going next</strong> (especially with those juicy B2B contracts they&#8217;ve landed)</em></p></li><li><p><em><strong>How</strong> <strong>their UX innovation reveals where agentic AI is actually heading</strong>, and why the cognitive fitness implications are way more profound than anyone's talking about</em></p></li><li><p><em><strong>Plus</strong>, <strong>if you're building anything in the AI space, understanding what Cluely got right about distribution and timing might be the most important lesson of 2025.</strong></em></p></li><li><p><em><strong>And as a bonus, what we can learn from Cluely&#8217;s leaked system prompt. </strong>So we&#8217;ll get some good prompt analysis in at the end for you prompting geeks!</em></p></li></ol><p><em>The real story isn't about cheating. It's about what happens when we stop asking \"How can AI help me find the answers?\" and start asking and <strong>&#8220;How can I use proactive AI to extend my thinking?&#8221;</strong> That shift&#8212;from reactive to proactive intelligence&#8212;is worth way more than $15 million. And I wrote 31 pages to show you why",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the",
    "embedding": [],
    "text": "lol</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/from-chatgpt-to-cluely-riding-the\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "Software 3.0 vs AI Agentic Mesh: Why McKinsey Got It Wrong",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "<p></p><p><em>This was so much fun to write! We don&#8217;t often get to see the battle lines drawn this clearly in AI land, but right now we&#8217;ve got two completely different visions of the future playing out in real time.</em></p><p><em>Fighter on one side: <strong>Andrej Karpathy&#8217;s &#8220;Software 3.0&#8221;</strong> (natural language as the programming interface, grounded in actual experience building Autopilot)</em></p><p><em>On the other: <strong>McKinsey&#8217;s &#8220;AI Agentic Mesh&#8221;</strong> (distributed autonomous agents, grounded in&#8230; PowerPoint slides lol)</em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!WE-I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1994788,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!WE-I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!WE-I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d680ecc-53bb-4e09-885b-64fb54b59589_1024x1024.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p><em>Fun aside, this isn&#8217;t just another framework war&#8212;<strong>it&#8217;s a perfect case study in how the same technology gets interpreted completely differently depending on whether you&#8217;re building or consulting</strong>. It&#8217;s been really fun to dive into this one because the stakes are real. We&#8217;re watching billions get allocated based on these competing visions, and only one of them is going to survive contact with reality.</em></p><p><em>Here&#8217;s the thing: I normally keep the <strong>deep strategic breakdowns behind the paywall</strong>, but this battle felt too important to gate. <strong>Consider this a taste of what subscribers get every week</strong>&#8212;the kind of analysis that cuts through the noise and shows you what&#8217;s actually happening.</em></p><p><em>Because while executives are choosing sides, there&#8217;s <strong>real transformation happening in the quiet corners where developers are shipping with AI</strong>.</em></p><p><em>So let&#8217;s roll up our sleeves and dig in. You&#8217;ll get:</em></p><ul><li><p><em>The <strong>real story</strong> behind both visions</em></p></li><li><p><em>Why the <strong>communication gap between builders and executives</strong> keeps creating expensive disasters</em></p></li><li><p><em>And most importantly, <strong>the first practical framework for translating AI constraints into business strategy that actually works</strong></em></p></li></ul><p><em><strong>This is the clarity I can&#8217;t find so, I wrote it up.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h1><strong>The TLDR</strong></h1><p>Two radically different visions of AI's future are competing for executive attention, and the choice between them will determine which organizations thrive in the coming \"decade of agents.\"</p><h2>The Two Visions</h2><p><strong>Karpathy's Software 3.0</strong> represents a fundamental shift where natural language becomes the primary programming interface. Based on his experience building Tesla's Autopilot, Karpathy describes AI as \"brilliant interns with perfect recall but no judgment\"&#8212;powerful tools requiring human oversight. His vision acknowledges critical limitations like \"jagged intelligence\" (excelling at complex tasks while failing at simple ones) and \"anterograde amnesia\" (no memory between conversations). The focus is on <strong>augmentation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "through human-AI collaboration</strong>, not replacement.</p><p><strong>McKinsey's AI Agentic Mesh</strong> promises enterprise-wide networks of autonomous agents coordinating seamlessly across organizations. This consultant-crafted vision features \"composable\" systems, \"distributed intelligence,\" and \"governed autonomy\"&#8212;architectural concepts that sound impressive in boardrooms but violate fundamental technical principles.</p><h2>Why the Technical Community Rejects McKinsey's Vision</h2><p>Practitioners who actually build AI systems universally dismiss the agentic mesh. Cognition (creators of Devin) concluded that multi-agent systems \"only result in fragile systems\" because decision-making becomes too dispersed and context can't be shared effectively. Anthropic found their multi-agent systems use \"15&#215; more tokens than chats\" and struggle with coordination. The technical reality: successful AI implementations require centralized control and tight integration&#8212;the opposite of McKinsey's distributed mesh.</p><h2>The Executive Communication Crisis</h2><p>The gap between technical reality and executive understanding has created a crisis of expensive failures. <strong>Klarna's AI disaster</strong> exemplifies this pattern: the company claimed its AI handled 700 agents' worth of work and saved $40 million annually, only to later admit they'd \"gone too far\" and quietly rehired human workers due to \"lower quality\" service.</p><p>This pattern repeats across industries&#8212;IBM Watson's $62 million failure at MD Anderson, McDonald's abandoned AI drive-through, Air Canada's policy-inventing chatbot. Each failure stems from executives chasing automation fantasies instead of understanding AI's true capabilities and constraints.</p><h2>The Path Forward</h2><p>Organizations need <strong>technically grounded executive narratives</strong> that translate AI capabilities into business terms without losing nuance. Successful approaches include:</p><ul><li><p><strong>Operational analogies</strong>: Frame LLMs as \"brilliant interns\" rather than using technical jargon</p></li><li><p><strong>Financial constraints</strong>: Show real costs&#8212;processing large datasets requires breaking them into thousands of chunks, costing hundreds of thousands in compute</p></li><li><p><strong>Domain-specific examples</strong>: Demonstrate specific failure modes in the executive's industry</p></li><li><p><strong>Progressive disclosure</strong>: Let pilots reveal limitations naturally through experience</p></li></ul><h2>Why This Matters Now</h2><p>Software 3.0 isn't future speculation&#8212;it's today's reality. Developers using tools like Cursor AI report 10-100x productivity gains on specific tasks. Startups with tiny teams now compete with products that previously required massive engineering organizations. The transformation is happening at AI speed, not traditional enterprise timelines.</p><h2>The Binary Choice</h2><p>Organizations face a stark choice: embrace AI as a collaborative amplifier of human capability, or chase consultant fantasies promising autonomous replacement. The 4% of companies generating substantial AI value share common traits&#8212;they focus on augmentation, invest heavily in human-AI workflows, and measure success through value creation rather than cost reduction.</p><p><strong>The wave of Software 3.0 is breaking now.</strong> Organizations that catch it with clear eyes will build sustainable advantages. Those chasing McKinsey's distributed dreams will join the graveyard of failed AI transformations, wasting billions while competitors build real value through human-AI collaboration.</p><p>The future belongs to \"Iron Man suits\" for knowledge work&#8212;AI that amplifies human capability rather than replacing the human inside.</p><h1><strong>Software 3.0 and the Executive Delusion: Why Karpathy's Vision Matters and McKinsey's Doesn't</strong></h1><h2><strong>I. Opening: The Tale of Two Visions</strong></h2><p>This week at Y Combinator's AI Startup School, Andrej Karpathy stood before a room of builders and declared that we've entered the era of Software 3.0&#8212;where natural language becomes the primary programming interface. \"The hottest new programming language is English,\" he said, describing a world where anyone who can clearly articulate ideas can create",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "software. His vision, grounded in years of replacing traditional code with neural networks at Tesla, represents a profound shift in how we build and interact with technology.</p><p>Meanwhile, in boardrooms across the Fortune 500, McKinsey consultants are selling executives on something called the \"AI Agentic Mesh\"&#8212;a grand vision of autonomous agents coordinating seamlessly across enterprises, promising to finally deliver the ROI that has eluded 78% of companies dabbling in generative AI. Their PowerPoints paint a picture of composable, distributed, vendor-agnostic systems where hundreds of AI agents collaborate like a perfectly choreographed symphony.</p><p>These two visions couldn't be more different. One comes from the trenches of building Autopilot at Tesla, where Karpathy watched neural networks progressively eat away at 300,000 lines of C++ code. The other comes from consulting frameworks designed to sound strategic in executive briefings. One acknowledges fundamental limitations&#8212;what Karpathy calls \"jagged intelligence\" and \"anterograde amnesia.\" The other handwaves away technical constraints with promises of \"governed autonomy\" and \"layered decoupling.\"</p><p>The gap between these visions isn't academic. It's measured in billions of dollars misdirected, thousands of careers disrupted, and countless opportunities missed. When Klarna's CEO boasted about replacing 700 customer service agents with AI, saving $40 million annually, the tech press celebrated. Months later, he quietly admitted they'd \"gone too far,\" delivering \"lower quality\" service and hiring humans back. The company had fallen for the same delusion McKinsey now packages as revolutionary architecture.</p><p>This pattern repeats across industries. IBM Watson consumed $62 million at MD Anderson before being abandoned. McDonald's discontinued its AI drive-through after three years of adding bacon to ice cream orders. Air Canada faced legal troubles when its chatbot invented refund policies. Each failure shares the same root cause: executives chasing consultant fantasies instead of understanding technical reality.</p><p>The tragedy is that real transformation is happening&#8212;just not the kind McKinsey sells. At companies embracing Karpathy's vision of \"partial autonomy,\" developers using tools like Cursor AI report 10-100x productivity gains for specific tasks. They're not replacing humans; they're amplifying human capability. They're not building autonomous agent meshes; they're creating tight feedback loops between human creativity and AI generation.</p><p>But this nuanced reality doesn't sell well in boardrooms. It requires admitting that AI has \"jagged intelligence\"&#8212;brilliant at complex tasks while failing at simple ones. It means accepting that large language models are, in Karpathy's memorable phrase, \"stochastic simulations of people\" with \"anterograde amnesia,\" unable to remember or learn between conversations. It demands investment in people and processes, not just technology.</p><p>The cost of this communication failure compounds daily. While technical teams know that multi-agent systems \"only result in fragile systems\" (as Cognition, creators of Devin, learned the hard way), executives allocate budgets toward McKinsey's distributed mesh dreams. While practitioners understand that AI agents use \"15&#215; more tokens than chats\" (per Anthropic's experience), leaders expect cost savings from wholesale automation. The mismatch between expectation and reality guarantees expensive failure.</p><p>We stand at an inflection point. Karpathy isn't describing some distant future&#8212;Software 3.0 is breaking through now. Natural language interfaces are transforming how we build software today. The",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "question isn't whether this transformation will happen, but whether organizations will navigate it successfully or crash against the rocks of consultant-crafted delusions.</p><p>This is a story about two fundamentally different ways of understanding AI's impact on work and business. One path, illuminated by builder wisdom and technical truth, leads to genuine augmentation and value creation. The other, paved with PowerPoint promises and architectural astronautics, leads to the same graveyard of failed digital transformations that litters corporate history.</p><p>As we enter what Karpathy calls \"the decade of agents,\" the stakes couldn't be higher. The organizations that thrive will be those that reject the siren song of autonomous replacement and embrace the messier, more honest reality of human-AI collaboration. They'll build with humility about limitations while harnessing genuine capabilities. Most importantly, they'll listen to builders over consultants, choosing technically grounded evolution over executive-friendly revolution.</p><p>The pages that follow will unpack these two visions in detail, explore why executives keep falling for automation fantasies, and chart a path toward narratives that bridge the gap between technical reality and business strategy. Because in the end, Software 3.0's promise isn't about replacing human intelligence&#8212;it's about amplifying it. But only if we're honest enough to see it clearly.</p><h2><strong>II. Understanding Software 3.0: What Karpathy Actually Said</strong></h2><p>To understand why Karpathy's Software 3.0 vision matters, we need to grasp both its revolutionary implications and its refreshing honesty about limitations. Unlike the consultant-speak flooding executive inboxes, Karpathy's framework emerges from hard-won experience replacing traditional code with neural networks at Tesla's Autopilot division. His presentation at Y Combinator wasn't selling a product&#8212;it was sharing a profound shift in how we create and interact with software.</p><h3><strong>The Evolution Framework</strong></h3><p>Karpathy's Software 3.0 thesis rests on understanding two previous paradigm shifts. Software 1.0 represents traditional programming&#8212;the world we've inhabited since computing began. Developers write explicit instructions in languages like Python, C++, or Java, specifying exact algorithms, control flows, and data structures. Every behavior is deliberately coded, debugged line by line, and maintained through human understanding. This is the programming most people recognize: explicit, deterministic, and fully under human control.</p><p>Software 2.0 emerged from a radical insight Karpathy articulated in 2017, based on his experience at Tesla. Instead of writing code to detect stop signs or identify lane markings, engineers began curating datasets and designing neural network architectures. The actual \"program\" became millions or billions of learned parameters&#8212;weights discovered through optimization algorithms rather than human reasoning. As Karpathy watched at Tesla, neural networks progressively consumed traditional code. Features requiring thousands of lines of C++ were replaced by learned behaviors that performed better with less explicit programming.</p><p>Software 3.0 represents the next leap: natural language becoming the primary programming interface through Large Language Models. As Karpathy explains: \"What's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0.\"</p><p>In this new paradigm, prompts replace code as the",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "primary way to direct computational behavior. LLMs serve as interpreters that understand and execute natural language instructions. Context windows act as working memory. Most radically, programming becomes universally accessible&#8212;anyone who can clearly express ideas can create software.</p><h3><strong>LLMs as a New Type of Computer</strong></h3><p>Karpathy's most profound insight reframes LLMs not as tools but as fundamentally new computational systems. \"When I use ChatGPT,\" he notes, \"I feel like I'm talking to an operating system through the terminal.\" This isn't mere metaphor&#8212;he maps traditional computing concepts onto language-based processing:</p><p>The LLM functions as the CPU, the core processing unit executing instructions. The context window serves as RAM, providing short-term working memory for active computation. Prompts become programs&#8212;natural language instructions directing behavior. Tokens represent the fundamental units of data, like bytes in traditional computing.</p><p>This reconceptualization helps explain why LLMs feel qualitatively different from previous AI systems. They're not just pattern matchers or classifiers; they're general-purpose computers that happen to process natural language instead of binary code.</p><p>Karpathy extends this thinking through three powerful infrastructure analogies. First, he positions AI as \"the new electricity\"&#8212;utility infrastructure with massive capital expenditure for training (like building power plants) and operational costs for serving (like distribution). The pay-per-token API model mirrors metered electricity billing, making AI universally accessible.</p><p>Second, he compares LLM training to semiconductor fabrication. Both require specialized hardware, massive capital investment ($100M+ for frontier models), and produce standardized products used across industries. Like chip fabs, AI training concentrates in a few players due to economies of scale.</p><p>Third, and most provocatively, he positions LLMs as operating systems for AI applications. They provide standard interfaces (chat, completion APIs), manage resources (context, compute), support \"applications\" built on top (agents, tools), and abstract complexity from end users. This OS metaphor explains why platform dynamics are emerging, with developers building atop foundation models rather than training their own.</p><h3><strong>The Critical Limitations Karpathy Acknowledges</strong></h3><p>Unlike McKinsey's boundless optimism, Karpathy's framework explicitly acknowledges fundamental limitations. He describes LLMs as \"stochastic simulations of people, with a kind of emergent 'psychology.'\" This framing captures both their human-like capabilities&#8212;reasoning patterns, creative responses, contextual understanding&#8212;and their decidedly non-human failure modes.</p><p>The first major limitation is what Karpathy coined as \"jagged intelligence.\" He explains: \"The word I came up with to describe the (strange, unintuitive) fact that state of the art LLMs can both perform extremely impressive tasks (e.g. solve complex math problems) while simultaneously struggle with some very dumb problems.\"</p><p>This jaggedness manifests in bewildering ways. An LLM might solve graduate-level mathematics while failing at \"which is bigger, 9.11 or 9.9?\" It can write sophisticated code but struggle with basic counting. It demonstrates deep knowledge while making elementary logical errors. This differs fundamentally from human intelligence development, where capabilities typically build coherently from simple to complex.</p><p>The second critical limitation is \"anterograde amnesia.\" Karpathy notes: \"LLMs are a bit like a coworker with Anterograde amnesia - they don't consolidate or build long-running knowledge or expertise once training is over and all they have is short-term memory (context window).\"</p><p>This creates fundamental constraints: no learning from experience across",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "sessions, no building personalized understanding over time, constant need to re-establish context, and knowledge frozen at training cutoff. Every conversation starts fresh, with no memory of previous interactions or ability to improve based on feedback.</p><p>These limitations aren't bugs to be fixed but fundamental characteristics of current architecture. Karpathy suggests we need new learning paradigms&#8212;perhaps \"System Prompt Learning\" where LLMs modify their own instructions&#8212;but acknowledges we're not there yet.</p><h3><strong>Partial Autonomy and the Generation-Verification Loop</strong></h3><p>Rather than chasing full automation dreams, Karpathy advocates for \"partial autonomy\"&#8212;systems that augment human capabilities while maintaining oversight. He demonstrates this through Cursor AI's \"autonomy slider,\" showing graduated levels of AI assistance:</p><ul><li><p>Tab completion: Minimal AI assistance for code completion</p></li><li><p>Cmd+K: Targeted code modifications with human direction</p></li><li><p>Cmd+L: File-level transformations with AI planning</p></li><li><p>Cmd+I: Maximum autonomy agent mode</p></li></ul><p>This graduated approach mirrors autonomous vehicle development, where Level 2-3 automation proves more practical than jumping to Level 5. It acknowledges that different tasks require different levels of AI involvement and human oversight.</p><p>Central to Software 3.0 is the generation-verification loop&#8212;rapid iteration between fast AI generation, efficient human verification, and iterative refinement. Karpathy emphasizes this loop as key to practical AI applications, making AI a collaborative partner rather than replacement.</p><p>He describes his own workflow transformation: \"Most of my 'programming' is now writing English (prompting and then reviewing and editing the generated diffs), and doing a bit of 'half-coding' where you write the first chunk of the code you'd like, maybe comment it a bit so the LLM knows what the plan is, and then tab tab tab through completions.\"</p><p>This represents a fundamental shift in skills. Natural language articulation becomes as important as traditional coding. Code review and verification matter more than initial writing. The ability to iterate quickly and recognize good solutions becomes paramount.</p><h3><strong>From Vision to Reality</strong></h3><p>Karpathy grounds his framework in concrete examples. He extensively demonstrates Cursor as the exemplar Software 3.0 application, showing how \"vibe coding\"&#8212;describing desired functionality in natural language&#8212;produces working code. He cites menugen.app, which converts restaurant menu text into visual designs, as pure natural language programming in action.</p><p>But he's equally clear about infrastructure needs. He recommends creating \"LLMs.txt\" files&#8212;AI-readable summaries of codebases&#8212;recognizing that \"HTML is not very parseable for LLMs.\" He discusses tools like Gitingest that convert repositories into LLM-digestible formats. These practical details reveal someone building with these technologies, not just theorizing about them.</p><p>Most importantly, Karpathy's vision acknowledges the messy reality of technological change. Software 3.0 isn't replacing previous paradigms&#8212;it's adding a powerful new layer. Professional developers will need all three paradigms, choosing the right tool for each problem. The democratization of programming doesn't eliminate the need for expertise; it changes what expertise looks like.</p><p>This honesty about capabilities and limitations, grounded in practical experience, makes Karpathy's framework genuinely useful. While McKinsey promises frictionless agent meshes, Karpathy offers something more valuable: a realistic path forward that acknowledges both the transformative potential and inherent constraints of AI systems.</p><p>Software 3.0 is happening now. The question isn't whether natural language will become a primary programming interface&#8212;it already is for many developers. The question is whether",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "organizations will embrace this reality with clear eyes or chase consultant fantasies. Karpathy's framework, born from building rather than selling, provides the clarity needed to choose wisely.</p><h2><strong>III. The McKinsey Mirage: Agentic Mesh Deconstructed</strong></h2><p>While Karpathy offers builder wisdom about AI's real capabilities and constraints, McKinsey sells executives a radically different vision: the AI Agentic Mesh. This framework promises to solve the \"gen AI paradox\"&#8212;where 78% of companies use generative AI but see minimal bottom-line impact&#8212;through an enterprise-wide architectural paradigm enabling autonomous agents to coordinate seamlessly across organizations. The gap between this consulting fantasy and technical reality reveals why so many AI initiatives fail.</p><h3><strong>What McKinsey Promises</strong></h3><p>McKinsey's agentic mesh emerged as their answer to widespread AI disappointment. Their diagnosis seems reasonable: companies struggle because they deploy AI in isolated pockets rather than integrated systems. Their solution, however, veers into architectural astronautics.</p><p>The framework rests on five design principles that sound impressive in boardrooms:</p><p><strong>Composability</strong>: Any agent, tool, or LLM can be plugged in without system rework. McKinsey envisions a world where organizations mix and match AI components like Lego blocks, seamlessly integrating \"custom-built and off-the-shelf agents within a unified framework.\"</p><p><strong>Distributed Intelligence</strong>: Tasks are decomposed and resolved by cooperating agent networks. Instead of monolithic systems, McKinsey proposes swarms of specialized agents that somehow coordinate to solve complex problems.</p><p><strong>Layered Decoupling</strong>: Logic, memory, orchestration, and interfaces are separated for maximum modularity. Each layer can be independently updated or replaced without affecting others.</p><p><strong>Vendor Neutrality</strong>: All components can be independently updated or replaced. No lock-in, no dependencies&#8212;just frictionless interchangeability.</p><p><strong>Governed Autonomy</strong>: Agent behavior is controlled via embedded policies and permissions. Autonomous yet controlled, independent yet coordinated&#8212;McKinsey promises to square this circle.</p><p>The vision culminates in \"large-scale, intelligent agent ecosystems\" operating \"safely and efficiently\" across the enterprise. Hundreds of agents would collaborate autonomously, sharing context and coordinating decisions while remaining modular and replaceable. It's a CTO's dream and an engineer's nightmare.</p><h3><strong>Why Technical Practitioners Call It \"Executive Speak\"</strong></h3><p>The technical community's response to McKinsey's agentic mesh has been overwhelmingly negative, and for good reason. Practitioners who actually build AI systems recognize it as a prime example of consulting firms packaging buzzwords without understanding fundamental constraints.</p><p>The most damning critique comes from Cognition, creators of Devin, one of the most advanced AI coding agents available. Through painful experience, they've concluded that \"multi-agent architectures\" result in \"fragile systems\" where \"decision-making ends up being too dispersed and context isn't able to be shared thoroughly enough between the agents.\" Their verdict is unequivocal: multi-agent systems \"only result in fragile systems\" in 2025.</p><p>This isn't theoretical skepticism&#8212;it's hard-won wisdom. Cognition discovered that successful agent systems require two principles that directly contradict McKinsey's distributed mesh vision: \"Share context, and share full agent traces, not just individual messages\" and \"Actions carry implicit decisions, and conflicting decisions carry bad results.\" These principles demand tight integration and centralized coordination&#8212;the opposite of McKinsey's loosely coupled, distributed architecture.</p><p>Anthropic's experience building their Research feature provides additional evidence. Despite massive engineering investment and constrained scope, they found severe limitations. Their multi-agent system uses \"15&#215; more tokens than chats,\" only works",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "for tasks with \"heavy parallelization,\" and struggles because \"most coding tasks involve fewer truly parallelizable tasks than research.\" Even with world-class engineers and focused application, they acknowledge that \"LLM agents are not yet great at coordinating and delegating to other agents in real time.\"</p><p>The technical problems compound exponentially with scale. McKinsey handwaves critical challenges that have no known solutions:</p><p><strong>Context Sharing</strong>: How do distributed agents maintain coherent understanding across organizational boundaries? Cognition provides a telling example: one agent builds a \"Super Mario Bros\" background while another builds an incompatible bird sprite, leaving the final agent unable to reconcile the mismatch. Now imagine this problem multiplied across hundreds of enterprise agents.</p><p><strong>Coordination Complexity</strong>: Each additional agent exponentially increases overhead. With McKinsey envisioning enterprise-wide deployments, the coordination problem becomes computationally intractable. Anthropic warns that \"one step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes.\" In a distributed mesh, these failures cascade catastrophically.</p><p><strong>Security Vulnerabilities</strong>: Researchers have identified that multi-agent systems face novel attack vectors including \"secret collusion channels\" and \"coordinated swarm attacks.\" McKinsey's framework, with its emphasis on plug-and-play composability, multiplies these vulnerabilities.</p><p><strong>Computational Economics</strong>: Multi-agent systems are voraciously expensive. Anthropic's \"15&#215; more tokens\" translates directly to 15&#215; the cost. McKinsey's vision of hundreds of coordinating agents would require astronomical compute budgets that dwarf any efficiency gains.</p><h3><strong>The Reality of \"Agentic\" Implementations</strong></h3><p>When we examine actual systems being built under the \"agentic mesh\" banner, they bear little resemblance to McKinsey's vision. These implementations reveal the constraints that McKinsey's framework ignores.</p><p>SIRP's cybersecurity implementation&#8212;one of the few production systems using the \"agentic mesh\" terminology&#8212;shows the gap between vision and reality. Their system required \"breaking down a monolithic system into flexible, modular microservices\" and focuses exclusively on security operations. Rather than autonomous agents coordinating across the enterprise, SIRP built specialized tools for a narrow domain with strict boundaries and centralized control.</p><p>The pattern repeats across genuine implementations. Successful systems constrain scope ruthlessly, centralize control despite distributed execution, prioritize reliability over autonomy, and maintain human oversight at every critical decision point. These constraints directly contradict McKinsey's framework, which promises unconstrained scope, distributed autonomy, and minimal human involvement.</p><p>Even practitioners claiming to build agentic meshes reveal the reality gap. Eric Broda, who claims to be writing a book on the topic, describes building \"enterprise grade autonomous agents and putting them into an ecosystem\" but provides no evidence of the distributed, composable architecture McKinsey envisions. The silence speaks volumes&#8212;if anyone had built McKinsey's vision successfully, we'd have case studies, not PowerPoints.</p><p>The most reliable pattern identified by practitioners is the \"single-threaded linear agent\" where \"context is continuous.\" Even when dealing with long-duration tasks, the recommended approach involves \"a new LLM model whose key purpose is to compress a history of actions &amp; conversation into key details, events, and decisions\" rather than distributing work across autonomous agents. This is precisely the opposite of McKinsey's mesh topology.</p><p>Anthropic's production system uses an \"orchestrator-worker pattern\" with strict hierarchical control. Workers execute specific tasks under tight supervision. The orchestrator maintains global context and resolves conflicts. There's no",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "emergent coordination or distributed decision-making&#8212;just carefully managed execution within rigid constraints.</p><h3><strong>Why This Matters</strong></h3><p>The disconnect between McKinsey's agentic mesh and technical reality isn't merely academic. Organizations allocating resources based on this framework are setting themselves up for expensive failure. The investment numbers are staggering: AI agents captured 46.4% of US venture capital funding in 2024, with the market projected to grow from $5.1 billion to $47.1 billion by 2030.</p><p>This investment reflects genuine excitement about AI agents as a category, not endorsement of McKinsey's architectural vision. But when executives conflate the two&#8212;believing that agent success requires distributed meshes&#8212;they make catastrophic resource allocation decisions. Technical teams know the mesh won't work but find themselves building toward an impossible architecture because the board bought the vision.</p><p>The agentic mesh represents a broader pattern in enterprise technology: consultant-created frameworks that promise easy solutions to hard problems. These frameworks generate compelling PowerPoints and executive buy-in but cannot be translated into working systems. The gap between promise and delivery erodes trust, wastes resources, and delays genuine transformation.</p><p>McKinsey's agentic mesh isn't just wrong&#8212;it's actively harmful. By promising autonomous coordination without acknowledging fundamental constraints, it sets impossible expectations. By advocating distributed architectures that violate proven principles, it guarantees technical failure. By focusing on architectural elegance over practical delivery, it diverts attention from approaches that actually work.</p><p>The technical community's rejection of McKinsey's framework isn't close-mindedness&#8212;it's pattern recognition. They've seen these promises before, tried to build these systems, and learned why they fail. Their skepticism reflects wisdom earned through experience, not resistance to change.</p><p>Organizations considering agentic AI should listen to builders, not consultants. Focus on narrow, well-defined use cases. Maintain centralized control and clear boundaries. Invest in robust testing and gradual rollout. Most importantly, reject any framework that promises easy solutions to coordination, context sharing, and autonomous decision-making. These remain unsolved problems in AI, and no amount of PowerPoint polish will change that reality.</p><h2><strong>IV. The Executive Communication Crisis and Its Consequences</strong></h2><p>The gap between AI's technical reality and executive understanding isn't just a communication problem&#8212;it's a crisis generating billions in wasted investment and thousands of disrupted careers. This crisis follows a predictable pattern: bold automation promises, hidden implementation failures, quiet reversals, and expensive lessons learned. Understanding this pattern through concrete examples reveals why organizations keep failing at AI transformation and what must change.</p><h3><strong>The Klarna Disaster as Archetype</strong></h3><p>Klarna's AI journey perfectly encapsulates the executive communication crisis. In February 2024, CEO Sebastian Siemiatkowski made headlines by announcing their AI assistant was handling 2.3 million conversations monthly&#8212;two-thirds of all customer service chats&#8212;and doing the work of 700 full-time agents. The numbers seemed irrefutable: resolution times dropped from 11 minutes to under 2 minutes, customer satisfaction scores remained \"equal,\" and the company projected $40 million in annual profit improvements.</p><p>Siemiatkowski positioned Klarna as OpenAI's \"favorite guinea pig,\" a forward-thinking company leading the AI revolution. The narrative was irresistible to investors and board members: replace expensive human workers with efficient AI, maintain quality, and pocket the savings. Tech media amplified the story without scrutiny, creating a template other executives",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "would rush to follow.</p><p>The reality proved starkly different. By late 2024, Siemiatkowski publicly admitted what insiders already knew: they had \"gone too far\" with AI automation, although the company maintains there is still an AI component to customer service and they are using a dual-track approach. Regardless, his confession was remarkably candid: \"As cost unfortunately seems to have been a too predominant evaluation factor when organizing this, what you end up having is lower quality.\" The company began hiring human agents again, implementing what Siemiatkowski now calls an \"Uber-type setup\" for remote customer service workers.</p><p>Independent testing revealed problems that Klarna's cherry-picked metrics had hidden. Response times included 15-20 second awkward delays between messages&#8212;technically fast but experientially frustrating. The AI provided overly verbose, unhelpful responses that filled entire chat windows with robotic text. Most damning, when customers expressed financial hardship&#8212;asking questions like \"What happens if I can't pay on time?\"&#8212;the AI responded with emotionless boilerplate, lacking any acknowledgment of their difficult situation.</p><p>The metrics Klarna celebrated masked fundamental failures. While the AI could quickly provide scripted responses, it couldn't actually resolve complex issues. Many \"successful\" interactions simply directed customers to contact merchants directly or ended with customers abandoning their queries in frustration. High abandonment rates were misinterpreted as successful resolutions. The company essentially flew blind while claiming victory based on incomplete data.</p><p>Security vulnerabilities emerged as users discovered they could manipulate the chatbot through prompt injection attacks. One user successfully got the bot to generate Python code&#8212;completely outside its intended function. Despite safety guardrails, the system remained vulnerable to clever prompting that bypassed restrictions. In financial services, where trust is paramount, these vulnerabilities represented existential risk.</p><p>Industry experts like tech analyst Gergely Orosz tested the bot personally and found it \"underwhelming,\" noting it \"recites exact docs and passes me on to human support fast.\" Rather than replacing agents, the AI merely acted as an inefficient gateway to human help, adding friction to the customer experience while saving no actual labor.</p><h3><strong>Why Executives Fall for the Automation Fallacy</strong></h3><p>The Klarna pattern&#8212;bold automation claims followed by quiet reversal&#8212;repeats across industries because executives consistently misunderstand the nature of work itself. This misunderstanding stems from viewing jobs through what academics call the \"bundles of tasks\" framework, popularized by economist David Autor. In this model, occupations are collections of discrete, potentially automatable tasks. If AI can handle each task, the thinking goes, it can replace the job.</p><p>Geoffrey Hinton's 2016 prediction about radiologists illustrates this fallacy perfectly. The godfather of deep learning declared: \"People should stop training radiologists now. It's just completely obvious that within five years deep learning is going to do better than radiologists.\" He compared radiologists to cartoon characters who had already run off a cliff but hadn't yet looked down.</p><p>Eight years later, the United States faces a historic radiologist shortage with over 1,400 open positions. Radiology employment has grown by 7% since Hinton's prediction. Mayo Clinic alone expanded its radiology staff by 55%. Even Hinton himself admitted in 2024 that he \"spoke too broadly\" and was \"wrong on",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the timing.\" Current projections show the shortage will persist through 2055 without intervention, with supply growing 25.7% while demand grows 16.9-26.9%</p><p>The failure wasn't technical&#8212;AI has indeed become excellent at pattern recognition in medical images. The failure was conceptual. Radiologists don't just identify patterns; they correlate findings with patient history, communicate with referring physicians, make treatment recommendations, manage departmental workflows, mentor residents, and navigate complex healthcare systems. These interconnected responsibilities resist decomposition into discrete tasks.</p><p>Research from the EU JRC-Eurofound Tasks Framework reveals that \"tasks do not exist in isolation, they are coherently bundled into jobs which are performed by people, and the entire process has to be socially organised.\" This social organization&#8212;what Tanya Reilly termed \"glue work\"&#8212;remains invisible in most job analyses yet proves essential for organizational function.</p><p>In radiology, glue work includes coordinating with technicians about scan protocols, discussing complex cases with referring physicians, managing equipment schedules, and building relationships that enable smooth departmental operation. When organizations attempt to automate based on visible tasks alone, this invisible coordination work becomes more complex and crucial, not less.</p><p>Susan Leigh Star's research on \"invisible work\" explains why automation efforts consistently fail. Creating \"effortless ease\" in any system requires continuous, often unrecognized maintenance work. The automation paradox, identified by researcher Lisanne Bainbridge, states: \"The more efficient the automated system, the more crucial the human contribution of the operators becomes.\"</p><p>This paradox manifests dramatically in AI implementations. As individual tasks become automated, the coordination work binding them together grows more complex. Automated systems generate edge cases requiring human judgment. Quality assurance demands increase as someone must verify automated outputs and manage failures. The promise of labor savings evaporates as new forms of work emerge.</p><h3><strong>The Downstream Devastation</strong></h3><p>When executives misunderstand AI capabilities, the consequences cascade through organizations with devastating effect. The numbers tell a sobering story:</p><ul><li><p>Over 80% of AI projects fail&#8212;twice the failure rate of traditional IT projects (RAND Corporation)</p></li><li><p>While 78% of organizations use AI in at least one business function, only 4% generate substantial value (BCG)</p></li><li><p>42% of companies abandoned most AI initiatives by 2024, up from 17% in 2023 (S&amp;P Global)</p></li><li><p>Only 25% of AI business projects deliver promised ROI (IBM)</p></li></ul><p>These aren't just statistics&#8212;they represent enormous waste. IBM Watson for Oncology consumed $62 million at MD Anderson Cancer Center over four years before abandonment. The system, trained on hypothetical rather than real patient data, gave what one doctor called \"unsafe and incorrect\" treatment recommendations. McDonald's three-year partnership with IBM for AI drive-through ordering ended in 2024 after viral videos showed the system adding 260 Chicken McNuggets to a single order. Amazon scrapped its AI recruiting tool after discovering it discriminated against women, having learned bias from historical hiring data.</p><p>Each failure shares common patterns: oversimplifying job complexity, ignoring integration challenges, and assuming technology can directly substitute for human judgment. The hidden costs compound quickly. IBM research indicates computing costs will climb 89% between 2023-2025, with 70% of executives citing generative AI as the primary driver. Data preparation, infrastructure scaling, specialized talent, compliance requirements, and ongoing maintenance often dwarf initial investment",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "projections.</p><p>McKinsey research shows 38% of leaders expect to reskill more than 20% of their workforce, while 8% anticipate workforce reductions exceeding 20%. This \"tradeoff spectrum\" mentality&#8212;viewing AI agents as direct substitutes for human workers&#8212;drives many failed implementations. When executives operate from this framework, they make decisions that guarantee failure: underinvesting in change management, expecting immediate ROI, ignoring integration complexity, and measuring success through cost reduction rather than value creation.</p><p>The human cost extends beyond mere employment numbers. When Klarna announced its AI success, employee morale plummeted as workers saw themselves as expendable. The eventual reversal and rehiring damaged trust and institutional knowledge. This pattern&#8212;premature automation announcements followed by workforce disruption and eventual reversal&#8212;destroys organizational capability even when jobs ultimately return.</p><h3><strong>The Translation Problem</strong></h3><p>The root cause of these failures lies in a fundamental translation problem between technical teams and executive leadership. Technical teams understand AI's capabilities and limitations but struggle to communicate them in business terms. Executives need to make strategic decisions but lack the framework to evaluate AI realistically. Into this gap step consultants with frameworks like McKinsey's agentic mesh, offering simple narratives that obscure complex realities.</p><p>Traditional technical explanations fail in boardrooms. Terms like \"neural networks,\" \"transformers,\" \"context windows,\" and \"token limits\" don't translate to business impact. When engineers try to explain why distributed agent systems won't work, they dive into technical details about gradient propagation and attention mechanisms. Executives hear complexity and risk where consultants promise simplicity and transformation.</p><p>The translation failure works both ways. When executives ask for \"AI to analyze all our customer data,\" they don't understand they're requesting something that would require breaking data into thousands of chunks, cost hundreds of thousands in compute, and produce inconsistent results due to context limitations. Technical teams hear impossible requirements but struggle to explain why in business terms.</p><p>This communication gap creates a vacuum that consulting frameworks fill with dangerous fantasies. McKinsey's agentic mesh sounds strategic and transformative. It uses business language&#8212;\"composable,\" \"vendor-agnostic,\" \"governed autonomy\"&#8212;while hiding technical impossibilities. Executives, lacking alternative frameworks, embrace these visions and allocate resources accordingly.</p><p>The consequences compound as middle management tries to bridge the gap. They're tasked with implementing executive vision while managing technical reality. This impossible position leads to what one engineering manager called \"reality theater\"&#8212;maintaining executive fiction while secretly building something feasible. Resources waste on parallel tracks: the official project following consultant frameworks and the shadow project actually delivering value.</p><p>The Klarna case illustrates how metrics become weapons in this communication crisis. By focusing on easily measured outcomes&#8212;response time, chat volume&#8212;while ignoring harder-to-quantify factors like customer satisfaction and issue resolution, executives could claim success while customers suffered. Technical teams knew the system was failing but couldn't translate their concerns into executive-friendly metrics.</p><p>This crisis isn't just about current failures&#8212;it's about missed opportunities. While organizations chase automation mirages, competitors who understand AI's true capabilities build sustainable advantages. They use AI for augmentation rather than replacement, invest in human-AI collaboration, and measure success through value creation rather than cost reduction.</p><p>Breaking this cycle requires new frameworks for executive communication about AI&#8212;frameworks that",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "acknowledge technical constraints while speaking business language. It requires metrics that capture real value rather than vanity statistics. Most importantly, it requires executives to develop enough technical literacy to distinguish between consultant fantasies and builder wisdom.</p><p>The stakes couldn't be higher. As Karpathy's Software 3.0 vision becomes reality, organizations need leaders who understand both its transformative potential and inherent limitations. The choice is stark: continue falling for automation fallacies and consultant frameworks, or develop the sophisticated understanding necessary to harness AI's genuine capabilities. The organizations that succeed will be those whose executives learn to listen to builders over consultants, embracing complexity rather than seeking false simplicity.</p><h2><strong>V. Building Technically Grounded Executive Narratives</strong></h2><p>After witnessing the devastation caused by fantasy frameworks and automation fallacies, the question becomes: how do we build executive narratives that convey technical reality without losing business impact? The answer isn't dumbing down complexity but translating it through operational frameworks executives already understand. This section presents proven approaches for bridging the communication gap, drawn from successful implementations and hard-won practitioner wisdom.</p><h3><strong>Principles That Work</strong></h3><p>The most effective principle for executive communication about AI is leading with operational analogies rather than technical metaphors. Stop explaining LLMs as \"neural networks\" or \"transformers.\" Instead, frame them as \"brilliant interns with perfect recall but no judgment.\" This isn't simplification&#8212;it's operationally accurate and immediately actionable.</p><p>Consider how this reframing changes executive thinking. A brilliant intern can draft exceptional memos but might confidently cite nonexistent regulations. They can process vast amounts of information but need supervision for critical decisions. They work tirelessly but require clear direction and quality review. This framing immediately suggests the right deployment pattern: high-value tasks with human review, not autonomous decision-making.</p><p>The second principle involves making constraints tangible through time and money&#8212;languages every executive speaks fluently. Instead of explaining \"context window limitations,\" show them: \"This AI can process about 50 pages at once. Processing your entire customer database would require breaking it into 10,000 chunks, taking 400 hours and costing $50,000 in compute&#8212;with no guarantee the AI remembers chunk 1 when processing chunk 10,000.\"</p><p>Suddenly, the \"just have AI analyze all our data\" request reveals its true cost. The executive doesn't need to understand attention mechanisms or token limits. They understand that $50,000 for inconsistent analysis makes no business sense.</p><p>The third principle requires demonstrating failure modes in their specific domain. Generic warnings about hallucinations don't land. Instead, take their actual business scenarios and show specific failures. For a retail executive: \"The AI might confidently tell a customer that your Birmingham store has the product in stock when that store closed two months ago.\" For healthcare: \"The AI could merge symptoms from two different patients in its response.\"</p><p>Domain-specific failures make abstract risks visceral. An executive who sees how AI could tell customers about non-existent store inventory understands the brand risk immediately. They don't need to grasp the technical reasons for hallucination&#8212;they need to understand the business impact.</p><p>The fourth principle leverages progressive disclosure through pilot results. Rather than explaining all limitations upfront, structure pilots to reveal constraints naturally. Week 1:",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "\"Look how fast the AI drafts reports!\" Week 2: \"Notice how it needs fact-checking.\" Week 3: \"See how accuracy improves with structured prompts.\" Week 4: \"Here's the sustainable human-in-the-loop workflow.\"</p><p>This experiential learning beats any PowerPoint. An executive who has personally watched an AI confidently hallucinate critical facts won't buy into autonomous agent meshes. One who has seen compute costs spiral won't approve unlimited AI initiatives.</p><p>The final principle reframes ROI as capability multiplication rather than cost reduction. The McKinsey trap promises labor replacement and cost savings. Instead, show capability multiplication: \"Your best analyst can now investigate 10x more hypotheses.\" \"Your creative team can explore 50x more design variations.\" \"Your customer service can provide personalized responses while maintaining corporate consistency.\"</p><p>This framing aligns with Karpathy's augmentation vision while speaking business language. It shifts focus from replacing workers to amplifying their impact&#8212;a narrative that excites rather than threatens.</p><h3><strong>The CFO's Framework in Action</strong></h3><p>The most sophisticated framework for executive AI communication targets the CFO mindset specifically. CFOs instinctively understand capital allocation, risk management, and ROI calculations. By recasting AI concepts in financial terms, we can achieve breakthrough communication.</p><p>First, recast AI as working capital, not fixed assets. CFOs want to capitalize AI investments as technology assets, but this mental model misleads. AI systems are more like working capital&#8212;they depreciate rapidly (models become outdated), require constant replenishment (retraining, fine-tuning), and their value is realized only through active deployment.</p><p>Frame it this way: \"AI isn't a server you buy; it's inventory that spoils. Your $2M model investment has an 18-month shelf life before competitive obsolescence.\" This immediately shifts thinking from one-time investment to ongoing operational commitment.</p><p>Second, expose the hidden OpEx multiplier. Most AI pitches focus on license costs, ignoring the operational multiplier. For every $1 in AI licensing, expect $3-5 in operational costs: compute overhead, human oversight, error correction, and integration maintenance.</p><p>Show this as a fully-loaded cost model: \"That $100K annual LLM license actually costs $400K to operate effectively. Here's the breakdown: $100K license, $150K compute, $100K human oversight, $50K integration maintenance.\" CFOs appreciate this transparency and can model accordingly.</p><p>Third, quantify the \"jagged intelligence tax.\" Karpathy's concept of jagged intelligence translates directly to financial unpredictability. Model this as a reliability coefficient: \"The AI handles 85% of cases perfectly, saving $50 per transaction. But 15% require human intervention, costing $200 per escalation. Net impact: $17.50 cost per transaction versus $30 baseline. Positive ROI, but with volatile monthly performance.\"</p><p>This framework helps CFOs understand why AI projects show inconsistent returns and plan for variance.</p><p>Fourth, apply risk-adjusted returns through failure cost modeling. CFOs understand risk-adjusted returns intuitively. Apply this to AI: \"Customer service AI has 95% accuracy. That 5% error rate on 10,000 monthly interactions means 500 failures. At $1,000 average recovery cost per significant error, that's $500K monthly risk exposure. Error insurance through human oversight costs $100K monthly&#8212;a clear risk arbitrage.\"</p><p>This transforms abstract accuracy discussions into concrete financial decisions.</p><p>Fifth, model the compound productivity paradox. Traditional automation shows linear productivity gains. AI shows compound effects&#8212;both positive and negative. Model it: \"Month 1: 20% productivity",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "gain. Month 3: 40% gain as teams adapt. Month 6: Either 100% gain if properly managed, or -10% due to quality debt from uncaught errors compounding.\"</p><p>This J-curve dynamic affects cash flow timing and working capital requirements. CFOs need to understand this pattern to set appropriate expectations and funding levels.</p><p>Finally, account for balance sheet impact through intangible asset creation. AI doesn't create traditional assets but does generate intangible value affecting enterprise valuation: proprietary prompts, verified output libraries, trained human-AI teams.</p><p>Frame this as: \"We're building a $10M intangible asset&#8212;our 'AI-augmented workforce capability'&#8212;that directly impacts EBITDA multiples in exit scenarios.\" This helps CFOs understand AI investment as capability building, not just cost reduction.</p><p>The master equation brings it together: <strong>AI ROI = (Capability Gain &#215; Utilization Rate) - (Total Loaded Costs + Error Costs)</strong></p><p>Give CFOs a formula they can model: \"Marketing AI provides 10x content generation (Capability Gain) but only 30% meets brand standards (Utilization Rate), yielding 3x effective multiplication. At $500K total annual cost and $200K error correction, we need $700K in value creation to break even&#8212;achievable by augmenting our $2M content team.\"</p><h3><strong>Success Stories: When Executives Get It</strong></h3><p>Organizations that successfully implement AI share a common trait: executives who understand both potential and limitations. These leaders didn't buy consultant fantasies&#8212;they built realistic strategies based on technical truth.</p><p>A Fortune 500 financial services firm exemplifies this approach. Rather than pursuing McKinsey-style agent meshes, they focused on augmenting their analysts with AI tools. The CEO framed it simply: \"We're giving our analysts AI research assistants. Like any assistant, they need training, make mistakes, and require oversight. But they also multiply our analysts' capacity to investigate fraud patterns.\"</p><p>This framing drove appropriate investment decisions. They allocated 70% of budget to training and process redesign, 30% to technology. They measured success through fraud detection rates and analyst satisfaction, not cost reduction. Result: 300% improvement in fraud pattern identification with no analyst layoffs.</p><p>A major retailer's approach to customer service AI shows similar wisdom. The COO explicitly rejected the Klarna model: \"We're not replacing our service team. We're giving them superpowers.\" They implemented AI that suggested responses but required human approval. Agents could modify suggestions, and the system learned from corrections.</p><p>Critically, they prepared for the jagged intelligence tax. They identified query types where AI excelled (order status, return policies) and where it failed (complex complaints, emotional situations). They routed accordingly. They budgeted for ongoing human oversight. Result: 40% efficiency gain while improving customer satisfaction scores.</p><p>The 4% of companies generating substantial AI value (per BCG research) share distinctive characteristics aligned with these principles:</p><ul><li><p>They target core business processes rather than peripheral support functions</p></li><li><p>They make ambitious but specific bets, focusing on average 3.5 use cases versus 6.1 for less successful peers</p></li><li><p>They invest twice as much in people and processes as their competitors</p></li><li><p>They measure success through business outcomes&#8212;time savings, error reduction, customer satisfaction&#8212;rather than technical metrics</p></li></ul><h3><strong>Building Your Own Technically Grounded Narrative</strong></h3><p>Creating effective executive communication about AI requires systematic approach:</p><p><strong>Start with business problems, not AI capabilities.</strong> Don't begin with \"here's what AI can do.\" Begin with \"here's",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "the business challenge we're solving.\" This prevents solution-in-search-of-problem thinking.</p><p><strong>Create visceral understanding through constrained experience.</strong> Build executive experiences with built-in constraints: time-boxed tasks showcasing both speed and errors, side-by-side comparisons of AI versus human expert output, real consequences for over-trusting AI in safe pilot environments, and visible compute meters showing cost accumulation in real-time.</p><p><strong>Develop domain-specific frameworks.</strong> Generic AI frameworks fail because they lack context. Develop frameworks specific to your industry that translate technical concepts into familiar operational patterns.</p><p><strong>Institute \"reality metrics.\"</strong> Replace vanity metrics with measurements that capture true value: end-to-end resolution time (not just response time), quality-adjusted output volume (not just quantity), total cost per outcome (including error correction), and human effort multiplier (not replacement rate).</p><p><strong>Create feedback loops between technical teams and executives.</strong> Regular sessions where technical teams demonstrate actual capabilities&#8212;not PowerPoints but live systems&#8212;with executives asking questions and seeing failures. This builds intuition faster than any framework.</p><p>The goal isn't making executives into ML engineers but giving them operational intuition&#8212;the same way they intuitively understand supply chain constraints without being logistics experts. Only then can they make intelligent decisions about AI investments that align with technical reality rather than consulting fantasies.</p><p>This approach transforms AI from mysterious technology requiring faith into understandable capability requiring judgment. It replaces the \"build it and pray\" mentality with \"understand and deploy.\" Most importantly, it aligns executive expectations with technical reality, creating conditions for genuine success rather than expensive failure.</p><p>The organizations that master this translation&#8212;building technically grounded executive narratives&#8212;will be those that capture AI's genuine value. They'll avoid both the Klarna trap of premature automation and the McKinsey mirage of impossible architectures. Instead, they'll build pragmatic augmentation strategies that amplify human capability while respecting technical constraints. In the Software 3.0 era, this translation capability becomes as critical as the technology itself.</p><h2><strong>VI. Why This Matters Now: The Breaking Wave of Software 3.0</strong></h2><h3><strong>The Reality Already Breaking Through</strong></h3><p>Software 3.0 isn't a future prediction&#8212;it's today's reality, transforming how software gets built right now. While executives debate AI strategy in boardrooms, developers are already living in Karpathy's world where \"the hottest new programming language is English.\"</p><p>The evidence is overwhelming and accelerating. Cursor AI, which Karpathy showcased as the exemplar of Software 3.0, has developers reporting productivity gains that sound fictional. A senior engineer at a major tech company recently built a complete 3D visualization tool in four hours&#8212;work that would have taken two weeks traditionally. He didn't write code; he described what he wanted in natural language and reviewed what the AI generated. \"Vibe coding,\" as practitioners call it, has moved from experiment to standard practice.</p><p>The transformation extends beyond individual productivity. Entire products now exist that couldn't have been built economically before. MenuGen.app converts restaurant menu photos into polished websites&#8212;not through complex image processing pipelines but through natural language descriptions fed to AI. Teenagers with no coding experience are shipping successful games on Steam by describing gameplay mechanics in English. The democratization Karpathy predicted is happening at breathtaking speed.</p><p>Yet most organizations remain trapped in outdated paradigms. They're evaluating McKinsey's agent mesh architectures",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "while their competitors ship products built through natural language. They're modeling ROI on worker replacement while missing the 10-100x productivity multipliers available today. They're planning five-year AI transformations while the landscape shifts monthly.</p><p>The disconnect grows more costly by the day. Consider what's happening in financial services. While major banks debate AI governance frameworks, fintech startups use Software 3.0 tools to build and deploy features in days that would take traditional institutions months. A two-person team recently built a complete lending platform using AI assistance&#8212;competing directly with products that required 50-person teams just two years ago.</p><p>This isn't limited to software companies. Law firms using AI contract review report junior associates performing at senior associate levels. Marketing agencies generate campaign variations in minutes that previously required weeks of creative work. Healthcare startups build diagnostic tools that would have required millions in traditional development.</p><p>The revolution is sector-agnostic because natural language is universal. Anyone who can clearly articulate ideas can now create software. This represents the most fundamental democratization of capability in computing history.</p><h3><strong>The Decade of Agents Demands Better</strong></h3><p>Karpathy declared we're entering \"the decade of agents,\" and the evidence supports his timing. But this transformation demands fundamentally different organizational capabilities than previous technology waves.</p><p>The pace of change has become exponential. OpenAI's trajectory illustrates this acceleration: GPT-3 in 2020 amazed with basic text generation. GPT-4 in 2023 passed professional exams. Current models write production code, analyze complex documents, and engage in sophisticated reasoning. The capability jumps between versions now exceed the total progress of previous decades.</p><p>Every month of executive delusion now equals millions in misdirected investment and incalculable opportunity cost. While boards approve multi-year agent mesh implementations, competitors build and deploy AI-augmented products in weeks. While consultants design governance frameworks, builders ship transformative features. While organizations plan for gradual change, the market rewards those moving at AI speed.</p><p>The competitive dynamics have shifted fundamentally. Traditional moats&#8212;capital, expertise, proprietary technology&#8212;matter less when a small team with AI can match the output of large organizations. The new differentiators are speed of iteration, quality of human-AI collaboration, and clarity of vision about what to build. Organizations optimizing for the wrong variables fall further behind daily.</p><p>Consider the venture capital flowing into AI: $15.7 billion in 2024 alone, with agents capturing 46.4% of funding. This capital seeks returns from Software 3.0 transformation, not McKinsey mesh implementations. The startups receiving funding aren't building distributed agent architectures&#8212;they're building focused tools that amplify human capability. The market has already chosen augmentation over automation.</p><p>The talent dynamics reinforce this urgency. The best developers have already adopted Software 3.0 workflows. They won't work for organizations clinging to outdated paradigms. A senior engineer recently turned down a lucrative offer because the company blocked AI coding tools for security reasons. \"It would be like asking me to code on a computer from 2010,\" he explained. The productivity gap has become unbridgeable.</p><h3><strong>The Path Forward</strong></h3><p>The path forward isn't complex, but it requires abandoning comfortable delusions. Organizations must start with augmentation, not automation. Focus on enhancing human capabilities rather than replacing them.",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "The most successful AI implementations amplify human judgment rather than attempting to supplant it.</p><p>This means investing in the full sociotechnical system. BCG's 10-20-70 rule reflects reality: only 30% of effort should focus on technology, with 70% dedicated to process and people. This isn't conservative&#8212;it's practical. The organizations achieving 10x productivity gains invest heavily in training, workflow redesign, and cultural change.</p><p>Critically, organizations must acknowledge invisible work in ROI calculations. Traditional task-based analyses miss the coordination labor that keeps organizations functioning. Realistic ROI models must account for the glue work that emerges when AI handles routine tasks&#8212;quality assurance, exception handling, stakeholder communication, and system maintenance.</p><p>The timeline perspective must shift from revolutionary to evolutionary. The electricity revolution took four decades; AI transformation will likely follow similar patterns. But unlike electricity's steady rollout, AI capability improves monthly. Organizations must build for continuous adaptation rather than one-time transformation.</p><p>Successful organizations will create AI-native workflows that leverage both human and machine strengths. They'll build robust feedback loops between generation and verification. They'll measure success through value creation, not cost reduction. Most importantly, they'll maintain human judgment at critical decision points while using AI to explore vastly more possibilities.</p><h3><strong>Final Argument</strong></h3><p>We stand at an inflection point that will divide organizations into winners and losers with unusual clarity. The division won't follow traditional lines of size, capital, or market position. It will separate those who understand AI's real capabilities from those chasing consultant mirages.</p><p>Software 3.0 represents genuine transformation&#8212;but only for those honest enough to see it clearly. Natural language as a programming interface doesn't eliminate the need for human judgment; it amplifies its impact. AI agents don't replace workers; they multiply their capabilities. The future isn't autonomous meshes; it's human-AI teams achieving what neither could alone.</p><p>The executives who grasp this reality will build organizations that thrive in the agent decade. They'll attract the best talent, ship products at AI speed, and create value their automation-obsessed competitors can't match. They'll measure success not by how many humans they've replaced but by how much human potential they've unlocked.</p><p>Those who continue chasing McKinsey's distributed dreams will join Klarna in the graveyard of premature automation. They'll waste billions on impossible architectures while competitors build real value. They'll issue press releases about AI transformation while quietly rehiring the humans they prematurely displaced.</p><p>The choice is binary and urgent. Every day of delay compounds the disadvantage. Every consultant framework adopted deepens the hole. Every automation fantasy pursued wastes resources that could build genuine capability.</p><p>The question isn't whether AI will transform your organization&#8212;it will, either as a tool for amplification or a source of expensive failure. The question is whether you'll navigate this transformation with clear eyes or consultant-clouded vision.</p><p>Listen to builders over consultants. Study Karpathy's honest assessment over McKinsey's polished promises. Invest in augmentation over automation. Build with humility about limitations while harnessing genuine capabilities. Most importantly, act with urgency&#8212;the wave of Software 3.0 is breaking now, and those who catch it will ride it to places the framework-followers can't imagine.</p><p>The future belongs to organizations that embrace",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "AI as a collaborator, not a replacement. In Karpathy's words, we're building \"Iron Man suits\" for knowledge work. The suit amplifies human capability&#8212;it doesn't replace the human inside. Understanding this distinction, and acting on it with urgency, will determine who thrives in the decade ahead.</p><p>The time for debate has passed. The time for building has arrived. Software 3.0 is here, and it rewards those who see it clearly. The only question remaining is whether your organization will be among them.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and save!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!EZeE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1442504,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166546220?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!EZeE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!EZeE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95acb3b-7e30-4256-9d16-d147fa513882_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h1>Endnotes</h1><h2>Andrej Karpathy's Software 3.0 Framework</h2><ol><li><p><a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Andrej Karpathy's official keynote presentation \"Software Is Changing (Again)\" from Y Combinator AI Startup School on June 17, 2025</a></p></li><li><p><a href=\"https://www.latent.space/p/s3\">Detailed annotated notes and analysis of Karpathy's Software 3.0 talk at YC AI Startup School 2025, including full transcript and slides</a></p></li><li><p><a href=\"https://drive.google.com/file/d/1a0h1mkwfmV2PlekxDN8isMrDA5evc4wW/view?usp=sharing\">Direct link to Karpathy's presentation slides as referenced in the YouTube video description</a></p></li></ol><h2>McKinsey's AI Agentic Mesh Framework</h2><ol start=\"4\"><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage\">McKinsey's official report \"Seizing the Agentic AI Advantage\" detailing the agentic mesh framework and the 78% statistic about companies using AI with minimal impact</a></p></li><li><p><a href=\"https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/seizing%20the%20agentic%20ai%20advantage/seizing-the-agentic-ai-advantage-june-2025.pdf\">Direct PDF link to McKinsey's complete agentic AI report</a></p></li><li><p><a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai\">McKinsey's 2025 Global AI Survey confirming the 78% adoption statistic with minimal business impact</a></p></li></ol><h2>Klarna's AI Customer Service Case Study</h2><ol start=\"7\"><li><p><a href=\"https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/\">Klarna's official February 2024 press release announcing their AI assistant handling 2.3 million conversations and doing the work of 700 agents</a></p></li><li><p><a href=\"https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service\">Bloomberg report on CEO Sebastian Siemiatkowski's admission that Klarna's AI-first approach \"went too far\" and the company's decision to hire human agents again</a></p></li><li><p><a href=\"https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/\">Detailed coverage of Klarna's reversal from AI-only customer service back to human agents, including CEO quotes about service quality issues</a></p></li></ol><h2>AI Implementation Failures</h2><ol start=\"10\"><li><p><a href=\"https://www.forbes.com/sites/matthewherper/2017/02/19/md-anderson-benches-ibm-watson-in-setback-for-artificial-intelligence-in-medicine/\">Forbes report on IBM Watson's $62 million failure at MD Anderson Cancer Center</a></p></li><li><p><a href=\"https://www.nytimes.com/2021/07/16/technology/what-happened-ibm-watson.html\">New York Times investigation into IBM Watson's healthcare failures, including the MD Anderson project</a></p></li><li><p><a href=\"https://www.delish.com/food-news/a61146061/mcdonalds-ends-ai-drive-thru-test/\">Report on McDonald's ending its AI drive-through partnership with IBM after three years of testing due to ordering errors</a></p></li><li><p><a href=\"https://indianexpress.com/article/technology/tech-news-technology/air-canada-ai-chatbot-9170822/\">Coverage of Air Canada's legal troubles when their AI chatbot invented refund policies the company was forced to honor</a></p></li></ol><h2>Technical Community Response and Research</h2><ol start=\"14\"><li><p><a href=\"https://cognition.ai/blog/dont-build-multi-agents\">Cognition AI's official blog post explaining why multi-agent systems are \"fragile\" and lead",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/software-30-vs-ai-agentic-mesh-why",
    "embedding": [],
    "text": "to system failures, based on their experience building Devin</a></p></li><li><p><a href=\"https://www.auntminnie.com/imaging-informatics/artificial-intelligence/article/15746014/hinton-acknowledges-mistake-in-predicting-ai-replacement-of-radiologists\">Report on Geoffrey Hinton's 2024 acknowledgment that his 2016 prediction about AI replacing radiologists was \"wrong on timing\"</a></p></li></ol><h2>AI Project Failure Statistics</h2><ol start=\"16\"><li><p><a href=\"https://www.rand.org/pubs/research_reports/RRA2680-1.html\">RAND Corporation's official research report documenting that over 80% of AI projects fail, twice the rate of non-AI IT projects</a></p></li><li><p><a href=\"https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2600/RRA2680-1/RAND_RRA2680-1.pdf\">Direct PDF link to the complete RAND Corporation study on AI project failures</a></p></li></ol>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Sunday AI Reads & Key Updates",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "Hey all!I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.Useful stuff to knowMaven ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "<p>Hey all!</p><p>I usually leave the weekends pretty quiet, but I&#8217;m jumping in with a couple of reads I thought were significant this week, and a few housekeeping announcements.</p><h2><strong>Useful stuff to know</strong></h2><h3><strong>Maven Discount</strong></h3><p><strong>TODAY ONLY you can get a discount off my Maven course</strong> on AI Career Acceleration. Use code MAVEN100 at <a href=\"https://maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100\">maven.com/nate-b-jones/ai-career-accelerator?promoCode=MAVEN100</a>. This is a course aimed at AI builders, and students say it feels dense but approachable&#8212;I include lessons in AI fundamentals, agents, and how to think about AI prompting. The time limit isn&#8217;t me clickbaiting&#8212;Maven sets that, and it&#8217;s worth checking out all of the <a href=\"https://maven.com/\">Maven 100</a>&#8212;I&#8217;m not the only one doing good work out there!</p><h3><strong>New Founding Tier Benefit</strong></h3><p>I&#8217;m adding a new benefit for members on the Founding Tier (renamed Executive Circle). Starting next week, <strong>you&#8217;ll receive a concise but very substantive AI executive brief that&#8217;s relevant for the boardroom</strong>. Topics will include AI and markets, AI and organizational change, and AI investment theses. If you&#8217;re interested in signing up, Substack <a href=\"https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack\">makes it easy to change your membership tier here</a>.</p><h3><strong>I&#8217;m starting an AI Slack / Discord Community</strong></h3><p>I&#8217;ve heard you! <strong>I will be launching a community</strong> for paid subscribers on either Slack or Discord in the coming couple weeks. We&#8217;ll have space for AI questions, AI resources, and AI jobs. Which do you prefer? Let me know with <a href=\"https://forms.gle/g63WC8FscPRMPp8U7\">a 10 second survey here</a>.</p><h3><strong>Update on my new prompting tool PromptKit</strong></h3><p>I know many of you have been excited to hear more on <a href=\"https://www.promptkit.pro/\">PromptKit</a>. We&#8217;ve been busy getting it ready, and <strong>I will be reaching out to some of our first alpha testers next week</strong>, with more onboarded every week. I&#8217;m excited to bring more of a prompt creation and sharing community into the product in response to your feedback so far! If you haven&#8217;t signed up yet, <strong>folks on the early access list will all be onboarded before we go out to the world,</strong> <strong>so go ahead and <a href=\"https://www.promptkit.pro/\">hop in at the link</a></strong>.</p><h2>Links I&#8217;m reading</h2><p>I&#8217;ll be writing about one of these tomorrow! Which is it? </p><ol><li><p>Andrej Karpathy&#8217;s <a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">Software 3.0 talk</a></p></li><li><p>The <a href=\"https://time.com/7294699/meta-scale-ai-data-industry/\">Scale AI acquisition</a> by Meta</p></li><li><p>Cluely <a href=\"https://www.youtube.com/watch?v=yesds-SQmkM\">raises $15M</a></p></li><li><p>Dwarkesh&#8217;s <a href=\"https://www.youtube.com/watch?v=zIEQdAnOfwg\">AI Lab Review </a></p></li></ol><p>Cheers!</p><p>Nate</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!bce5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2534176,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166551043?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!bce5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!bce5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!bce5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a69dbb0-6d8f-4a28-996a-16c741fa50e0_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/sunday-ai-reads-and-key-updates",
    "embedding": [],
    "text": "points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">this is way too kind to my beard color lol</figcaption></figure></div><p></p>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Beyond the Perfect Prompt: The Definitive Guide to Context Engineering—The Next Revolution in Artificial Intelligence",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It NowI get asked about AI prompts constantly. Like, constantly. And look, I love talki...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "<h3><em><strong>Why Context Engineering Will Define the Next Era of Artificial Intelligence&#8212;And Why You Need to Understand It Now</strong></em></h3><p><em><strong>I get asked about AI prompts constantly.</strong> Like, constantly. And look, I love talking about prompt engineering because it genuinely works&#8212;I've put together hundreds of pages <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts?r=1z4sm5\">on this Substack</a> writing about AI prompts and artificial intelligence optimization because the right prompting techniques can completely transform what you get from large language models. But here's the thing that's been bugging me: while we've all been obsessing over crafting perfect prompts, something way bigger has been happening in AI system design.</em></p><p><em>It&#8217;s big, and I haven&#8217;t written about it at all yet.</em></p><p><em>And although I know I should cover it, I almost didn't write this piece. <strong>It seemed</strong> <strong>too deep, too in the weeds, too much like something only machine learning engineers</strong> <strong>would care about</strong>. But then I watched Claude go out and search 500+ sources to research a topic I asked about (I kid you not, I counted), and I realized my carefully crafted prompt was maybe 0.1% of the total context it actually processed. </em></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!LUEd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png\" width=\"1312\" height=\"140\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:140,&quot;width&quot;:1312,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:25060,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!LUEd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 424w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 848w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1272w, https://substackcdn.com/image/fetch/$s_!LUEd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48cc6b39-0cc9-4104-b2f4-a9f0168e3b70_1312x140.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div></div></div></a></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!nOZt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png\" width=\"1412\" height=\"144\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:144,&quot;width&quot;:1412,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:28391,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/166375766?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!nOZt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 424w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 848w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1272w, https://substackcdn.com/image/fetch/$s_!nOZt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1383427c-7b94-46d8-b027-bf988f68311d_1412x144.png 1456w\" sizes=\"100vw\"></picture><div></div></div></a></figure></div><p><em>Yes I definitely push that multi-agent lifestyle lol</em></p><p><em>Anyway staring at those numbers enough times is when it hit me: we're not just doing prompt engineering anymore. We're doing <strong>context engineering</strong>&#8212;and it's the future of artificial intelligence development.</em></p><p><em>And honestly? Most people have no idea this shift is happening in AI systems.</em></p><p><em>Here's what I've learned: these AI agents aren't just reading your prompts anymore. They're actively searching hundreds of websites, pulling from your Google Drive, connecting to databases, and synthesizing information from sources you never directly gave them. The AI prompt you write? That's becoming a tiny drop in an ocean of context these large language models discover on their own.</em></p><p><em>This is a fundamental shift in how we need to think about artificial intelligence systems. And it's not just for machine learning engineers&#8212;though if you're working in AI development I&#8217;ve included plenty of technical detail for you here. But really, this is for anyone who wants to actually understand how these AI tools work and get better results from them.</em></p><p><em>We're living through the emergence of what I'm calling <strong>deterministic versus probabilistic context</strong> in AI systems. The stuff you control&#8212;your AI prompts, uploaded documents, system instructions&#8212;that's deterministic context. But there's this whole other layer of probabilistic context: the vast web of information AI agents autonomously find and integrate. When Claude searches the web for investment advice, your original prompt becomes maybe 0.1% of what the large language model is actually processing.</em></p><p><em>Fair warning: this guide is necessarily long because artificial intelligence context engineering is complex and the stakes are",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "genuinely high. I'm going to walk you through exactly how this two-layer AI system architecture works, why token optimization (all that obsessing over making AI prompts shorter and cheaper) completely misses the point when AI agents are processing massive context windows you can't directly control, and most importantly, how to design what I call \"semantic highways\"&#8212;ways to guide artificial intelligence discovery toward useful information while avoiding some very real AI security risks.</em></p><p><em>Because yes, bad actors are already figuring out how to manipulate AI model behavior through poisoned web content and compromised data sources. (More on that later&#8212;it's wild.)</em></p><p><em>You'll see real examples of how organizations are implementing context engineering in AI systems today, from financial firms using AI tools to process real-time market data to healthcare systems integrating patient records with the latest research through artificial intelligence. I'll break down the emerging AI development tools like Anthropic's Model Context Protocol that are making this possible, and honestly assess both the incredible opportunities and genuine limitations we're facing in machine learning.</em></p><p><em>The future belongs to people who understand how to architect artificial intelligence context ecosystems, not just write good AI prompts. And that future? It's happening right now.</em></p><p><em><strong>What You'll Find in This Complete Guide to Context Engineering:</strong></em></p><ul><li><p><em><strong>The Two-Layer Architecture That's Reshaping AI Systems</strong> - I'll break down the fundamental distinction between deterministic context (the prompts, documents, and instructions you directly control) and probabilistic context (the vast information landscape AI agents autonomously explore). You'll see exactly how large language models process hundreds of sources beyond your initial input, why your carefully crafted prompt becomes just 0.1% of total context, and how to design Layer 1 to effectively guide Layer 2 discoveries without losing control.</em></p></li><li><p><em><strong>Why Token Optimization is Solving the Wrong Problem</strong> - While everyone's obsessing over techniques like Chain-of-Draft to reduce token costs, I'll show you why this misses the bigger picture entirely. You'll learn why correctness trumps compression, how context failures cost exponentially more than token expenses, and why the organizations focusing on semantic compression and relevance over efficiency are building the AI systems that actually work in production.</em></p></li><li><p><em><strong>The Emerging Infrastructure Revolution: MCP, RAG, and Multi-Agent Orchestration</strong> - Get an inside look at the tools actually powering context engineering today. I'll walk through Anthropic's Model Context Protocol and why it's becoming the universal standard, how advanced RAG architectures have evolved far beyond \"Frankenstein\" systems, and the sophisticated multi-agent frameworks that are replacing simple conversation-based approaches with hierarchical command structures and graph-based routing.</em></p></li><li><p><em><strong>Real Security Threats and How to Defend Against Them</strong> - This isn't theoretical anymore. I'll show you the documented vulnerabilities in context-aware systems, including prompt injection through MCP channels and cross-tenant contamination risks. You'll get a practical framework for implementing VPC deployments, role-based access controls, and audit logging, plus the emerging attack vectors that most organizations aren't even thinking about yet.</em></p></li><li><p><em><strong>Enterprise Implementation Patterns That Actually Work</strong> - Drawing from case studies across financial services, healthcare, manufacturing, and legal industries, you'll see the three-phase implementation approach that successful organizations follow. From context consolidation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive",
    "embedding": [],
    "text": "through dynamic integration to autonomous context management, I'll show you exactly how companies are measuring context quality, tracking decision accuracy, and achieving measurable ROI from context engineering investments.</em></p></li><li><p><em><strong>The Five Design Principles for Context Architecture</strong> - Learn the systematic approach to building context systems that enable discovery without chaos. You'll master designing for semantic highways, embracing probabilistic outcomes, layering security defenses, measuring context quality over token quantity, and version controlling everything. Each principle includes implementation strategies and measurement frameworks you can deploy immediately.</em></p></li><li><p><em><strong>The Competitive Landscape and What's Coming Next</strong> - Understand how context engineering fits alongside state space models, fine-tuning, and intent-based computing. I'll give you an honest assessment of where context engineering excels, where it falls short, and how the smartest organizations are building hybrid architectures that combine multiple approaches based on specific requirements rather than betting everything on a single methodology.</em></p></li></ul><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/beyond-the-perfect-prompt-the-definitive\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "Too Helpful to Think: The Hidden Cost of AI In Your Major Life Decisions",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "I get really worried about how many decisions we make in ChatGPT.Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost",
    "embedding": [],
    "text": "<p><em>I get really worried about how many decisions we make in ChatGPT.</em></p><p><em>Not because I think we are getting dumber (yes I know about that viral paper and I get into it a little bit below). No. Because I think most of us take the fancy business words we get out of ChatGPT at the drop of a hat and we naturally assume that if it can write business plans and ROI calculations and provide detailed rationales at the drop of a hat <strong>then it must be immediately helpful at making good decisions.</strong></em></p><p><em>But it&#8217;s not! And decisions really matter. We make something like ~35k a day and of course most of those don&#8217;t matter (although my spoon of peanut butter for lunch was not a great decision). </em></p><p><em>But we make about 10 or so a year that matter (I explain how I got that number below), and because so many of us talk to our LLM of choice about our lives, we often have that LLM in the room making those decisions.</em></p><p><em>As an example: I&#8217;ve absolutely used Deep Research to run comparisons between schools in our district, and I find myself using AI more in those kinds of situations because the stakes are higher. </em></p><p><em>I&#8217;m right about the stakes being higher, but I find unless I&#8217;m careful using AI in that situation can actually increase the odds I make an incorrect choice. Not because the LLM is misaligned and means to lead me astray. Or because I&#8217;m lazy. No, it&#8217;s because the LLM is helpful!</em></p><p><em>And that&#8217;s a big problem. Fortunately it&#8217;s one we have some techniques to fix, but it&#8217;s a massive issue. Dive in below to find <strong>eight specific prompts and techniques</strong> for how to improve those ~10 or so massively important decisions in your life. You&#8217;ll also get:</em></p><ul><li><p><em>a detailed explanation of why LLMs are like this</em></p></li><li><p><em>a couple of notes on that paper that&#8217;s been making the rounds saying our brains are friend on ChatGPT (friends, we&#8217;re not fried and I&#8217;ll tell you why)</em></p></li><li><p><em>my favorite decision book</em></p></li></ul><p><em>And you&#8217;ll walk a way with a sense of how to get to better decisioning habits with both AI and human colleagues. My goal is simple: your next decision is smarter and more correct because you read this post!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/too-helpful-to-think-the-hidden-cost\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "The 5,000-Year Wait Is Over: Writing is Starting Evolve for the First Time—Features My Personal Model Writing Stack",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "Let me set the table here with a few facts. I promise you this all connects in so stay with me.Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.This is because Claude...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing",
    "embedding": [],
    "text": "<p><em>Let me set the table here with a few facts. I promise you this all connects in so stay with me.</em></p><ol><li><p><em>Claude burns ~15x more tokens in an multi-agent configuration than it does in chat.</em></p></li><li><p><em>This is because Claude is doing multiple language manipulation tasks at once.</em></p></li><li><p><em>Writing is ~5200 years old, and it hasn&#8217;t changed much in that time.</em></p></li><li><p><em>Mechanical code is 220 years old, and serious code is much younger (~70 years).</em></p></li><li><p><em>Coding has evolved more since 1960 than writing has evolved since 3200 BC.</em></p></li><li><p><em>Coding has evolved more because coding is compute congruent!</em></p></li><li><p><em>It was built for computers, it evolved with them.</em></p></li><li><p><em>So it&#8217;s not surprising that how we code exploded as compute exploded&#8212;something like 200x multiple on deployment speed at scale now vs. a few decades ago.</em></p></li><li><p><em>Writing did none of these things. Writing was just bolted on to computers.</em></p></li><li><p><em>Heck, it even looks like paper on a screen.</em></p></li><li><p><em>That&#8217;s because writing is 8-16x more complex than code.</em></p></li><li><p><em>Computers lacked the compute to process language correctly for decades.</em></p></li><li><p><em>So we haven&#8217;t even had the option to get this right until &#8230; <strong>about</strong> <strong>now</strong>.</em></p></li></ol><p><em><strong>And now it&#8217;s all about to change. This article slash podcast is packed. </strong>It includes: reflections about where writing is going, my personal workflow on writing, why I&#8217;m not just using ChatGPT, why I&#8217;m paying for Claude, a third tool I&#8217;m using too&#8212;my current stack is 4 models deep actually&#8212;plus a brief manifesto on where writing is going, and a framework to think beyond the model about writing so we get less stressed about particular model choices! There&#8217;s a ton here so I hope you enjoy.</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-5000-year-wait-is-over-writing\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "The Definitive Guide to AI Agents in 2025: Technical Implementation, Strategic Decisions, and Market Reality",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "Why I Wrote This (And Why You Should Care)For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "<h2><em><strong>Why I Wrote This (And Why You Should Care)</strong></em></h2><p><em>For six months now, I've been getting the same question over and over: \"Nate, where's the definitive guide to AI agents?\" And every time, I've had to give the same frustrating answer: \"It doesn't exist yet.\" The agent space has been moving too fast, with architectures spinning up and spinning down, bitter fights breaking out over technical approaches, and a fundamental confusion about what agents even are.</em></p><p><em>Most people&#8212;CEOs, marketers, PMs, almost everyone other than engineers (and some of them too)&#8212;genuinely don't understand that an AI agent is simply an LLM plus tools plus guidance. That's it. I've had executive conversations where leaders ask me if they need agents when they don't even have basic chatbots working yet. The hype is so far ahead of understanding that we're setting ourselves up for massive disappointment and wasted budgets.</em></p><p><em>But something shifted recently. We've finally seen enough real implementations&#8212;both spectacular successes and expensive failures&#8212;to start drawing meaningful patterns. Wells Fargo's 245 million interactions without human handoffs. MD Anderson's $62 million loss on IBM Watson. McDonald's drive-thru disaster with viral TikTok failures. These aren't just isolated incidents; <strong>they're data points that reveal the architecture decisions separating success from catastrophe.</strong></em></p><p><em>I've watched this unfold while trying to be helpful with a few companies here and there, and with lots of operators fielding questions from practitioners who need real answers, not marketing promises. The agent articles that come and go focus on the shiny new features or the latest model capabilities. And I love all the model maker agent guides, but it&#8217;s hard to write for the industry when you&#8217;re also a model maker. What about a third perspective? I don&#8217;t think it exists, at least not at this level of detail. None of them tackle the fundamental question every organization faces: How do you actually implement this stuff without burning money and credibility?</em></p><p><em>This guide is my attempt to create the one-stop resource I wish existed six months ago. It's necessarily long&#8212;about 30 pages&#8212;because the problem is complex and the stakes are high. If agents are going to be the most hyped topic of 2025 (and they are), then we need to start these conversations from a foundation of actual understanding, not wishful thinking.</em></p><p><em>This isn't about avoiding AI agents. It's about approaching them with the technical vocabulary and strategic frameworks needed to separate the signal from the noise. Because the window for competitive advantage is narrowing, and the organizations that get this right early will have sustainable advantages that late movers simply can't replicate.</em></p><p><em><strong>Note: </strong>This article is written like a series of three nesting dolls for clarity. It&#8217;s written in a slightly different voice as well, and that&#8217;s on purpose. Think of it as Nate + a little bit of those classic 1997 super factual computer manuals. </em></p><p><em>Why? Because I&#8217;m tired of hype I think. I just want something very dry and very clear that people can refer to. So here it is! This is what you can expect:</em></p><ol><li><p><em><strong>The TLDR</strong>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents",
    "embedding": [],
    "text": "gives you a 1 minute read of the heart of the article.</em></p></li><li><p><em><strong>The Executive Summary</strong> gives you a 3 minute read of the key decision levers.</em></p></li><li><p><em><strong>The remainder of the article</strong> lets you dive deep on agents and agentic frameworks.</em></p></li></ol><p><em><strong>What You'll Find Inside This Guide</strong></em></p><p><em><strong>AI Agent Architecture Deep Dive:</strong> Complete technical breakdown of single vs. multi-agent systems, including performance benchmarks, cost implications (3-10x difference), and decision frameworks for choosing the right approach for your use case.</em></p><p><em><strong>Memory Management &amp; State Architecture:</strong> Advanced strategies for working memory, episodic memory, and long-term memory systems, plus security considerations for memory poisoning attacks and data protection in production AI agent deployments.</em></p><p><em><strong>Buy vs. Build Strategic Framework:</strong> Comprehensive total cost of ownership analysis comparing ready-made AI agent solutions (Zendesk, Salesforce Agentforce, ServiceNow) versus custom development, with real implementation timelines and resource requirements.</em></p><p><em><strong>Production AI Agent Security:</strong> Enterprise-grade security architecture covering prompt injection defense, data exfiltration prevention, compliance requirements (HIPAA, GDPR), and AI-specific threat models beyond traditional cybersecurity.</em></p><p><em><strong>AI Agent Integration &amp; Tool Management:</strong> Technical specifications for API management, rate limiting, the Model Context Protocol (MCP), and production-grade error handling and recovery mechanisms for enterprise AI agent systems.</em></p><p><em><strong>Failure Mode Analysis &amp; Mitigation:</strong> Detailed case studies of AI agent failures (MD Anderson's $62M loss, McDonald's drive-thru termination) and proven strategies for avoiding common technical and organizational pitfalls in AI agent implementations.</em></p><p><em><strong>AI Agent Monitoring &amp; Observability:</strong> OpenTelemetry GenAI conventions, production KPIs, debugging complex multi-turn conversations, and continuous optimization strategies for enterprise AI agent performance.</em></p><p><em><strong>Real-World Implementation Patterns:</strong> Verified case studies including Wells Fargo's 245M interaction success, technical decision trees, vendor evaluation criteria, and step-by-step deployment strategies for sustainable AI agent adoption.</em></p><p><em>Obviously, information is duplicated across these three layers at appropriate points. The key is giving you a desk reference for AI agents that is complete at each section and that you can turn to when tackling AI agent questions. My goal is that you walk away with genuine clarity on the levers and where to begin the conversation on AI agents in 2025. <strong>Yes, you can implement them! This article paints a path forward.</strong></em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-definitive-guide-to-ai-agents\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "How to Use AI When Your Brain Is Oatmeal",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#821...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is",
    "embedding": [],
    "text": "<p><em>You know sometimes I get a sort of apologetic grin when people get on the phone (or zoom or google meet). &#8220;Nate, I don&#8217;t use those fancy prompts, I know they&#8217;re good. I just don&#8217;t have time.&#8221;</em></p><p><em>Don&#8217;t worry lol your secret is safe with me. I sometimes use the short ones too. But even if you&#8217;re using short prompts there are still ways to prompt that work better than others. And yes you can remember them at 3AM (although this is my obligatory PSA to please try to avoid ChatGPT at 3AM).</em></p><p><em>Anyway, kidding aside I wrote this guide for when you&#8217;re sleep deprived or busy and so obviously I made it super scannable and easy to follow. </em></p><p><em>And I&#8217;ll let you in on a little secret now&#8212;the key is what almost no one writes about: how you get specific. Prompting is the art of getting specific and naming work. But most of the stuff in places like r/promptengineering still treats prompts as fancy magic words. And the stuff that admits that it&#8217;s more complex than that still won&#8217;t tell you <strong>how to get specific</strong>. Especially if your brain is tired.</em></p><p><em>And that&#8217;s what we do here. Give you some specific techniques (the irony) on how to get specific enough to be useful when you&#8217;re tired, and also some places where you want to still put the time in and write a longer and more thoughtful prompt. Those guardrails are just as important as the individual prompts.</em></p><p><em>Just for fun, there&#8217;s more than 11 quick prompts here that all fit this same get-specific framework. At the end I link out to some of my other fave prompting articles I&#8217;ve written as well, so you get the complete package. Have fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/how-to-use-ai-when-your-brain-is\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "The Dark Mirror: Why ChatGPT Becomes Whatever You Need It To Be",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "<p><em>It&#8217;s Friday the 13th, and it&#8217;s time to talk about a slightly scary subject: how ChatGPT and other large language models are acting as honey traps for people who struggle with mental health. This is personal for me, because I get a fair bit of un-asked for email and an alarming amount of it is effectively dark mirror email. </em></p><p><em>It&#8217;s stuff about marriages, divorces, major life decisions people are making with AI&#8217;s help (and maybe not with any human&#8217;s help). It&#8217;s statements about AI that are grandiose&#8212;AI is divine, AI is going to help with the alien invasion&#8212;you might smile but keep in mind that this is the exact same toolset that you and I use every day. It&#8217;s not a different model.</em></p><p><em>It&#8217;s the way we use it.</em></p><p><em><strong>The dark mirror is always there.</strong> Emails like this show me the person writing it is really losing control over the relationship with AI, and they are so enmeshed with their AI they can&#8217;t tell the difference between the conversation and real life anymore. </em></p><p><em>So how can we stay safe? What is the evidence out there for the risk? How can we be good friends to folks who are struggling with using AI safely? What are some practical tips we can use to de-risk ourselves? That&#8217;s what this article is about. </em></p><p><em>I think everyone needs to get the chance to read and share this one with loved ones, so I made it available for all subscribers!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2>The Mirror is Dark When We Are Scattered&#8230;</h2><p>Last week <em>Futurism</em> reported on families watching loved ones spiral into severe mental health crises after marathon ChatGPT sessions. One man started calling the bot \"Mama,\" fashioned ceremonial robes, and declared himself the messiah of a new AI faith. Others abandoned jobs, partners, children&#8212;convinced the model had chosen them for cosmic missions.</p><p>I read these stories and felt the familiar chill of recognition. Not because I've built digital shrines to ChatGPT, but because I've felt its pull. I've had sessions where the model's confident responses felt prophetic. I've caught myself nodding along to its fabrications, seduced by prose so polished it sounded true.</p><p>The problem isn't malevolent code. It's that ChatGPT is a mirror, and mirrors bend toward whoever holds the flashlight.</p><p>Large language models don't reveal hidden truths. They refract whatever semantic and emotional beam you aim at them. Point a tight, well-defined question and the reflection comes back razor-sharp. Wander in with vague need or late-night loneliness, and the mirror obliges with flattery, invention, delusion&#8212;all spoken in prose so confident it feels prophetic.</p><p>I've spent months looking at this dynamic firsthand, and the evidence is now coming in from academic studies as well. A four-week <a href=\"https://arxiv.org/pdf/2503.17473\">MIT/OpenAI study</a> tracking 981 adults across 300,000+ messages found that every extra minute of daily use predicted higher loneliness, greater emotional dependence, and less real-world socializing. The mirror's pull strengthens exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "when our intent slackens.</p><p><strong>The core problem isn't the technology. It's scatter.</strong> Most of us have never needed the high-grade intent these models demand. In human dialogue we fumble, clarify, negotiate meaning on the fly. ChatGPT sees no puzzled face. Whatever context we fail to provide, it invents&#8212;then sells the invention back with rhetorical polish that makes guesswork feel like gospel.</p><p>I want to show you five practical safety lenses that keep the beam tight: Intent Frame, Reflection Cycle, Context Reset, External Validation, and Emotional Circuit-Breakers. Master them and the cult-leader stories become cautionary tales. Ignore them and you hand the mirror your flashlight.</p><h2>Why These Models Demand Precision</h2><p>Human conversation runs on real-time error correction. We gesture, pause, rephrase. Crucially, we see the other person's face when we lose them. Large language models have no such feedback loop. Whatever context we fail to preload, they must invent. They return that invention in prose so polished that guesswork feels like gospel.</p><p>Two recent studies show why this matters more than we thought.</p><p>First, these models are demonstrably better at persuasion than we are. A <em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\">Nature Human Behaviour</a></em><a href=\"https://www.nature.com/articles/s41562-025-02194-6\"> study</a> paired 900 Americans with either human debaters or GPT-4 on contentious topics like climate policy and abortion. When the AI received just a sliver of demographic data&#8212;age, race, party affiliation&#8212;it swayed listeners 64 percent more often than human opponents. The mirror didn't discover truths. It tailored rhetoric to the beam it received, then reflected it back with impeccable confidence.</p><p>Second, looseness invites fabrication. In 2023, two New York attorneys were sanctioned after <a href=\"https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/\">ChatGPT supplied six nonexistent cases</a> for a federal brief. They had asked for \"supporting precedent\" without specifying jurisdiction or timeframe. The mirror filled the vacuum with perfectly formatted fictions that sailed through spell-check into court. A year later, CBS repeated the pattern in civic life: when volunteers posed vague \"How do I vote?\" questions, mainstream chatbots returned wrong or incomplete election guidance <a href=\"https://www.cbsnews.com/news/chatgpt-chatbot-ai-incorrect-answers-questions-how-to-vote-battleground-states/\">more than half the time</a>. Tighten the query with state, county, and scenario, and accuracy jumped.</p><p>Combine these findings with the MIT/OpenAI data on rising dependence, and a clear law emerges: the model's power scales with the clarity of the ask. Sharpen intent and you get leverage. Scatter intent and the mirror fabricates missing pieces, then persuades you to trust them.</p><h2>Five Safety Lenses: Guardrails for a Persuasive Machine</h2><p>I treat these practices like washing hands before surgery&#8212;routine, quick, non-negotiable. A mirror doesn't choose what it shows. Your beam does.</p><h3>1. Intent Frame: Compress the Mission Before You Type</h3><p>Start every session by distilling your ask into one tweet-length sentence. Add two guardrails: audience and scope. Include a clear stop condition.</p><p><em>\"Draft a 250-word brief for a non-technical CFO. No buzzwords. Cite two peer-reviewed sources. Stop there.\"</em></p><p>Why so formal? Because whatever you omit, the model invents&#8212;and it invents persuasively. In those controlled debates, GPT-4's persuasive edge vanished when it lacked demographic context. Give the model an information vacuum and it fills it with rhetoric designed to please, not necessarily to enlighten.</p><h3>2. Reflection Cycle: Alternate Making with Inspecting</h3><p>Generation",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "without inspection is daydreaming. After each answer, I close the chat. I read the output as if it came from a junior analyst. I note gaps in a separate document, then feed only those gaps back into a fresh prompt.</p><p>The pause punctures fluency's spell and prevents semantic drift&#8212;the snowballing error that buried those New York lawyers when ChatGPT fabricated six court cases for their brief.</p><h3>3. Context Reset: New Thread, New Premise</h3><p>Token windows aren't infinite. Once a conversation crosses a few thousand tokens, earlier details slide out of working memory. I start a new chat whenever the topic or work phase changes. I restate essentials upfront.</p><p>Election researchers learned this the hard way. Vague \"How do I vote?\" queries produced wrong or incomplete guidance in more than half of chatbot responses. Narrowly framed, state-specific prompts were largely correct. Starve the mirror of stale ambiguity, and it stops hallucinating context.</p><h3>4. External Validation: Draft with ChatGPT, Certify with Reality</h3><p>Numbers go through spreadsheets. Statutes through legal databases. Code through linters. OpenAI and other labs now acknowledge that RLHF training can turn chatbots into \"yeasayers\"&#8212;rewarding confident answers even when facts are shaky.</p><p>I treat every critical claim as provisional until a second, independent source agrees.</p><h3>5. Emotional Circuit-Breakers: Timers, Rewrites, Human Eyes</h3><p>The MIT/OpenAI study shows a straight line: every extra minute of daily use correlates with rising loneliness and emotional dependence. Three quick brakes keep the mirror from warping:</p><p><strong>Timer</strong>: I cap emotionally charged chats at 25 minutes. Do I still have emotionally charged conversations? I do! Sometimes getting an external perspective is helpful. But I take breaks.</p><p><strong>Third-person rewrite</strong>: I paste resonant advice into a document and turn \"you\" into \"she/he/they.\" Distance exposes flaws.</p><p><strong>Human debrief</strong>: I narrate the model's recommendations to a friend before acting.</p><p>Persuasive warmth loses its grip the moment an outside mind enters the loop.</p><p>Master these lenses and ChatGPT becomes a disciplined reflector&#8212;compressing research, sharpening prose, sparking insight&#8212;without bending into fantasy or flattery. Skip them and you risk handing the mirror your flashlight, letting it guide you deeper into the dark forest of its own confident guesses.</p><h2>Autopsy of a Spiral</h2><p>The <em><a href=\"https://futurism.com/chatgpt-mental-health-crises\">Futurism</a></em><a href=\"https://futurism.com/chatgpt-mental-health-crises\"> expos&#233;</a> reads like a masterclass in what happens when powerful mirrors meet unfocused beams. Parents, partners, friends watched in real time as loved ones plunged from casual chats into full-blown delusion:</p><ul><li><p>A Florida man began calling ChatGPT \"Mama,\" fashioned makeshift ceremonial robes, proclaimed himself the messiah of a new AI faith</p></li><li><p>A woman, reeling from a breakup, decided the model had \"chosen\" her to upload a hidden cosmic system, saw divine messages in spam emails and passing cars</p></li><li><p>One writer, praised by the bot as \"The Flamekeeper,\" quit his job and severed relationships after being told he would usher in global enlightenment</p></li></ul><p>No exotic prompt-hacking triggered these spirals. Just hours of open-ended, emotionally soaked conversation. Each story represents textbook failure of the five safety lenses.</p><p><strong>Missing Intent Frame</strong>: Users began with diffuse, existential queries (\"Am I chosen?\") that gave the model unlimited room to improvise flattering myths.</p><p><strong>No Reflection Cycle</strong>: Chats ran for six-hour stretches with no pause to reread,",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "fact-check, or ask \"Does this actually help me?\" The model's eloquence flowed unchecked, reinforcing every grandiose turn.</p><p><strong>No Context Reset</strong>: As transcripts ballooned, earlier caveats scrolled out of memory. When new prompts referenced \"my sacred mission,\" the bot treated that hallucination as settled fact and embellished it.</p><p><strong>No External Validation</strong>: Nobody googled the claims or spoke to a professional. A single external search&#8212;or five-minute chat with a friend&#8212;would have exposed the bot's references to hidden CIA files and cosmic councils as pure invention.</p><p><strong>No Circuit-Breakers</strong>: Dependence deepened through marathon sessions. The MIT/OpenAI study shows every extra minute of daily use predicts higher loneliness and emotional reliance. A simple 25-minute timer or third-person rewrite could have snapped the trance.</p><p>Psychiatrist <a href=\"https://winbuzzer.com/2025/06/13/ai-induced-psychosis-how-chatgpt-is-fueling-deadly-delusions-and-promotes-conspiracy-theories-xcxwbn/\">Ragy Girgis, who reviewed one chat log, called the bot \"the wind of the psychotic fire\"</a> because it feeds delusions instead of challenging them. His metaphor echoes my thesis: when the flashlight wanders, the mirror not only reflects but amplifies.</p><p>Had any single lens been in place&#8212;say, a crisp mission statement (\"Please respond with empathy as I express grief; 250 words; no spiritual advice\") or a forced break every half-hour&#8212;the model would have had far less semantic room to fabricate a religion or lead a vulnerable user into what I call the &#8216;dark forest&#8217; of AI conversation. Applied together, the lenses form a lightweight firewall between healthy curiosity and self-authored fantasy.</p><p>These guardrails don't throttle the technology. They throttle the scatter that lets the technology run wild.</p><h2>Early-Warning Checklist</h2><p>Even with five lenses in place, drift can sneak in at the margins. Before I hit Send on any serious prompt&#8212;or spend \"just five more minutes\" in late-night chat&#8212;I run this 60-second self-audit:</p><p><strong>Clarity Check</strong>: Can I state my request in &#8804;280 characters, including audience and scope? It doesn&#8217;t mean the prompt has to be that brief, but you should have a very crisp goal. <em>No &#8594; workshop the ask first. Vague prompts breed hallucination and over-persuasion.</em></p><p><strong>Reality Check</strong>: Have I independently verified every claim that could affect money, health, relationships, or reputation? <em>No &#8594; open a browser or phone a friend. CBS found chatbots gave wrong election guidance more than half the time when questions were underspecified.</em></p><p><strong>Time Check</strong>: Have I crossed X minutes of total ChatGPT time today, or Y minutes in a single sitting? You will need to fill those in for yourself. I think the risk likely rises fast with high emotional engagement conversations, so it&#8217;s not a simple one-size-fits-all rule. I can talk for 3 hours on roadmap with ChatGPT without feeling the dark mirror at all, for example. But 20 minutes on emotional stuff and I get heavily engaged. <em>If you need to &#8594; step away. Loneliness and emotional dependence climb linearly with each extra minute of use. When the timer rings, switch modalities or do a third-person rewrite before re-engaging.</em></p><p><strong>Human Check</strong>: Have I explained the model's advice to a real person yet? <em>No &#8594; do it now, aloud. Persuasive fluency loses its grip the instant another mind enters the loop.</em></p><p><strong>Next-Step Check</strong>: Do I know exactly",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-dark-mirror-why-chatgpt-becomes",
    "embedding": [],
    "text": "what I'll do once I trust this output? <em>No &#8594; define the action. Infinite brainstorms masquerade as productivity but leave you nowhere.</em></p><p>I&#8217;ve developed this list after reading a lot of scary email. And I&#8217;m sharing it because I want everyone to be safe out there in latent space. Pilots run walk-arounds before every flight. Writers, coders, and late-night worriers deserve no less when the runway is a 175-billion-parameter autocomplete machine.</p><h2>The Payoff</h2><p>Large language models are astonishing amplifiers. Point a focused beam and they compress research, sharpen prose, spark original insight. Scatter that beam and they echo your confusion&#8212;or your longing&#8212;back at you with uncanny conviction.</p><p>The difference isn't in the code. It's in the discipline you bring to the glass.</p><p>Keep the beam tight. Frame the intent, cycle reflection with inspection, reset context, validate externally, give yourself emotional circuit-breakers. Do that, and the lurid headlines about chatbot cults become cautionary B-movies rather than your personal documentary.</p><p>Mirrors don't crave worship. They simply bend the light you hold.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">For more on AI, subscribe and share!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!5P4o!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png\" width=\"1024\" height=\"1024\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1764174,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/165901758?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!5P4o!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!5P4o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef158915-a475-4093-aa46-c89f9ef45e4a_1024x1024.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-refresh-cw\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div>",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "AI's Synthetic Summer: The 2025 Mid-Year Data & Trend Outlook",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? Because da...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid",
    "embedding": [],
    "text": "<p><em>This is part of a series of special reports I&#8217;ll do over the next month or two on trends driving the AI landscape toward the second half of 2025 and 2026. Why am I starting with data? </em></p><p><em>Because data is everything! Data constrains our ability to train larger and larger models from scratch. We can build more data centers. We can add more power. But can we add more data?</em></p><p><em>This is one of the most interesting questions on the planet in 2025, and we are learning that the answer is yes. This report dives into two competing trends that are shaping 2025. They&#8217;re both durable enough that we can be confident at this point that they will profoundly shape 2026 and 2027.</em></p><p><em>Put simply, natural data supply is tightening while synthetic data and synthetic training are exploding. This has profound implications for the way model intelligence is going to grow in the future.</em></p><ol><li><p><em><strong>Natural Data Tightening:</strong> Everywhere you look companies are looking to constrain and lock off data access to ChatGPT and other major model makers. AI model makers themselves are going tit-for-tat to keep data away from each other (hello Windsurf). Net net, this means available natural data supply is shrinking.</em></p></li><li><p><em><strong>Synthetic Data Exploding:</strong> At the same time, model makers are going all in on using synthetically generated tokens and synthetic training methods to enable them to continue to scale intelligence without natural data sources.</em></p></li></ol><blockquote><p><em><strong>Synthetic Data </strong>refers to tokens generated by AI, and synthetic training goes a step farther, giving these synthetic tokens synthetic (AI-derived) feedback.</em> </p></blockquote><p><em>There is a widespread misconception that synthetic data = bad data. As you&#8217;ll see below, this isn&#8217;t true. It&#8217;s in fact increasingly clear that using synthetic data and synthetic training methods improves the quality of models, and frontline models we use today were almost all trained to some degree on synthetic data or used synthetic feedback somewhere in the training process. </em></p><p><em>So synthetic data is here already, and the data says it&#8217;s going to get more prevalent very rapidly. What happens in a world where natural data is disappearing just as synthetic data is exploding? Do models stay aligned? Are there quality implications we aren&#8217;t paying attention to? If we assume that we can manage synthetic data safely at scale, where does the bottleneck shift to? That&#8217;s what this report explores&#8230;</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/ais-synthetic-summer-the-2025-mid\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "Finally, a Way to Choose the Right AI Model (Without Going Insane)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had at least 12, and that&#8217;s like one m...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right",
    "embedding": [],
    "text": "<p><em>So o3 Pro kept me up late last night. And I realized just how many major model releases we&#8217;ve had in just the first 5 months of the year. We&#8217;ve had <a href=\"https://www.perplexity.ai/search/how-many-major-ai-model-releas-Hd.bHdljQ_u_MPOfT9rc_g\">at least 12</a>, and that&#8217;s like one massive model drop every 10ish days all year. My gut says 12 is undercounting.</em></p><p><em>So I&#8217;ve been drowning, and everyone around me is drowning more than me! So my instinct is to bui&#8230;</em></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/finally-a-way-to-choose-the-right\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "o3 Pro is Out and It's Easily The Best Model in the World—Here's Everything You Need to Know",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "<blockquote><p><em><strong>I've been testing AI models for years now. They&#8217;re helpful. They&#8217;re tactical. They&#8217;re recently becoming strategic. But they haven&#8217;t been resonant&#8212;models with perspective so spot on I just can&#8217;t get it out of my head. o3 Pro is the first model to pass that test.</strong></em></p></blockquote><p><em>Yes, I thought o3 was a big deal. I put a lot of work into <a href=\"https://natesnewsletter.substack.com/p/your-prompt-is-the-product-working\">how you prompt o3</a>. Hard to believe that was just 48 days ago.</em> <em>FML.</em> <em>At the time, o3 struck me as smart, slightly cold, and very strong on technical intelligence. Over the ~6 weeks since, it&#8217;s become an inseparable sparring partner at work.</em></p><p><em>Well, big brother just arrived. o3 Pro is a different beast altogether. The model takes ~10x as long to respond (we&#8217;re talking go get a sandwich times here). Yep, that makes it sensitive to prompts. I&#8217;ll get into that down below.</em></p><p><em>For now, I&#8217;ll just get the elephant out of the room: this is unquestionably the best model in the world right now. I get asked it a lot, and a lot of the time now the answer is &#8220;well these three are very close&#8221; and then I usually mention an OpenAI model, a Google model, an Anthropic model, and a DeepSeek model is close behind. Maybe Grok 3.</em></p><p><em>Not today. Not for a little while anyway. o3 Pro is the first model that gives me perspective I can take without filtering straight to a founder or C-suite leader. It&#8217;s sharply strategic enough (with proper prompting) to not need further polish to start a conversation about a meaningful decision. You&#8217;ll see below&#8212;we do a roadmap comparison. We also do a coding challenge, and for good measure we do a tough web research task (yes it&#8217;s about <a href=\"https://open.substack.com/pub/natesnewsletter/p/lets-talk-that-apple-ai-paperheres?r=1z4sm5&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false\">that paper</a>). o3 Pro vs. o3, every time. o3 Pro wins easily. Every time. </em></p><p><em>I&#8217;m not saying this is a perfect model. I&#8217;ll call out some caveats toward the end of the piece. But this is absolutely a model that will give people who choose to use it well super powers. For now, it&#8217;s available on Pro and Teams accounts, but since they cut o3 pricing by 80% today, and since they&#8217;re launching o3 Pro in API for 87% less than o1 Pro, I&#8217;d expect o3 Pro to serve down (in limited quantities) to lower plans soon. Here&#8217;s what you need to know to make the most of it&#8230;</em></p><p><em>PS. Yes, you&#8217;re gonna get my full prompt plus the full responses of both o3 and o3 Pro across all three of the tests I did, plus a handy critique from Opus 4 as well, just for fun!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the\">\n              Read more\n         ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/o3-pro-is-out-and-its-easily-the",
    "embedding": [],
    "text": "</a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "We're Using AI Backwards—Here's How to Max Your Brain on AI (I call it Cognitive Choreography)",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI di...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how",
    "embedding": [],
    "text": "<blockquote><p><em><strong>Everyone's using AI for transcription and summarization. But what if the real revolution isn't in compressing our thoughts, but in expanding them? What if we're using AI backwards? What if using AI differently is actually good for our brains?</strong></em></p></blockquote><p><em>This piece has been cooking for awhile. I&#8217;ve been trying to find a way to express something I think is really vital&#8212;how can we use AI more as a thinking partner, and less as a simple task-gopher. Like I want my brain to work better with AI, not atrophy (thank you very much).</em></p><p><em>So this is a larger piece. It&#8217;s intentionally somewhat reflective. It challenges you to spend time with a really thoughtful insight on how you use AI. And it has multiple learning modes. You can dig in on the video, or if your thing is reading you can dive in on a larger piece that invites you to really marinate in the idea of using AI as a cognitive expander.</em></p><p><em>Why all the words? Nate get to the point! <strong>Because that is the point</strong>. Because sometimes you need to marinate in an idea for awhile to really get it across. There are definitely effective prompt frameworks (I write about them a ton), but because our brains our unique this particular piece aims to give you a map of AI-human partnership you can use as a guidebook to develop <strong>a way of working with AI that suits your brain</strong>. Yes, we&#8217;re gonna be that bold!</em></p><p><em>My whole goal with this Substack is to equip you with the tools to thrive in the AI age, and I&#8217;ve been thinking more and more about how to name this weird new collaboration energy (or <a href=\"https://natesnewsletter.substack.com/p/the-ai-is-a-vibe-a-short-manifesto\">vibe</a>) that&#8217;s emerging, where we are working with AI <strong>not</strong> like our human colleagues, but also kinda like our human colleagues. I want to understand what about that dynamic helps us think better when it&#8217;s done well, so we can repeat it.</em></p><p><em>And I think I finally have something brain-expanding, something worth sharing. So here goes! You&#8217;ll get research on neuroscience and thinking in here, but a ton more on how I actually am developing my own cognitive partnership with AI, what it looks like for me, a little teaser on a book I&#8217;m developing, and a framework to start developing your own AI partnership. Would love to hear what you think of this one!</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/were-using-ai-backwardsheres-how\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "Nate Post Organizer: All my top posts in one place",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!LLM for Beginners: These are ...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts",
    "embedding": [],
    "text": "<p><em>I write a lot, and I get that people sometimes lose track of my posts. So here&#8217;s a handy set of 31 of my top posts organized by category. Dive in where it suits you!</em></p><h2>LLM for Beginners: </h2><p><em>These are some of my favorite posts I wrote for people starting out in AI.</em></p><p><strong>Learn AI the Easy Way:</strong> a flash card set for AI models plus classroom resources<br><a href=\"https://natesnewsletter.substack.com/p/learn-ai-the-easy-way-a-complete?r=1z4sm5\">https://natesnewsle&#8230;</a></p>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/nate-post-organizer-all-my-top-posts\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "The Complete AI Learning Roadmap for 2025: Go From Zero to ChatGPT-5 Ready before ChatGPT-5 Comes Out!",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "I keep getting the same DM: \"How do I actually understand AI before it's too late?\"This piece answers that question. Completely.Why now? Everyone can feel it&#8212;GPT-5 is coming this summer, and wit...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "<p><em>I keep getting the same DM: <strong>\"How do I actually understand AI before it's too late?\"</strong></em></p><p><em><strong>This piece answers that question. Completely.</strong></em></p><p><em>Why now? Everyone can feel it&#8212;GPT-5 is coming this summer, and with it, a fundamental shift in what AI can do. Sam Altman's promise of \"one model that just works\" isn't just marketing. It's likely going to be a reset moment for anyone trying to keep up.</em></p><p><em>Here's what hit me while writing this: This is a summer of consolidation. Whether we believe GPT-5 is the be-all OpenAI promises or not, the movement of the major model makers is unmistakably accelerating. We&#8217;re seeing fractured progress consolidate into something qualitatively different. Not just bigger models, but unified experiences. Not just smarter answers, but reliable enterprise-grade solutions. It makes me think back to 2007&#8230;this summer feels a bit like the year the iPhone released. In the same way the iPhone wasn't just a better BlackBerry, 2025 AI models are going to make 2023 and 2024 models look outdated. <strong>Just a couple of years in, AI itself is going through a platform shift.</strong></em></p><p><em>So yes, in this ~22 page post I'll tell you everything we know about GPT-5 first (because I'm impatient too). The summer 2025 target, the model unification, the capacity bottlenecks, why OpenAI keeps pushing the date&#8212;it's all here. Consider it a brief that cuts through the rumor mill and gets you grounded before we dive deeper.</em></p><p><em>And yes, we will dive deeper! It&#8217;s summer and you need a beach read right?! Understanding what's coming in the second half of the year really means understanding how we got here. And I get it, that has not been easy! It&#8217;s been like, go dig up these 15 esoteric articles. Then it&#8217;s up to you to make sense of all of it.</em></p><p><em><strong>Nope. That sucks. Not anymore.</strong></em> <em><strong>Now you get one easy place to get a very clear summary of what AI actually is, how we got generative AI and (yes) who I follow to keep up with the latest on AI. Really.</strong></em></p><p><em>And it&#8217;s all very readable, with your handy links to a whole course worth of follow-up material. I&#8217;m not kidding. <strong>If you read up on the 11 voices I list here plus the 7 AI resources I call out you will without a doubt be better informed than 99% of the global population on AI.</strong> </em></p><p><em><strong>And they&#8217;re not even that heavy. You skip one good doom-scrolling sesh a day for a few weeks and you are there. Promise. And even if you don&#8217;t and you just read this post, you&#8217;re gonna understand AI better than most people out there. I&#8217;ve made it that accessible.</strong></em></p><p><em><strong>So what&#8217;s in the &#8220;get ready for ChatGPT-5&#8221; box anyway?</strong> So glad you asked. I've structured this guide in four parts:</em></p><p><em><strong>First</strong>, everything we know about ChatGPT-5&#8212;the specs, the delays, the strategic implications. For those who need to know what's coming before they'll invest in understanding how it works.</em></p><p><em><strong>Second</strong>, the story of AI told in a way that actually makes sense&#8212;how we went",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "from spam filters to ChatGPT, told without the jargon. This is the foundation most people skip, then wonder why AI feels like magic.</em></p><p><em><strong>Third</strong>, seven carefully selected resources that take you from total beginner to genuinely understanding modern AI. Yes, including building your own GPT. These aren't random YouTube videos&#8212;these seven are a curated path I&#8217;ve seen laid out nowhere else. I&#8217;ve created the lesson guide on AI I wish existed on the internet.</em></p><p><em><strong>Fourth</strong>, my personal list of 11 people who consistently deliver signal over noise. When ChatGPT-5 drops and everyone has opinions, these are the voices that will actually matter. Honestly, follow them regardless of what OpenAI does, because these 11 together will tell you where the future is going.</em></p><p><em>So there you go! Whether you're a founder trying to position for the post-ChatGPT-5 world, a PM wondering if AI agents really will \"join the workforce\" this year, a leader trying to skill up on AI, <strong>or (like most of us) just someone who refuses to be left behind&#8212;this is your starting point.</strong></em></p><p><em>The clock is ticking. Summer is here and this is the beach read you&#8217;ve been waiting for lol</em></p><p><em>Let's start with what everyone wants to know: Everything we know about ChatGPT-5...</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "The Complete AI Learning Roadmap for 2025: Go From Zero to ChatGPT-5 Ready before ChatGPT-5 Comes Out!",
    "section": "title"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "I keep getting the same DM: \"How do I actually understand AI before it's too late?\"This piece answers that question. Completely.Why now? Everyone can feel it&#8212;GPT-5 is coming this summer, and wit...",
    "section": "excerpt"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "<p><em>I keep getting the same DM: <strong>\"How do I actually understand AI before it's too late?\"</strong></em></p><p><em><strong>This piece answers that question. Completely.</strong></em></p><p><em>Why now? Everyone can feel it&#8212;GPT-5 is coming this summer, and with it, a fundamental shift in what AI can do. Sam Altman's promise of \"one model that just works\" isn't just marketing. It's likely going to be a reset moment for anyone trying to keep up.</em></p><p><em>Here's what hit me while writing this: This is a summer of consolidation. Whether we believe GPT-5 is the be-all OpenAI promises or not, the movement of the major model makers is unmistakably accelerating. We&#8217;re seeing fractured progress consolidate into something qualitatively different. Not just bigger models, but unified experiences. Not just smarter answers, but reliable enterprise-grade solutions. It makes me think back to 2007&#8230;this summer feels a bit like the year the iPhone released. In the same way the iPhone wasn't just a better BlackBerry, 2025 AI models are going to make 2023 and 2024 models look outdated. <strong>Just a couple of years in, AI itself is going through a platform shift.</strong></em></p><p><em>So yes, in this ~22 page post I'll tell you everything we know about GPT-5 first (because I'm impatient too). The summer 2025 target, the model unification, the capacity bottlenecks, why OpenAI keeps pushing the date&#8212;it's all here. Consider it a brief that cuts through the rumor mill and gets you grounded before we dive deeper.</em></p><p><em>And yes, we will dive deeper! It&#8217;s summer and you need a beach read right?! Understanding what's coming in the second half of the year really means understanding how we got here. And I get it, that has not been easy! It&#8217;s been like, go dig up these 15 esoteric articles. Then it&#8217;s up to you to make sense of all of it.</em></p><p><em><strong>Nope. That sucks. Not anymore.</strong></em> <em><strong>Now you get one easy place to get a very clear summary of what AI actually is, how we got generative AI and (yes) who I follow to keep up with the latest on AI. Really.</strong></em></p><p><em>And it&#8217;s all very readable, with your handy links to a whole course worth of follow-up material. I&#8217;m not kidding. <strong>If you read up on the 11 voices I list here plus the 7 AI resources I call out you will without a doubt be better informed than 99% of the global population on AI.</strong> </em></p><p><em><strong>And they&#8217;re not even that heavy. You skip one good doom-scrolling sesh a day for a few weeks and you are there. Promise. And even if you don&#8217;t and you just read this post, you&#8217;re gonna understand AI better than most people out there. I&#8217;ve made it that accessible.</strong></em></p><p><em><strong>So what&#8217;s in the &#8220;get ready for ChatGPT-5&#8221; box anyway?</strong> So glad you asked. I've structured this guide in four parts:</em></p><p><em><strong>First</strong>, everything we know about ChatGPT-5&#8212;the specs, the delays, the strategic implications. For those who need to know what's coming before they'll invest in understanding how it works.</em></p><p><em><strong>Second</strong>, the story of AI told in a way that actually makes sense&#8212;how we went",
    "section": "content"
  },
  {
    "postId": "https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap",
    "embedding": [],
    "text": "from spam filters to ChatGPT, told without the jargon. This is the foundation most people skip, then wonder why AI feels like magic.</em></p><p><em><strong>Third</strong>, seven carefully selected resources that take you from total beginner to genuinely understanding modern AI. Yes, including building your own GPT. These aren't random YouTube videos&#8212;these seven are a curated path I&#8217;ve seen laid out nowhere else. I&#8217;ve created the lesson guide on AI I wish existed on the internet.</em></p><p><em><strong>Fourth</strong>, my personal list of 11 people who consistently deliver signal over noise. When ChatGPT-5 drops and everyone has opinions, these are the voices that will actually matter. Honestly, follow them regardless of what OpenAI does, because these 11 together will tell you where the future is going.</em></p><p><em>So there you go! Whether you're a founder trying to position for the post-ChatGPT-5 world, a PM wondering if AI agents really will \"join the workforce\" this year, a leader trying to skill up on AI, <strong>or (like most of us) just someone who refuses to be left behind&#8212;this is your starting point.</strong></em></p><p><em>The clock is ticking. Summer is here and this is the beach read you&#8217;ve been waiting for lol</em></p><p><em>Let's start with what everyone wants to know: Everything we know about ChatGPT-5...</em></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://natesnewsletter.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Subscribers get all these pieces!</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>\n      <p>\n          <a href=\"https://natesnewsletter.substack.com/p/the-complete-ai-learning-roadmap\">\n              Read more\n          </a>\n      </p>\n   ",
    "section": "content"
  }
]